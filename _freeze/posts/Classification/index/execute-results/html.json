{
  "hash": "89db0d407a5100b614b73ff20c974624",
  "result": {
    "markdown": "---\ntitle: \"Clasification\"\ndate: \"2023-11-05\"\nauthor: Swethaa\nimage: \"Image.png\"\n---\n\n```{=html}\n<h1>Classification</h1>\n<img src=\"regression-vs-classification-in-machine-learning.png\" alt=\"\" height=\"300px\">\n    <h3>What is Classification?</h3>\n    <p>Classification is a type of supervised machine learning task where the goal is to predict the categorical class labels of new instances based on past observations. In classification, the algorithm is trained on a labeled dataset, where each example\n        has an associated class label, and the model learns to map input features to specific categories.</p>\n    <p>Classification algorithms used in machine learning utilize input training data for the purpose of predicting the likelihood or probability that the data that follows will fall into one of the predetermined categories. One of the most common applications\n        of classification is for filtering emails into “spam” or “non-spam”, as used by today’s top email service providers</p>\n    <h3> Binary Classification vs. Multi-Class Classification:</h3>\n    <ul>\n        <li><b>Binary Classification</b>\n            <p>In binary classification, there are two possible classes or outcomes (e.g., spam or not spam, positive or negative)</p>\n        </li>\n        <li><b>Multi-Class Classification</b>\n            <p>In multi-class classification, there are more than two classes (e.g., classifying animals into categories like cats, dogs, and birds)</p>\n        </li>\n    </ul>\n    <h3>Types of Classification Algorithms</h3>\n    <ul>\n        <li><b>Linear Classifiers</b>\n            <p>Algorithms that create a linear decision boundary, such as Logistic Regression or Linear Support Vector Machines (SVM)</p>\n        </li>\n        <li><b>Non-linear Classifiers</b>\n            <p>Algorithms that can capture non-linear relationships, such as Decision Trees, Random Forests, k-Nearest Neighbors (k-NN), and Support Vector Machines with non-linear kernels</p>\n        </li>\n        <li><b>Ensemble Methods</b>\n            <p>Techniques that combine multiple base classifiers to improve overall performance, such as AdaBoost, Gradient Boosting, and Random Forests</p>\n        </li>\n        <li><b>Neural Networks</b>\n            <p>Deep learning models, such as artificial neural networks, are powerful for complex tasks but require a larger amount of data and computational resources</p>\n        </li>\n    </ul>\n    <img src=\"classification.png\" alt=\"\" height=\"300px\">\n    <h3>Training a Classification Model</h3>\n    <ul>\n        <li><b>Dataset</b>\n            <p>A labeled dataset is required, where each instance is associated with a class label</p>\n        </li>\n        <li><b>Feature Extraction</b>\n            <p>Features are extracted from the input data. These can be numeric or categorical variables</p>\n        </li>\n        <li><b>Model Training</b></li>\n        <p>The algorithm learns the mapping between features and class labels using the labeled training data</p>\n    </ul>\n```\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndf = pd.read_csv('creditcard.csv')\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndf.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>284807.000000</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>...</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>284807.000000</td>\n      <td>284807.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>94813.859575</td>\n      <td>1.168375e-15</td>\n      <td>3.416908e-16</td>\n      <td>-1.379537e-15</td>\n      <td>2.074095e-15</td>\n      <td>9.604066e-16</td>\n      <td>1.487313e-15</td>\n      <td>-5.556467e-16</td>\n      <td>1.213481e-16</td>\n      <td>-2.406331e-15</td>\n      <td>...</td>\n      <td>1.654067e-16</td>\n      <td>-3.568593e-16</td>\n      <td>2.578648e-16</td>\n      <td>4.473266e-15</td>\n      <td>5.340915e-16</td>\n      <td>1.683437e-15</td>\n      <td>-3.660091e-16</td>\n      <td>-1.227390e-16</td>\n      <td>88.349619</td>\n      <td>0.001727</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>47488.145955</td>\n      <td>1.958696e+00</td>\n      <td>1.651309e+00</td>\n      <td>1.516255e+00</td>\n      <td>1.415869e+00</td>\n      <td>1.380247e+00</td>\n      <td>1.332271e+00</td>\n      <td>1.237094e+00</td>\n      <td>1.194353e+00</td>\n      <td>1.098632e+00</td>\n      <td>...</td>\n      <td>7.345240e-01</td>\n      <td>7.257016e-01</td>\n      <td>6.244603e-01</td>\n      <td>6.056471e-01</td>\n      <td>5.212781e-01</td>\n      <td>4.822270e-01</td>\n      <td>4.036325e-01</td>\n      <td>3.300833e-01</td>\n      <td>250.120109</td>\n      <td>0.041527</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>-5.640751e+01</td>\n      <td>-7.271573e+01</td>\n      <td>-4.832559e+01</td>\n      <td>-5.683171e+00</td>\n      <td>-1.137433e+02</td>\n      <td>-2.616051e+01</td>\n      <td>-4.355724e+01</td>\n      <td>-7.321672e+01</td>\n      <td>-1.343407e+01</td>\n      <td>...</td>\n      <td>-3.483038e+01</td>\n      <td>-1.093314e+01</td>\n      <td>-4.480774e+01</td>\n      <td>-2.836627e+00</td>\n      <td>-1.029540e+01</td>\n      <td>-2.604551e+00</td>\n      <td>-2.256568e+01</td>\n      <td>-1.543008e+01</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>54201.500000</td>\n      <td>-9.203734e-01</td>\n      <td>-5.985499e-01</td>\n      <td>-8.903648e-01</td>\n      <td>-8.486401e-01</td>\n      <td>-6.915971e-01</td>\n      <td>-7.682956e-01</td>\n      <td>-5.540759e-01</td>\n      <td>-2.086297e-01</td>\n      <td>-6.430976e-01</td>\n      <td>...</td>\n      <td>-2.283949e-01</td>\n      <td>-5.423504e-01</td>\n      <td>-1.618463e-01</td>\n      <td>-3.545861e-01</td>\n      <td>-3.171451e-01</td>\n      <td>-3.269839e-01</td>\n      <td>-7.083953e-02</td>\n      <td>-5.295979e-02</td>\n      <td>5.600000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>84692.000000</td>\n      <td>1.810880e-02</td>\n      <td>6.548556e-02</td>\n      <td>1.798463e-01</td>\n      <td>-1.984653e-02</td>\n      <td>-5.433583e-02</td>\n      <td>-2.741871e-01</td>\n      <td>4.010308e-02</td>\n      <td>2.235804e-02</td>\n      <td>-5.142873e-02</td>\n      <td>...</td>\n      <td>-2.945017e-02</td>\n      <td>6.781943e-03</td>\n      <td>-1.119293e-02</td>\n      <td>4.097606e-02</td>\n      <td>1.659350e-02</td>\n      <td>-5.213911e-02</td>\n      <td>1.342146e-03</td>\n      <td>1.124383e-02</td>\n      <td>22.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>139320.500000</td>\n      <td>1.315642e+00</td>\n      <td>8.037239e-01</td>\n      <td>1.027196e+00</td>\n      <td>7.433413e-01</td>\n      <td>6.119264e-01</td>\n      <td>3.985649e-01</td>\n      <td>5.704361e-01</td>\n      <td>3.273459e-01</td>\n      <td>5.971390e-01</td>\n      <td>...</td>\n      <td>1.863772e-01</td>\n      <td>5.285536e-01</td>\n      <td>1.476421e-01</td>\n      <td>4.395266e-01</td>\n      <td>3.507156e-01</td>\n      <td>2.409522e-01</td>\n      <td>9.104512e-02</td>\n      <td>7.827995e-02</td>\n      <td>77.165000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>172792.000000</td>\n      <td>2.454930e+00</td>\n      <td>2.205773e+01</td>\n      <td>9.382558e+00</td>\n      <td>1.687534e+01</td>\n      <td>3.480167e+01</td>\n      <td>7.330163e+01</td>\n      <td>1.205895e+02</td>\n      <td>2.000721e+01</td>\n      <td>1.559499e+01</td>\n      <td>...</td>\n      <td>2.720284e+01</td>\n      <td>1.050309e+01</td>\n      <td>2.252841e+01</td>\n      <td>4.584549e+00</td>\n      <td>7.519589e+00</td>\n      <td>3.517346e+00</td>\n      <td>3.161220e+01</td>\n      <td>3.384781e+01</td>\n      <td>25691.160000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 31 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndf.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 284807 entries, 0 to 284806\nData columns (total 31 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   Time    284807 non-null  float64\n 1   V1      284807 non-null  float64\n 2   V2      284807 non-null  float64\n 3   V3      284807 non-null  float64\n 4   V4      284807 non-null  float64\n 5   V5      284807 non-null  float64\n 6   V6      284807 non-null  float64\n 7   V7      284807 non-null  float64\n 8   V8      284807 non-null  float64\n 9   V9      284807 non-null  float64\n 10  V10     284807 non-null  float64\n 11  V11     284807 non-null  float64\n 12  V12     284807 non-null  float64\n 13  V13     284807 non-null  float64\n 14  V14     284807 non-null  float64\n 15  V15     284807 non-null  float64\n 16  V16     284807 non-null  float64\n 17  V17     284807 non-null  float64\n 18  V18     284807 non-null  float64\n 19  V19     284807 non-null  float64\n 20  V20     284807 non-null  float64\n 21  V21     284807 non-null  float64\n 22  V22     284807 non-null  float64\n 23  V23     284807 non-null  float64\n 24  V24     284807 non-null  float64\n 25  V25     284807 non-null  float64\n 26  V26     284807 non-null  float64\n 27  V27     284807 non-null  float64\n 28  V28     284807 non-null  float64\n 29  Amount  284807 non-null  float64\n 30  Class   284807 non-null  int64  \ndtypes: float64(30), int64(1)\nmemory usage: 67.4 MB\n```\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndf.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n(284807, 31)\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndf.columns\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\nIndex(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n       'Class'],\n      dtype='object')\n```\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndf.dropna(inplace=True)\n```\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndf.isnull().sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\nTime      0\nV1        0\nV2        0\nV3        0\nV4        0\nV5        0\nV6        0\nV7        0\nV8        0\nV9        0\nV10       0\nV11       0\nV12       0\nV13       0\nV14       0\nV15       0\nV16       0\nV17       0\nV18       0\nV19       0\nV20       0\nV21       0\nV22       0\nV23       0\nV24       0\nV25       0\nV26       0\nV27       0\nV28       0\nAmount    0\nClass     0\ndtype: int64\n```\n:::\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\ndf['Class'].unique()\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\narray([0, 1], dtype=int64)\n```\n:::\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ncorrelation = df.corr()\nsns.heatmap(correlation)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n<Axes: >\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-2.png){width=566 height=438}\n:::\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nsns.countplot(x='Class', data=df)\nplt.xlabel('Class (0: Not Fraud, 1: Fraud)')\nplt.ylabel('Count')\nplt.title('Credit Card Fraud Detection - Class Distribution')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-2.png){width=618 height=449}\n:::\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 6))\nsns.histplot(x='Time', data=df, hue='Class', bins=30, kde=True)\nplt.xlabel('Transaction Time')\nplt.ylabel('Count')\nplt.title('Credit Card Fraud Detection - Transaction Time Distribution by Class')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-2.png){width=833 height=523}\n:::\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nplt.figure(figsize=(8, 6))\nsns.barplot(x='Class', y='Amount', data=df, palette='Purples')\nplt.xlabel('Class (0: Not Fraud, 1: Fraud)')\nplt.ylabel('Transaction Amount')\nplt.title('Distribution of Transaction Amounts by Class')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-2.png){width=667 height=523}\n:::\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nplt.figure(figsize=(8, 6))\nsns.barplot(x='Class', y='Time', data=df, palette='Purples')\nplt.xlabel('Class (0: Not Fraud, 1: Fraud)')\nplt.ylabel('Transaction Time')\nplt.title('Distribution of Transaction Times by Class')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-16-output-2.png){width=684 height=523}\n:::\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nplt.figure(figsize=(8, 6))\nsns.barplot(x='Class', y='Amount', data=df, estimator=np.mean, palette='Blues')\nplt.xlabel('Class (0: Not Fraud, 1: Fraud)')\nplt.ylabel('Average Transaction Amount')\nplt.title('Average Transaction Amount by Class')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-2.png){width=667 height=523}\n:::\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\ndf['Day_of_Week'] = pd.to_datetime(df['Time'], unit='s').dt.day_name()\nplt.figure(figsize=(12, 6))\nsns.countplot(x='Day_of_Week', data=df[df['Class'] == 1], order=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], palette='Greens')\nplt.xlabel('Day of the Week')\nplt.ylabel('Count of Fraudulent Transactions')\nplt.title('Count of Fraudulent Transactions by Day of the Week')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-18-output-2.png){width=965 height=523}\n:::\n:::\n\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nsns.stripplot(x='Class', y='Time', data=df, palette='viridis')\nplt.xlabel('Class (0: Not Fraud, 1: Fraud)')\nplt.ylabel('Transaction Time')\nplt.title('Distribution of Transaction Times by Class')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_17660\\78416919.py:1: FutureWarning:\n\nPassing `palette` without assigning `hue` is deprecated.\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-19-output-2.png){width=618 height=449}\n:::\n:::\n\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nsns.boxplot(x=df['Time'], palette='Blues')\nplt.title('Boxplot of Transaction Times')\nplt.xlabel('Transaction Time')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-20-output-2.png){width=552 height=449}\n:::\n:::\n\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nsns.lmplot(x='Amount', y='V25', data=df, hue='Class', palette='Set1', markers=['o', 's'], scatter_kws={'s': 50})\nplt.title('lmplot with Amount and V25')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-21-output-2.png){width=525 height=490}\n:::\n:::\n\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nsns.lmplot(x='Amount', y='V1', data=df, hue='Class', palette='Set1', markers=['o', 's'], scatter_kws={'s': 50})\nplt.title('lmplot with Amount and V1')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-22-output-2.png){width=526 height=490}\n:::\n:::\n\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\nX = df.drop(columns='Class')\ny = df['Class']\n```\n:::\n\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nX.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Day_of_Week</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>0.363787</td>\n      <td>...</td>\n      <td>-0.018307</td>\n      <td>0.277838</td>\n      <td>-0.110474</td>\n      <td>0.066928</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>149.62</td>\n      <td>Thursday</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>-0.255425</td>\n      <td>...</td>\n      <td>-0.225775</td>\n      <td>-0.638672</td>\n      <td>0.101288</td>\n      <td>-0.339846</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>2.69</td>\n      <td>Thursday</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>-1.514654</td>\n      <td>...</td>\n      <td>0.247998</td>\n      <td>0.771679</td>\n      <td>0.909412</td>\n      <td>-0.689281</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>378.66</td>\n      <td>Thursday</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>-1.387024</td>\n      <td>...</td>\n      <td>-0.108300</td>\n      <td>0.005274</td>\n      <td>-0.190321</td>\n      <td>-1.175575</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>123.50</td>\n      <td>Thursday</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>0.817739</td>\n      <td>...</td>\n      <td>-0.009431</td>\n      <td>0.798278</td>\n      <td>-0.137458</td>\n      <td>0.141267</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>69.99</td>\n      <td>Thursday</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n```\n:::\n\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\nX_train_numeric = X_train.select_dtypes(exclude=['object'])\nscaler = StandardScaler()\nscaler.fit(X_train_numeric)\nX_train_scaled = scaler.transform(X_train_numeric)\nX_train_scaled\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\narray([[ 0.4628655 , -0.76417848, -0.58517942, ..., -0.05427856,\n         0.47278134,  0.27637606],\n       [ 0.99884641, -0.43199832,  0.8362486 , ..., -0.21155863,\n        -0.17561255, -0.20845219],\n       [-1.06243719, -0.5473776 ,  0.36358059, ..., -0.1751781 ,\n         0.27717173, -0.30005801],\n       ...,\n       [-0.31423311, -0.07216301,  0.59345235, ..., -0.29402614,\n        -0.59027941, -0.32887389],\n       [-0.1428877 , -1.49506753,  1.40403542, ...,  1.21908694,\n         1.01135271, -0.34027614],\n       [-0.38613248,  0.62850772, -0.46466388, ...,  0.00552523,\n         0.11653329,  0.09409522]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\nX_train_encoded = pd.get_dummies(X_train)\nscaler.fit(X_train_encoded)\nX_train_scaled = scaler.transform(X_train_encoded)\nX_train_scaled\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\narray([[ 0.4628655 , -0.76417848, -0.58517942, ...,  0.27637606,\n         1.0191673 , -1.0191673 ],\n       [ 0.99884641, -0.43199832,  0.8362486 , ..., -0.20845219,\n         1.0191673 , -1.0191673 ],\n       [-1.06243719, -0.5473776 ,  0.36358059, ..., -0.30005801,\n        -0.98119318,  0.98119318],\n       ...,\n       [-0.31423311, -0.07216301,  0.59345235, ..., -0.32887389,\n        -0.98119318,  0.98119318],\n       [-0.1428877 , -1.49506753,  1.40403542, ..., -0.34027614,\n         1.0191673 , -1.0191673 ],\n       [-0.38613248,  0.62850772, -0.46466388, ...,  0.09409522,\n        -0.98119318,  0.98119318]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport pandas as pd\nclf = LogisticRegression(random_state=0, solver='sag', max_iter=1000)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\nnumeric_features = X.select_dtypes(include=['float64']).columns\ncategorical_features = X.select_dtypes(include=['object']).columns\nnumeric_transformer = Pipeline(steps=[\n    ('scaler', StandardScaler())\n])\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000))])\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(f'Accuracy: {accuracy:.4f}')\nprint('\\nConfusion Matrix:')\nprint(conf_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.9992\n\nConfusion Matrix:\n[[113718     14]\n [    74    117]]\n```\n:::\n:::\n\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\nclf.score(X_test, y_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```\n0.9992275484318356\n```\n:::\n:::\n\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\nclf.score(X_train, y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\n0.9991982865569626\n```\n:::\n:::\n\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\nfrom sklearn.metrics import classification_report\ntarget_names = ['not_fraud', 'fraud']\nprint(classification_report(y_test, y_pred, target_names=target_names))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              precision    recall  f1-score   support\n\n   not_fraud       1.00      1.00      1.00    113732\n       fraud       0.89      0.61      0.73       191\n\n    accuracy                           1.00    113923\n   macro avg       0.95      0.81      0.86    113923\nweighted avg       1.00      1.00      1.00    113923\n\n```\n:::\n:::\n\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\nfrom sklearn.metrics import log_loss\ny_pred_proba = clf.predict_proba(X_test)[:, 1]\nloss = log_loss(y_test, y_pred_proba)\nprint(f'Log Loss: {loss:.4f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLog Loss: 0.0038\n```\n:::\n:::\n\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.naive_bayes import GaussianNB\nX_train_numeric, X_test_numeric, y_train, y_test = train_test_split(X_train_numeric, y_train, test_size=0.4, random_state=42)\nscaler = StandardScaler()\nX_train_scaled_numeric = scaler.fit_transform(X_train_numeric)\nX_test_scaled_numeric = scaler.transform(X_test_numeric)\nclf = GaussianNB()\nclf.fit(X_train_scaled_numeric, y_train)\ny_pred = clf.predict(X_test_scaled_numeric)\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclassification_report_str = classification_report(y_test, y_pred)\nprint(f'Accuracy: {accuracy:.4f}')\nprint('\\nConfusion Matrix:')\nprint(conf_matrix)\nprint('\\nClassification Report:')\nprint(classification_report_str)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.9774\n\nConfusion Matrix:\n[[66706  1530]\n [   15   103]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      0.98      0.99     68236\n           1       0.06      0.87      0.12       118\n\n    accuracy                           0.98     68354\n   macro avg       0.53      0.93      0.55     68354\nweighted avg       1.00      0.98      0.99     68354\n\n```\n:::\n:::\n\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport pandas as pd\nX_train_numeric, X_test_numeric, y_train, y_test = train_test_split(X_train_numeric, y_train, test_size=0.4, random_state=42)\nscaler = StandardScaler()\nX_train_scaled_numeric = scaler.fit_transform(X_train_numeric)\nX_test_scaled_numeric = scaler.transform(X_test_numeric)\ndt_classifier = DecisionTreeClassifier(random_state=0)\ndt_classifier.fit(X_train_scaled_numeric, y_train)\ny_pred_dt = dt_classifier.predict(X_test_scaled_numeric)\naccuracy_dt = accuracy_score(y_test, y_pred_dt)\nconf_matrix_dt = confusion_matrix(y_test, y_pred_dt)\nclassification_report_dt = classification_report(y_test, y_pred_dt)\nprint(f'Decision Tree Classifier:')\nprint(f'Accuracy: {accuracy_dt:.4f}')\nprint('\\nConfusion Matrix:')\nprint(conf_matrix_dt)\nprint('\\nClassification Report:')\nprint(classification_report_dt)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDecision Tree Classifier:\nAccuracy: 0.9987\n\nConfusion Matrix:\n[[40919    34]\n [   20    39]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     40953\n           1       0.53      0.66      0.59        59\n\n    accuracy                           1.00     41012\n   macro avg       0.77      0.83      0.80     41012\nweighted avg       1.00      1.00      1.00     41012\n\n```\n:::\n:::\n\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\nfrom sklearn.svm import SVC\nX_train_numeric, X_test_numeric, y_train, y_test = train_test_split(X_train_numeric, y_train, test_size=0.4, random_state=42)\nscaler = StandardScaler()\nX_train_scaled_numeric = scaler.fit_transform(X_train_numeric)\nX_test_scaled_numeric = scaler.transform(X_test_numeric)\nsvm_classifier = SVC(random_state=0)\nsvm_classifier.fit(X_train_scaled_numeric, y_train)\ny_pred_svm = svm_classifier.predict(X_test_scaled_numeric)\naccuracy_svm = accuracy_score(y_test, y_pred_svm)\nconf_matrix_svm = confusion_matrix(y_test, y_pred_svm)\nclassification_report_svm = classification_report(y_test, y_pred_svm)\nprint(f'SVM Classifier:')\nprint(f'Accuracy: {accuracy_svm:.4f}')\nprint('\\nConfusion Matrix:')\nprint(conf_matrix_svm)\nprint('\\nClassification Report:')\nprint(classification_report_svm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSVM Classifier:\nAccuracy: 0.9990\n\nConfusion Matrix:\n[[24568     0]\n [   25    15]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     24568\n           1       1.00      0.38      0.55        40\n\n    accuracy                           1.00     24608\n   macro avg       1.00      0.69      0.77     24608\nweighted avg       1.00      1.00      1.00     24608\n\n```\n:::\n:::\n\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\nfrom sklearn.neighbors import KNeighborsClassifier\nX_train_numeric, X_test_numeric, y_train, y_test = train_test_split(X_train_numeric, y_train, test_size=0.4, random_state=42)\nscaler = StandardScaler()\nX_train_scaled_numeric = scaler.fit_transform(X_train_numeric)\nX_test_scaled_numeric = scaler.transform(X_test_numeric)\nknn_classifier = KNeighborsClassifier()\nknn_classifier.fit(X_train_scaled_numeric, y_train)\ny_pred_knn = knn_classifier.predict(X_test_scaled_numeric)\naccuracy_knn = accuracy_score(y_test, y_pred_knn)\nconf_matrix_knn = confusion_matrix(y_test, y_pred_knn)\nclassification_report_knn = classification_report(y_test, y_pred_knn)\nprint(f'K-Nearest Neighbors Classifier:')\nprint(f'Accuracy: {accuracy_knn:.4f}')\nprint('\\nConfusion Matrix:')\nprint(conf_matrix_knn)\nprint('\\nClassification Report:')\nprint(classification_report_knn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nK-Nearest Neighbors Classifier:\nAccuracy: 0.9988\n\nConfusion Matrix:\n[[14725     3]\n [   15    21]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     14728\n           1       0.88      0.58      0.70        36\n\n    accuracy                           1.00     14764\n   macro avg       0.94      0.79      0.85     14764\nweighted avg       1.00      1.00      1.00     14764\n\n```\n:::\n:::\n\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Dense,Activation,Flatten\nfrom tensorflow.keras import Sequential\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n\n```\n:::\n:::\n\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\nmodel = Sequential()\nmodel.add(Flatten(input_shape=(X_train_scaled_numeric.shape[1],)))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nearlystop = EarlyStopping(monitor='val_loss', patience=2, verbose=0, mode='min')\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.fit(X_train_scaled_numeric, y_train, epochs=10, validation_data=(X_test_scaled_numeric, y_test), callbacks=[earlystop])\ny_pred_probs = model.predict(X_test_scaled_numeric)\ny_pred_nn = (y_pred_probs > 0.5).astype(int)  \naccuracy_nn = accuracy_score(y_test, y_pred_nn)\nconf_matrix_nn = confusion_matrix(y_test, y_pred_nn)\nclassification_report_nn = classification_report(y_test, y_pred_nn)\nprint(f'Neural Network Classifier:')\nprint(f'Accuracy: {accuracy_nn:.4f}')\nprint('\\nConfusion Matrix:')\nprint(conf_matrix_nn)\nprint('\\nClassification Report:')\nprint(classification_report_nn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nWARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\nEpoch 1/10\nWARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n\nWARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n\n\r  1/693 [..............................] - ETA: 8:09 - loss: 0.6898 - accuracy: 0.4375\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 48/693 [=>............................] - ETA: 0s - loss: 0.4806 - accuracy: 0.8477  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/693 [===>..........................] - ETA: 0s - loss: 0.3245 - accuracy: 0.9262\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r127/693 [====>.........................] - ETA: 0s - loss: 0.2715 - accuracy: 0.9417\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r172/693 [======>.......................] - ETA: 0s - loss: 0.2149 - accuracy: 0.9564\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r225/693 [========>.....................] - ETA: 0s - loss: 0.1730 - accuracy: 0.9663\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r273/693 [==========>...................] - ETA: 0s - loss: 0.1466 - accuracy: 0.9718\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r306/693 [============>.................] - ETA: 0s - loss: 0.1348 - accuracy: 0.9745\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r350/693 [==============>...............] - ETA: 0s - loss: 0.1187 - accuracy: 0.9777\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r404/693 [================>.............] - ETA: 0s - loss: 0.1041 - accuracy: 0.9804\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r454/693 [==================>...........] - ETA: 0s - loss: 0.0931 - accuracy: 0.9825\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r501/693 [====================>.........] - ETA: 0s - loss: 0.0852 - accuracy: 0.9839\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r552/693 [======================>.......] - ETA: 0s - loss: 0.0783 - accuracy: 0.9853\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r607/693 [=========================>....] - ETA: 0s - loss: 0.0719 - accuracy: 0.9865\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r662/693 [===========================>..] - ETA: 0s - loss: 0.0661 - accuracy: 0.9876\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r693/693 [==============================] - 2s 2ms/step - loss: 0.0637 - accuracy: 0.9881 - val_loss: 0.0086 - val_accuracy: 0.9988\nEpoch 2/10\n\r  1/693 [..............................] - ETA: 1s - loss: 0.0016 - accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 60/693 [=>............................] - ETA: 0s - loss: 0.0040 - accuracy: 0.9995\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r118/693 [====>.........................] - ETA: 0s - loss: 0.0027 - accuracy: 0.9997\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r181/693 [======>.......................] - ETA: 0s - loss: 0.0039 - accuracy: 0.9995\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r239/693 [=========>....................] - ETA: 0s - loss: 0.0038 - accuracy: 0.9992\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r296/693 [===========>..................] - ETA: 0s - loss: 0.0041 - accuracy: 0.9990\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r352/693 [==============>...............] - ETA: 0s - loss: 0.0037 - accuracy: 0.9991\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r410/693 [================>.............] - ETA: 0s - loss: 0.0043 - accuracy: 0.9991\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r468/693 [===================>..........] - ETA: 0s - loss: 0.0047 - accuracy: 0.9991\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r527/693 [=====================>........] - ETA: 0s - loss: 0.0054 - accuracy: 0.9989\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r585/693 [========================>.....] - ETA: 0s - loss: 0.0060 - accuracy: 0.9988\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r642/693 [==========================>...] - ETA: 0s - loss: 0.0056 - accuracy: 0.9989\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r693/693 [==============================] - 1s 1ms/step - loss: 0.0055 - accuracy: 0.9989 - val_loss: 0.0074 - val_accuracy: 0.9988\nEpoch 3/10\n\r  1/693 [..............................] - ETA: 0s - loss: 2.7365e-04 - accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 64/693 [=>............................] - ETA: 0s - loss: 0.0033 - accuracy: 0.9995    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r125/693 [====>.........................] - ETA: 0s - loss: 0.0019 - accuracy: 0.9998\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r186/693 [=======>......................] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r244/693 [=========>....................] - ETA: 0s - loss: 0.0021 - accuracy: 0.9995\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r304/693 [============>.................] - ETA: 0s - loss: 0.0020 - accuracy: 0.9995\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r366/693 [==============>...............] - ETA: 0s - loss: 0.0020 - accuracy: 0.9994\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r427/693 [=================>............] - ETA: 0s - loss: 0.0021 - accuracy: 0.9993\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r463/693 [===================>..........] - ETA: 0s - loss: 0.0021 - accuracy: 0.9993\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r529/693 [=====================>........] - ETA: 0s - loss: 0.0025 - accuracy: 0.9993\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r593/693 [========================>.....] - ETA: 0s - loss: 0.0026 - accuracy: 0.9993\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r657/693 [===========================>..] - ETA: 0s - loss: 0.0032 - accuracy: 0.9992\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r693/693 [==============================] - 1s 1ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.0081 - val_accuracy: 0.9985\nEpoch 4/10\n\r  1/693 [..............................] - ETA: 0s - loss: 2.7800e-04 - accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 64/693 [=>............................] - ETA: 0s - loss: 0.0025 - accuracy: 0.9985    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r127/693 [====>.........................] - ETA: 0s - loss: 0.0020 - accuracy: 0.9988\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r193/693 [=======>......................] - ETA: 0s - loss: 0.0022 - accuracy: 0.9990\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/693 [==========>...................] - ETA: 0s - loss: 0.0036 - accuracy: 0.9990\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r321/693 [============>.................] - ETA: 0s - loss: 0.0035 - accuracy: 0.9989\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/693 [===============>..............] - ETA: 0s - loss: 0.0030 - accuracy: 0.9991\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r448/693 [==================>...........] - ETA: 0s - loss: 0.0031 - accuracy: 0.9991\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r513/693 [=====================>........] - ETA: 0s - loss: 0.0033 - accuracy: 0.9990\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r579/693 [========================>.....] - ETA: 0s - loss: 0.0032 - accuracy: 0.9990\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r646/693 [==========================>...] - ETA: 0s - loss: 0.0030 - accuracy: 0.9991\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r693/693 [==============================] - 1s 1ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0080 - val_accuracy: 0.9988\n\r  1/462 [..............................] - ETA: 28s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 78/462 [====>.........................] - ETA: 0s \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r155/462 [=========>....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r235/462 [==============>...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r311/462 [===================>..........] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r389/462 [========================>.....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r462/462 [==============================] - 0s 647us/step\nNeural Network Classifier:\nAccuracy: 0.9988\n\nConfusion Matrix:\n[[14722     6]\n [   12    24]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     14728\n           1       0.80      0.67      0.73        36\n\n    accuracy                           1.00     14764\n   macro avg       0.90      0.83      0.86     14764\nweighted avg       1.00      1.00      1.00     14764\n\n```\n:::\n:::\n\n\n::: {.cell execution_count=38}\n``` {.python .cell-code}\nhistory = model.fit(X_train_scaled_numeric, y_train, epochs=10, validation_data=(X_test_scaled_numeric, y_test), callbacks=[earlystop])\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(['Train', 'Validation'], loc='upper right')\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['Train', 'Validation'], loc='lower right')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 1/10\n\r  1/693 [..............................] - ETA: 2s - loss: 0.0012 - accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 63/693 [=>............................] - ETA: 0s - loss: 0.0014 - accuracy: 0.9995\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r128/693 [====>.........................] - ETA: 0s - loss: 0.0029 - accuracy: 0.9993\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r193/693 [=======>......................] - ETA: 0s - loss: 0.0037 - accuracy: 0.9990\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r258/693 [==========>...................] - ETA: 0s - loss: 0.0028 - accuracy: 0.9993\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r323/693 [============>.................] - ETA: 0s - loss: 0.0024 - accuracy: 0.9994\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r387/693 [===============>..............] - ETA: 0s - loss: 0.0021 - accuracy: 0.9994\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r451/693 [==================>...........] - ETA: 0s - loss: 0.0023 - accuracy: 0.9994\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r513/693 [=====================>........] - ETA: 0s - loss: 0.0025 - accuracy: 0.9993\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r573/693 [=======================>......] - ETA: 0s - loss: 0.0026 - accuracy: 0.9993\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r633/693 [==========================>...] - ETA: 0s - loss: 0.0026 - accuracy: 0.9993\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r693/693 [==============================] - 1s 1ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0090 - val_accuracy: 0.9986\nEpoch 2/10\n\r  1/693 [..............................] - ETA: 0s - loss: 1.5978e-04 - accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 65/693 [=>............................] - ETA: 0s - loss: 0.0018 - accuracy: 0.9986    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r128/693 [====>.........................] - ETA: 0s - loss: 0.0016 - accuracy: 0.9990\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r194/693 [=======>......................] - ETA: 0s - loss: 0.0016 - accuracy: 0.9992\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r259/693 [==========>...................] - ETA: 0s - loss: 0.0013 - accuracy: 0.9994\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r325/693 [=============>................] - ETA: 0s - loss: 0.0013 - accuracy: 0.9994\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r389/693 [===============>..............] - ETA: 0s - loss: 0.0013 - accuracy: 0.9994\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r455/693 [==================>...........] - ETA: 0s - loss: 0.0016 - accuracy: 0.9995\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r520/693 [=====================>........] - ETA: 0s - loss: 0.0020 - accuracy: 0.9992\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r584/693 [========================>.....] - ETA: 0s - loss: 0.0024 - accuracy: 0.9990\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r648/693 [===========================>..] - ETA: 0s - loss: 0.0023 - accuracy: 0.9991\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r693/693 [==============================] - 1s 1ms/step - loss: 0.0022 - accuracy: 0.9991 - val_loss: 0.0095 - val_accuracy: 0.9988\nEpoch 3/10\n\r  1/693 [..............................] - ETA: 1s - loss: 6.3921e-05 - accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 60/693 [=>............................] - ETA: 0s - loss: 0.0026 - accuracy: 0.9990    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r120/693 [====>.........................] - ETA: 0s - loss: 0.0017 - accuracy: 0.9995\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r178/693 [======>.......................] - ETA: 0s - loss: 0.0017 - accuracy: 0.9995\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r243/693 [=========>....................] - ETA: 0s - loss: 0.0013 - accuracy: 0.9996\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r308/693 [============>.................] - ETA: 0s - loss: 0.0018 - accuracy: 0.9996\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r373/693 [===============>..............] - ETA: 0s - loss: 0.0016 - accuracy: 0.9996\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r438/693 [=================>............] - ETA: 0s - loss: 0.0019 - accuracy: 0.9994\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r502/693 [====================>.........] - ETA: 0s - loss: 0.0017 - accuracy: 0.9995\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r567/693 [=======================>......] - ETA: 0s - loss: 0.0016 - accuracy: 0.9996\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r632/693 [==========================>...] - ETA: 0s - loss: 0.0019 - accuracy: 0.9995\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r693/693 [==============================] - 1s 1ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.0099 - val_accuracy: 0.9988\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-39-output-2.png){width=1142 height=566}\n:::\n:::\n\n\n::: {.cell execution_count=39}\n``` {.python .cell-code}\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport pandas as pd\n\nX, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_train, y_train)\ny_train_series = pd.Series(y_train)\nprint(\"Original Training Set Class Distribution:\")\nprint(y_train_series.value_counts())\nprint(\"\\nResampled Training Set Class Distribution:\")\nprint(pd.Series(y_resampled).value_counts())\nprint(\"\\nTest Set Class Distribution:\")\nprint(pd.Series(y_test).value_counts())\nscaler = StandardScaler()\nX_resampled_scaled = scaler.fit_transform(X_resampled)\nX_test_scaled = scaler.transform(X_test)\nknn_classifier = KNeighborsClassifier()\nknn_classifier.fit(X_resampled_scaled, y_resampled)\ny_pred_knn = knn_classifier.predict(X_test_scaled)\naccuracy_knn = accuracy_score(y_test, y_pred_knn)\nconf_matrix_knn = confusion_matrix(y_test, y_pred_knn)\nclassification_report_knn = classification_report(y_test, y_pred_knn)\nprint(f'\\nK-Nearest Neighbors Classifier:')\nprint(f'Accuracy: {accuracy_knn:.4f}')\nprint('\\nConfusion Matrix:')\nprint(conf_matrix_knn)\nprint('\\nClassification Report:')\nprint(classification_report_knn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal Training Set Class Distribution:\n1    627\n0     73\nName: count, dtype: int64\n\nResampled Training Set Class Distribution:\n1    627\n0    627\nName: count, dtype: int64\n\nTest Set Class Distribution:\n1    273\n0     27\nName: count, dtype: int64\n\nK-Nearest Neighbors Classifier:\nAccuracy: 0.9267\n\nConfusion Matrix:\n[[ 24   3]\n [ 19 254]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.56      0.89      0.69        27\n           1       0.99      0.93      0.96       273\n\n    accuracy                           0.93       300\n   macro avg       0.77      0.91      0.82       300\nweighted avg       0.95      0.93      0.93       300\n\n```\n:::\n:::\n\n\n::: {.cell execution_count=40}\n``` {.python .cell-code}\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport pandas as pd\n\nX, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X_train, y_train)\ny_resampled_series = pd.Series(y_resampled)\nprint(\"Resampled Training Set Class Distribution:\")\nprint(y_resampled_series.value_counts())\nscaler = StandardScaler()\nX_resampled_scaled = scaler.fit_transform(X_resampled)\nX_test_scaled = scaler.transform(X_test)\nknn_classifier = KNeighborsClassifier()\nknn_classifier.fit(X_resampled_scaled, y_resampled)\ny_pred_knn = knn_classifier.predict(X_test_scaled)\naccuracy_knn = accuracy_score(y_test, y_pred_knn)\nconf_matrix_knn = confusion_matrix(y_test, y_pred_knn)\nclassification_report_knn = classification_report(y_test, y_pred_knn)\nprint(f'\\nK-Nearest Neighbors Classifier:')\nprint(f'Accuracy: {accuracy_knn:.4f}')\nprint('\\nConfusion Matrix:')\nprint(conf_matrix_knn)\nprint('\\nClassification Report:')\nprint(classification_report_knn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResampled Training Set Class Distribution:\n0    73\n1    73\nName: count, dtype: int64\n\nK-Nearest Neighbors Classifier:\nAccuracy: 0.9200\n\nConfusion Matrix:\n[[ 27   0]\n [ 24 249]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.53      1.00      0.69        27\n           1       1.00      0.91      0.95       273\n\n    accuracy                           0.92       300\n   macro avg       0.76      0.96      0.82       300\nweighted avg       0.96      0.92      0.93       300\n\n```\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}