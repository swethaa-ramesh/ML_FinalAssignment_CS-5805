---
title: "Linear and nonlinear regression"
date: "2023-11-19"
image: "Image.png"
author: Swethaa
---


```{=html}
<h3>a)Linear Regression
</h3> 
    <h3>Index</h3>
    <ul>
        <li>What is Linear Regression</li>
        <li>Types of Linear Regression</li>
        <li>Training a Linear Regression Model</li>
        <li>Finding the best fit line</li>
    </ul>
    <h3>What is Linear Regression</h3>
    <p>One of the simplest and most widely used machine learning algorithms is linear regression. It's a statistical technique for forecasting analysis. For continuous, real, or numerical variables like sales, salary, age, product price, etc., linear regression generates predictions.</p>
    <p>Mathematically, we can represent a linear regression as:</p>
    <b>y= a0+a1x+ ε</b>
    <ul>
        <li>Y= Dependent Variable (Target Variable)</li>
        <li>X= Independent Variable (predictor Variable)</li>
        <li>a0= intercept of the line (Gives an additional degree of freedom)</li>
        <li>a1 = Linear regression coefficient (scale factor to each input value).</li>
        <li>ε = random error</li>
    </ul>
    <p></p> The values for x and y variables are training datasets for Linear Regression model representation.</p>
    <h3>Types of Linear Regression</h3>
    <ul>
        <li> <b>Simple Linear Regression</b>
            <p>A linear regression algorithm is referred to as simple linear regression if it uses one independent variable to predict the value of a numerical dependent variable.</p>
            <h4> <b>Y=β 0+β 1⋅X+ε</b></h4>
            <ul>
                <li>
                    Y is the dependent variable.
                </li>
                <li>X is the independent variable.</li>
                <li>β 0is the intercept (where the line crosses theY-axis).</li>
                <li>β 1 is the slop (the change in Y for a one-unitchange in X).</li>
                <li>ε is the error term, representing the variability in Y that is not explained by the linear relationship withX</li>
            </ul>
        </li>
        <li> <b>Multiple Linear Regression</b>
            <p>A type of linear regression algorithm known as multiple linear regression is used to predict the value of a numerical dependent variable by utilizing multiple independent variables.</p>
            <h4> <b>Y=β 0+β 1⋅X 1 +β 2⋅X 2 +…+β n⋅X n+ε </b></h4>
            <ul>
                <li>
                    Y is the dependent variable.
                </li>
                <li>X 1,X 2,…,Xn are the independent variables</li>
                <li>β 0is the intercept </li>
                <li>β1,β 2,…,β nare the coefficients representing the impact of each independent variable.</li>
                <li>ε is the error term</li>
            </ul>
        </li>
    </ul>
    <h3>Training a Linear Regression Model</h3>
    <p>Finding the values of the coefficients (β0,β1,...,βn) that minimize the sum of squared differences between the predicted and actual values in the training data is the first step in training a linear regression model.</p>
    <h3>Finding the best fit line</h3>
    <p>The best fit line should be found in order to minimize the error between the predicted and actual values. There will be the least error in the best fit line. We use the cost function to calculate the best values for a0 and a1 in order to find the best fit line because different weights or coefficients of lines (a0, a1) result in different regression lines.</p>
    <h3>Continuous random variable</h3>
    <p>A continuous random variable has two main characteristics:</p>
    <p>Since there is no countable range of possible values, we integrate a function known as the probability density function to find the likelihood that a given value will fall within a given interval.</p>
    <h3>Common continuous distributions</h3>
    <p>Some examples of continuous distributions that are commonly used in statistics and probability theory can be found in the following table.</p>
    <table>
        <tr>
            <th>Name of the continuous distribution</th>
            <th>Support</th>
        </tr>
        <tr>
            <td>Uniform</td>
            <td>All the real numbers in the interval [0,1]</td>
        </tr>
         <tr>
            <td>Normal</td>
            <td>The whole set of real numbers</td>
        </tr>
         <tr>
            <td>Chi-square</td>
            <td>The set of all non-negative real numbers</td>
        </tr>
    </table>
    <img src="Image2.png" alt="" height="300px" width="300px">
```
```{python}
from sklearn.ensemble import GradientBoostingRegressor
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, GridSearchCV
import warnings
import seaborn as sns
import matplotlib.pyplot as plt
warnings.filterwarnings('ignore')
```
```{python}
df = pd.read_csv('CarPrice_Assignment.csv')
df.head()
```
```{python}
df.columns
```
```{python}
df.info()
```
```{python}
df.describe()
```
```{python}
df.shape
```
```{python}
df.isnull()
```
```{python}
sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')
```
```{python}
df.dropna(inplace =True)
df.isnull().sum()
```
```{python}
df.duplicated().any()
```
```{python}
df.describe(include=object)
```
```{python}
df = df.select_dtypes(include=['float64', 'int64'])
df.corr()
```
```{python}
correlation_matrix = df[['carlength', 'carwidth', 'curbweight']].corr()
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix,  cmap='coolwarm', center=0)
for i in range(len(correlation_matrix)):
    for j in range(len(correlation_matrix)):
        plt.text(j + 0.5, i + 0.5, f"{correlation_matrix.iloc[i, j]:.2f}", ha='center', va='center', fontsize=10)
plt.title('Correlation Matrix')
plt.show()
```
```{python}
df = df.drop(['symboling', 'car_ID', ], axis=1)
print(df.head())
```
```{python}
df.shape
```
```{python}
df.columns
```
```{python}
X = df.drop('price', axis=1)
y = df['price']
```
```{python}
X.head()
```
```{python}
y
```
```{python}
plt.figure(figsize=(8, 6))
sns.heatmap(df.corr(), cmap='coolwarm', fmt=".2f", linewidths=.5)
for i in range(len(df.corr())):
    for j in range(len(df.corr())):
        plt.text(j + 0.5, i + 0.5, f"{df.corr().iloc[i, j]:.2f}", ha='center', va='center', fontsize=10)
plt.title('Correlation Heatmap')
plt.show()
```

```{python}
sns.pairplot(df)
```

```{python}
plt.figure(figsize=(8, 6))
sns.histplot(df['enginesize'], kde=True, color='skyblue')
plt.title('Distribution of Engine Size')
plt.show()
```

```{python}
plt.figure(figsize=(8, 6))
sns.histplot(df['wheelbase'], kde=True)
plt.title('Distribution of Wheelbase')
plt.show()
```

```{python}
plt.figure(figsize=(8, 6))
sns.histplot(df['carlength'], kde=True)
plt.title('Distribution of Car Length')
plt.show()
```

```{python}

plt.figure(figsize=(8, 6))
sns.histplot(df['carlength'], kde=True)
plt.title('Distribution of Car Length')
plt.show()
```

```{python}
plt.figure(figsize=(8, 6))
sns.histplot(df['carheight'], kde=True)
plt.title('Distribution of Car Height')
plt.show()
```

```{python}
plt.figure(figsize=(8, 6))
sns.histplot(df['curbweight'], kde=True)
plt.title('Distribution of Curb Weight')
plt.show()
```

```{python}
plt.scatter(df['enginesize'], df['price'])
plt.xlabel('Engine Size')
plt.ylabel('Price')
plt.title('Scatter Plot between Engine Size and Price')
plt.show()
```
```{python}
sns.scatterplot(x='horsepower', y='price', data=df)
plt.title('Horsepower vs. Price (Scatter Plot)')
plt.show()
```
```{python}
sns.histplot(df['price'], kde=True, color='skyblue')
plt.title('Distribution of Car Prices with KDE')
plt.show()
```

```{python}
sns.histplot(df['citympg'], kde=True, bins=20, color='skyblue')
plt.title('Distribution of City MPG')
plt.xlabel('City MPG')
plt.show()
```



```{python}
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=42)
```

```{python}
sns.regplot(x='wheelbase', y='price', data=df, scatter_kws={'s': 20}, line_kws={'color': 'red'})
plt.title('Wheelbase vs. Price')
plt.xlabel('Wheelbase')
plt.ylabel('Price')
plt.show()
```

```{python}
sns.regplot(x='carlength', y='price', data=df, scatter_kws={'s': 20}, line_kws={'color': 'red'})
plt.title('Car Length vs. Price')
plt.xlabel('Car Length')
plt.ylabel('Price')
plt.show()
```

```{python}
sns.regplot(x='curbweight', y='price', data=df, scatter_kws={'s': 20}, line_kws={'color': 'red'})
plt.title('Curb Weight vs. Price')
plt.xlabel('Curb Weight')
plt.ylabel('Price')
plt.show()
```

```{python}
sns.regplot(x='enginesize', y='price', data=df, scatter_kws={'s': 20}, line_kws={'color': 'red'})
plt.title('Engine Size vs. Price')
plt.xlabel('Engine Size')
plt.ylabel('Price')
plt.show()
```

```{python}
sns.regplot(x='horsepower', y='price', data=df, scatter_kws={'s': 20}, line_kws={'color': 'red'})
plt.title('Horsepower vs. Price')
plt.xlabel('Horsepower')
plt.ylabel('Price')
plt.show()
```

```{python}
sns.regplot(x='citympg', y='price', data=df, scatter_kws={'s': 20}, line_kws={'color': 'red'})
plt.title('City MPG vs. Price')
plt.xlabel('City MPG')
plt.ylabel('Price')
plt.show()
```

```{python}
print(X_train.columns)
```
```{python}
import pandas as pd
categorical_cols = ['enginesize', 'horsepower', 'peakrpm']
X_train_encoded = pd.get_dummies(X_train, columns=categorical_cols)
X_test_encoded = pd.get_dummies(X_test, columns=categorical_cols)
```

```{python}
for column in X_train.columns:
    unique_values = X_train[column].nunique()
    print(f"Column '{column}' has {unique_values} unique values.")
```

```{python}
from sklearn.linear_model import LinearRegression
regression=LinearRegression()
```
```{python}
for col in categorical_cols:
    print(f"Unique values in '{col}': {X_train[col].unique()}")
```

```{python}
print(X_train.dtypes)
```
```{python}
X_train_encoded = pd.get_dummies(X_train, columns=categorical_cols)
```

```{python}
X_train_encoded = pd.get_dummies(X_train, columns=categorical_cols)
X_test_encoded = pd.get_dummies(X_test, columns=categorical_cols)
```

```{python}
non_numeric_cols_train = X_train_encoded.select_dtypes(exclude=['float64', 'int64']).columns
non_numeric_cols_test = X_test_encoded.select_dtypes(exclude=['float64', 'int64']).columns
print("Non-numeric columns in X_train_encoded:", non_numeric_cols_train)
print("Non-numeric columns in X_test_encoded:", non_numeric_cols_test)
```

```{python}
from sklearn.preprocessing import LabelEncoder
label_encoder_train = LabelEncoder()
for col in non_numeric_cols_train:
    X_train_encoded[col] = label_encoder_train.fit_transform(X_train_encoded[col])

label_encoder_test = LabelEncoder()
for col in non_numeric_cols_test:
    X_test_encoded[col] = label_encoder_test.fit_transform(X_test_encoded[col])
```

```{python}
for col in non_numeric_cols_train:
    print(f"Unique values in {col}: {X_train_encoded[col].unique()}")
```

```{python}
for col in non_numeric_cols_test:
    print(f"Unique values in {col}: {X_test_encoded[col].unique()}")
```

```{python}
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
label_encoder = LabelEncoder()

for col in categorical_cols:
    X_train[col] = label_encoder.fit_transform(X_train[col])

onehot_encoder = OneHotEncoder(drop='first', sparse=False)

X_train_encoded = pd.get_dummies(X_train, columns=categorical_cols, drop_first=True)
X_test_encoded = pd.get_dummies(X_test, columns=categorical_cols, drop_first=True)

X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='outer', axis=1, fill_value=0)

regression.fit(X_train_encoded, y_train)
```

```{python}
from sklearn.model_selection import cross_val_score
validation_score=cross_val_score(regression,X_train,y_train,scoring='neg_mean_squared_error',
                             cv=3)
```

```{python}
np.mean(validation_score)
```

```{python}
y_pred = regression.predict(X_test_encoded)
y_pred
```
```{python}
from sklearn.metrics import mean_absolute_error,mean_squared_error
mse=mean_squared_error(y_test,y_pred)
mae=mean_absolute_error(y_test,y_pred)
rmse=np.sqrt(mse)
print(mse)
print(mae)
print(rmse)
```

```{python}
from sklearn.metrics import r2_score
score=r2_score(y_test,y_pred)
print(score)
print(1 - (1-score)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1))
```

```{python}
plt.scatter(y_test,y_pred)
```

```{python}
residuals=y_test-y_pred
print(residuals)
```

```{python}
sns.displot(residuals,kind='kde')
```

```{python}
plt.scatter(y_pred,residuals)
```

```{python}
sns.histplot(y_pred, kde=True, color='skyblue')
plt.title('Distribution of Predicted Prices')
plt.xlabel('Predicted Prices')
plt.show()
```

```{=html}
<h3 >3.b)Non-Linear Regression</h3>
    <h3>What is a Non-Linear Regression?</h3>
    <p>One kind of polynomial regression is non-linear regression. A non-linear relationship between the dependent and independent variables can be modeled using this technique. When the data exhibits a curved trend and non-linear regression yields more accurate results than linear regression, it is used instead of linear regression. This is because the assumption behind linear regression is that the data is linear. </p>
    <h3>How does a Non-Linear Regression work?</h3>
    <p>When we pay close attention, we will see that non-linear regression is the next step up from linear regression. All that needs to be done is add the dependent features' higher-order terms to the feature space. Though not precisely, feature engineering is another name for this at times.</p>
    <h3>Applications of Non-Linear Regression</h3>
    <p> that non-linear regression techniques are superior to linear regression techniques because most real-world data is non-linear. Non-linear regression methods aid in the creation of a solid model whose forecasts are trustworthy and consistent with the historical trend observed in the data. Non-Linear Regression techniques successfully completed tasks related to logistic pricing model, financial forecasting, and exponential growth or decay of a population.</p>
```
```{python}
from sklearn.ensemble import GradientBoostingRegressor
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, GridSearchCV
import warnings
import seaborn as sns
import matplotlib.pyplot as plt
warnings.filterwarnings('ignore')
```

```{python}
df = pd.read_csv('miami-housing.csv')
df.head()
```

```{python}
df.columns
```
```{python}
df.info()
```
```{python}
df.describe()
```
```{python}
df.shape
```
```{python}
df.isnull()
```
```{python}
df.dropna(inplace =True)
df.isnull().sum()
```
```{python}
df.duplicated().any()
```
```{python}
df.corr()
```
```{python}
df.drop(['LONGITUDE', 'WATER_DIST'], axis=1,inplace=True)
```
```{python}
low_corr_features = ['PARCELNO', 'age', 'avno60plus', 'month_sold']
df.drop(low_corr_features, axis=1,inplace=True)
```
```{python}
df.corr()
```
```{python}
plt.figure(figsize=(8, 6))
sns.histplot(df['CNTR_DIST'], bins=30, kde=True)
plt.title('Distribution of CNTR_DIST')
plt.show()
```
```{python}
plt.figure(figsize=(8, 6))
sns.regplot(x='OCEAN_DIST', y='SALE_PRC', data=df)
plt.title('Regression plot of OCEAN_DIST vs. SALE_PRC')
plt.show()
```
```{python}
plt.figure(figsize=(8, 6))
sns.scatterplot(x='LND_SQFOOT', y='SALE_PRC', data=df)
plt.title('Scatter plot of LND_SQFOOT vs. SALE_PRC')
plt.show()
```
```{python}
plt.figure(figsize=(8, 6))
sns.histplot(df['SALE_PRC'], bins=30, kde=True)
plt.title('Distribution of SALE_PRC')
plt.show()
```
```{python}
sns.set(style="darkgrid")
color_palette = "viridis"
sns.relplot(x='CNTR_DIST', y='SALE_PRC', sizes=(1, 100), hue='TOT_LVG_AREA', palette=color_palette, size='SALE_PRC', data=df)
plt.xlabel('Distance to Miami Central Business')
plt.ylabel('Sale Price')
plt.ticklabel_format(style='plain', axis='y')
plt.xticks(fontsize=8)
plt.show()
```
```{python}

sns.scatterplot(x='HWY_DIST', y='SALE_PRC', data=df)
plt.title('Scatter plot of HWY_DIST vs. SALE_PRC')
plt.show()
```
```{python}

sns.histplot(df['RAIL_DIST'], bins=30, kde=True)
plt.title('Distribution of RAIL_DIST')
plt.show()
```
```{python}
features = ['LATITUDE', 'LONGITUDE', 'LND_SQFOOT', 'TOT_LVG_AREA', 'SPEC_FEAT_VAL', 'RAIL_DIST', 'OCEAN_DIST', 'WATER_DIST', 'CNTR_DIST', 'SUBCNTR_DI', 'HWY_DIST', 'age', 'avno60plus', 'month_sold', 'structure_quality']
target = 'SALE_PRC'
```

```{python}
df = pd.read_csv('miami-housing.csv')
X = df[features]
y = df[target]
```



```{python}
correlation_matrix = df[features + [target]].corr()
plt.figure(figsize=(10, 6))
sns.heatmap(correlation_matrix,  cmap="coolwarm", fmt=".2f")
for i in range(len(correlation_matrix)):
    for j in range(len(correlation_matrix)):
        plt.text(j + 0.5, i + 0.5, f"{correlation_matrix.iloc[i, j]:.2f}", ha='center', va='center', fontsize=10)
plt.title("Correlation Heatmap")
plt.show()
```

```{python}

sns.histplot(df[target], kde=True)
plt.title("Distribution of Sale Price")
plt.xlabel("Sale Price")
plt.show()
```

```{python}
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)
model = gb_model.fit(X_train, y_train)
y_pred = model.predict(X_test)
r2_score(y_pred,y_test)
```

```{python}
fea_importances = model.feature_importances_

fea_importance_df = pd.DataFrame({'Feature': features, 'Importance': fea_importances})
```

```{python}

fea_importance_df = fea_importance_df.sort_values(by='Importance', ascending=False)
```

```{python}

plt.barh(fea_importance_df['Feature'], fea_importance_df['Importance'])
plt.xlabel('Importance')
plt.title('Feature Importance')
plt.show()
```

```{python}
%matplotlib inline
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
chosen_feature = 'LATITUDE'
y = df['SALE_PRC']
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_features.fit_transform(X[[chosen_feature]])
lin_reg = LinearRegression()
lin_reg.fit(X_poly, y)
plt.scatter(X[chosen_feature], y, color='r', label='Original Data')
X_line = np.linspace(X[chosen_feature].min(), X[chosen_feature].max(), 100).reshape(-1, 1)
X_line_poly = poly_features.transform(X_line)
y_line_pred = lin_reg.predict(X_line_poly)
plt.plot(X_line, y_line_pred, color='b', label='Polynomial Regression')
plt.xlabel(chosen_feature)
plt.ylabel('SALE_PRC')
plt.legend()
plt.show()
```

```{python}
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)
```

```{python}
from sklearn.linear_model import LinearRegression

y = df['SALE_PRC']
num_features = X.shape[1]
fig, axes = plt.subplots(nrows=num_features, ncols=1, figsize=(10, 2*num_features))

for i, feature in enumerate(X.columns):
    X_single_feature = X[feature].values.reshape(-1, 1)
    lin_reg = LinearRegression()
    lin_reg.fit(X_single_feature, y)
    axes[i].scatter(X[feature], y, color='r', label='Original Data')
    X_line = np.linspace(X[feature].min(), X[feature].max(), 100).reshape(-1, 1)
    y_line_pred = lin_reg.predict(X_line)
    axes[i].plot(X_line, y_line_pred, color='b', label='Simple Linear Regression')
    axes[i].set_xlabel(feature)
    axes[i].set_ylabel('SALE_PRC')
    axes[i].legend()
plt.show()
```

```{python}
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_features.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)
poly_reg_model = LinearRegression()
poly_reg_model.fit(X_train, y_train)
accuracy = poly_reg_model.score(X_test, y_test)
print(f"Polynomial Regression Model Accuracy: {accuracy}")
```

```{python}
X_poly.round(2)
```

```{python}
print(poly_reg_model.coef_.round(2))
```

```{python}
print(poly_reg_model.intercept_)
```



```{python}
degree = 2
fig, axes = plt.subplots(nrows=num_features, ncols=1, figsize=(10, 2*num_features))
fig.tight_layout(h_pad=4)
for i, feature in enumerate(X.columns):
    X_single_feature = X[feature].values.reshape(-1, 1)
    poly_features = PolynomialFeatures(degree=degree, include_bias=False)
    X_poly = poly_features.fit_transform(X_single_feature)
    poly_reg = LinearRegression()
    poly_reg.fit(X_poly, y)
    axes[i].scatter(X[feature], y, color='r', label='Original Data')
    X_line = np.linspace(X[feature].min(), X[feature].max(), 100).reshape(-1, 1)
    X_line_poly = poly_features.transform(X_line)
    y_line_pred = poly_reg.predict(X_line_poly)
    axes[i].plot(X_line, y_line_pred, color='b', label=f'Polynomial Regression (Degree {degree})')
    axes[i].set_xlabel(feature)
    axes[i].set_ylabel('SALE_PRC')
    axes[i].legend()

plt.show()
```

```{python}
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_features.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)
poly_reg_model = LinearRegression()
poly_reg_model.fit(X_train, y_train)
accuracy = poly_reg_model.score(X_test, y_test)
print(f"Polynomial Regression Model Accuracy: {accuracy}")
```

```{python}
num_features = X.shape[1]
degree = 3
fig, axes = plt.subplots(nrows=num_features, ncols=1, figsize=(10, 2*num_features))
fig.tight_layout(h_pad=4)
for i, feature in enumerate(X.columns):
    X_single_feature = X[feature].values.reshape(-1, 1)
    poly_features = PolynomialFeatures(degree=degree, include_bias=True) 
    X_poly = poly_features.fit_transform(X_single_feature)
    poly_reg = LinearRegression()
    poly_reg.fit(X_poly, y)
    axes[i].scatter(X[feature], y, color='r', label='Original Data')
    X_line = np.linspace(X[feature].min(), X[feature].max(), 100).reshape(-1, 1)
    X_line_poly = poly_features.transform(X_line)
    y_line_pred = poly_reg.predict(X_line_poly)
    axes[i].plot(X_line, y_line_pred, color='b', label=f'Polynomial Regression (Degree {degree})')
    axes[i].set_xlabel(feature)
    axes[i].set_ylabel('SALE_PRC')
    axes[i].legend()
plt.show()
```

```{python}
from sklearn.preprocessing import PolynomialFeatures
X = df[features]
degree = 3
poly_features = PolynomialFeatures(degree=degree, include_bias=False)
X_poly = poly_features.fit_transform(X)
poly_feature = poly_features.get_feature_names_out(X.columns)
X_poly_df = pd.DataFrame(X_poly, columns=poly_feature)
print(X_poly_df.head())
```

```{python}
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import confusion_matrix, roc_auc_score
import lightgbm as lgb
import xgboost as xgb
from datetime import datetime

df = pd.read_csv('miami-housing.csv')
thresholdPrice = 500000
df['Above_Threshold'] = (df['SALE_PRC'] > thresholdPrice).astype(int)
df = df.drop(['PARCELNO'], axis=1)
y = df['Above_Threshold'].values
labelencoder = LabelEncoder()
Y = labelencoder.fit_transform(y)   
X = df.drop(labels=['Above_Threshold', 'SALE_PRC'], axis=1)  
feature_names = np.array(X.columns)
scaler = StandardScaler()
scaler.fit(X)
X = scaler.transform(X)
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

dtrain_lgb = lgb.Dataset(X_train, label=y_train)

lgbm_params = {
    'learning_rate': 0.05,
    'boosting_type': 'gbdt',
    'objective': 'binary',
    'metric': ['auc', 'binary_logloss'],
    'num_leaves': 100,
    'max_depth': 10
}
startLgb = datetime.now()
clf_lgb = lgb.train(lgbm_params, dtrain_lgb, 50)
stopLgb = datetime.now()
executionTime_lgb = stopLgb - startLgb
ypred_lgb = clf_lgb.predict(X_test)
ypred_lgb = (ypred_lgb >= 0.5).astype(int)
dtrain_xgb = xgb.DMatrix(X_train, label=y_train)
parameters_xgb = {
    'max_depth': 10,
    'objective': 'binary:logistic',
    'eval_metric': 'auc',
    'learning_rate': 0.05
}
startXgb = datetime.now()
xg = xgb.train(parameters_xgb, dtrain_xgb, 50)
stopXgb = datetime.now()
executionTime_xgb = stopXgb - startXgb
dtest_xgb = xgb.DMatrix(X_test)
ypred_xgb = xg.predict(dtest_xgb)
ypred_xgb = (ypred_xgb >= 0.5).astype(int)
cm_xgb = confusion_matrix(y_test, ypred_xgb)
sns.heatmap(cm_xgb,fmt='g',cmap="crest" )
for i in range(len(cm_xgb)):
    for j in range(len(cm_xgb[0])):
        plt.text(j + 0.5, i + 0.5, str(cm_xgb[i, j]), ha='center', va='center', fontsize=16, color='white')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('XGBoost Confusion Matrix')
plt.show() 
print("_________________________________________________")
print("LGBM execution time is: ", executionTime_lgb )
print("XGBoost execution time is: ", executionTime_xgb)
print("_________________________________________________")
print("Accuracy with LGBM = ", np.mean(ypred_lgb == y_test))
print("Accuracy with XGBoost = ", np.mean(ypred_xgb == y_test))
print("___________________________________________________")
print("AUC score with LGBM is: ", roc_auc_score(y_test, ypred_lgb))
print("AUC score with XGBoost is: ", roc_auc_score(y_test,ypred_xgb))
```
