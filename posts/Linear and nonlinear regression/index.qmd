---
title: "Linear and nonlinear regression"
date: "2023-11-19"
image: "Image.png"
author: Swethaa
---


```{=html}
<h3>a)Linear Regression
</h3> 
    <h3>Index</h3>
    <ul>
        <li>What is Linear Regression</li>
        <li>Types of Linear Regression</li>
        <li>Training a Linear Regression Model</li>
        <li>Finding the best fit line</li>
    </ul>
    <h3>What is Linear Regression</h3>
    <p>Linear regression is one of the easiest and most popular Machine Learning algorithms. It is a statistical method that is used for predictive analysis. Linear regression makes predictions for continuous/real or numeric variables such as sales, salary,
        age, product price, etc.</p>
    <p>Mathematically, we can represent a linear regression as:</p>
    <b>y= a0+a1x+ ε</b>
    <ul>
        <li>Y= Dependent Variable (Target Variable)</li>
        <li>X= Independent Variable (predictor Variable)</li>
        <li>a0= intercept of the line (Gives an additional degree of freedom)</li>
        <li>a1 = Linear regression coefficient (scale factor to each input value).</li>
        <li>ε = random error</li>
    </ul>
    <p></p> The values for x and y variables are training datasets for Linear Regression model representation.</p>
    <h3>Types of Linear Regression</h3>
    <ul>
        <li> <b>Simple Linear Regression</b>
            <p>If a single independent variable is used to predict the value of a numerical dependent variable, then such a Linear Regression algorithm is called Simple Linear Regression.</p>
            <h4> <b>Y=β 0+β 1⋅X+ε</b></h4>
            <ul>
                <li>
                    Y is the dependent variable.
                </li>
                <li>X is the independent variable.</li>
                <li>β 0is the intercept (where the line crosses theY-axis).</li>
                <li>β 1 is the slop (the change in Y for a one-unitchange in X).</li>
                <li>ε is the error term, representing the variability in Y that is not explained by the linear relationship withX</li>
            </ul>
        </li>
        <li> <b>Multiple Linear Regression</b>
            <p>If more than one independent variable is used to predict the value of a numerical dependent variable, then such a Linear Regression algorithm is called Multiple Linear Regression.</p>
            <h4> <b>Y=β 0+β 1⋅X 1 +β 2⋅X 2 +…+β n⋅X n+ε </b></h4>
            <ul>
                <li>
                    Y is the dependent variable.
                </li>
                <li>X 1,X 2,…,Xn are the independent variables</li>
                <li>β 0is the intercept </li>
                <li>β1,β 2,…,β nare the coefficients representing the impact of each independent variable.</li>
                <li>ε is the error term</li>
            </ul>
        </li>
    </ul>
    <h3>Training a Linear Regression Model</h3>
    <p>The process of training a linear regression model involves finding the values of the coefficients (β0,β 1,…,βn) that minimize the sum of squared differences between the predicted and actual values in the training data.</p>
    <h3>Finding the best fit line</h3>
    <p>goal is to find the best fit line that means the error between predicted values and actual values should be minimized. The best fit line will have the least error. The different values for weights or the coefficient of lines (a0, a1) gives a different
        line of regression, so we need to calculate the best values for a0 and a1 to find the best fit line, so to calculate this we use cost function.</p>
    <h3>Continuous random variable</h3>
    <p>A continuous random variable has two main characteristics:</p>
    <p>the set of its possible values is uncountable;
we compute the probability that its value will belong to a given interval by integrating a function called probability density function.</p>
    <h3>Common continuous distributions</h3>
    <p>The next table contains some examples of continuous distributions that are frequently encountered in probability theory and statistics.</p>
    <table>
        <tr>
            <th>Name of the continuous distribution</th>
            <th>Support</th>
        </tr>
        <tr>
            <td>Uniform</td>
            <td>All the real numbers in the interval [0,1]</td>
        </tr>
         <tr>
            <td>Normal</td>
            <td>The whole set of real numbers</td>
        </tr>
         <tr>
            <td>Chi-square</td>
            <td>The set of all non-negative real numbers</td>
        </tr>
    </table>
    <img src="Image2.png" alt="" height="300px" width="300px">
```
```{python}
from sklearn.ensemble import GradientBoostingRegressor
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, GridSearchCV
import warnings
import seaborn as sns
import matplotlib.pyplot as plt
warnings.filterwarnings('ignore')
```
```{python}
df = pd.read_csv('CarPrice_Assignment.csv')
df.head()
```
```{python}
df.columns
```
```{python}
df.info()
```
```{python}
df.describe()
```
```{python}
df.shape
```
```{python}
df.isnull()
```
```{python}
sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')
```
```{python}
df.dropna(inplace =True)
df.isnull().sum()
```
```{python}
df.duplicated().any()
```
```{python}
df.describe(include=object)
```
```{python}
df = df.select_dtypes(include=['float64', 'int64'])
df.corr()
```
```{python}
correlation_matrix = df[['carlength', 'carwidth', 'curbweight']].corr()
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix,  cmap='coolwarm', center=0)
for i in range(len(correlation_matrix)):
    for j in range(len(correlation_matrix)):
        plt.text(j + 0.5, i + 0.5, f"{correlation_matrix.iloc[i, j]:.2f}", ha='center', va='center', fontsize=10)
plt.title('Correlation Matrix')
plt.show()
```
```{python}
df = df.drop(['symboling', 'car_ID', ], axis=1)
print(df.head())
```
```{python}
df.shape
```
```{python}
df.columns
```
```{python}
X = df.drop('price', axis=1)
y = df['price']
```
```{python}
X.head()
```
```{python}
y
```
```{python}
plt.figure(figsize=(8, 6))
sns.heatmap(df.corr(), cmap='coolwarm', fmt=".2f", linewidths=.5)
for i in range(len(df.corr())):
    for j in range(len(df.corr())):
        plt.text(j + 0.5, i + 0.5, f"{df.corr().iloc[i, j]:.2f}", ha='center', va='center', fontsize=10)
plt.title('Correlation Heatmap')
plt.show()
```

```{python}
sns.pairplot(df)
```

```{python}
plt.figure(figsize=(8, 6))
sns.histplot(df['enginesize'], kde=True, color='skyblue')
plt.title('Distribution of Engine Size')
plt.show()
```

```{python}
plt.figure(figsize=(8, 6))
sns.histplot(df['wheelbase'], kde=True)
plt.title('Distribution of Wheelbase')
plt.show()
```

```{python}
plt.figure(figsize=(8, 6))
sns.histplot(df['carlength'], kde=True)
plt.title('Distribution of Car Length')
plt.show()
```

```{python}

plt.figure(figsize=(8, 6))
sns.histplot(df['carlength'], kde=True)
plt.title('Distribution of Car Length')
plt.show()
```

```{python}
plt.figure(figsize=(8, 6))
sns.histplot(df['carheight'], kde=True)
plt.title('Distribution of Car Height')
plt.show()
```

```{python}
plt.figure(figsize=(8, 6))
sns.histplot(df['curbweight'], kde=True)
plt.title('Distribution of Curb Weight')
plt.show()
```

```{python}
plt.scatter(df['enginesize'], df['price'])
plt.xlabel('Engine Size')
plt.ylabel('Price')
plt.title('Scatter Plot between Engine Size and Price')
plt.show()
```
```{python}
sns.scatterplot(x='horsepower', y='price', data=df)
plt.title('Horsepower vs. Price (Scatter Plot)')
plt.show()
```
```{python}
sns.histplot(df['price'], kde=True, color='skyblue')
plt.title('Distribution of Car Prices with KDE')
plt.show()
```

```{python}
sns.histplot(df['citympg'], kde=True, bins=20, color='skyblue')
plt.title('Distribution of City MPG')
plt.xlabel('City MPG')
plt.show()
```



```{python}
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=42)
```

```{python}
sns.regplot(x='wheelbase', y='price', data=df, scatter_kws={'s': 20}, line_kws={'color': 'red'})
plt.title('Wheelbase vs. Price')
plt.xlabel('Wheelbase')
plt.ylabel('Price')
plt.show()
```

```{python}
sns.regplot(x='carlength', y='price', data=df, scatter_kws={'s': 20}, line_kws={'color': 'red'})
plt.title('Car Length vs. Price')
plt.xlabel('Car Length')
plt.ylabel('Price')
plt.show()
```

```{python}
sns.regplot(x='curbweight', y='price', data=df, scatter_kws={'s': 20}, line_kws={'color': 'red'})
plt.title('Curb Weight vs. Price')
plt.xlabel('Curb Weight')
plt.ylabel('Price')
plt.show()
```

```{python}
sns.regplot(x='enginesize', y='price', data=df, scatter_kws={'s': 20}, line_kws={'color': 'red'})
plt.title('Engine Size vs. Price')
plt.xlabel('Engine Size')
plt.ylabel('Price')
plt.show()
```

```{python}
sns.regplot(x='horsepower', y='price', data=df, scatter_kws={'s': 20}, line_kws={'color': 'red'})
plt.title('Horsepower vs. Price')
plt.xlabel('Horsepower')
plt.ylabel('Price')
plt.show()
```

```{python}
sns.regplot(x='citympg', y='price', data=df, scatter_kws={'s': 20}, line_kws={'color': 'red'})
plt.title('City MPG vs. Price')
plt.xlabel('City MPG')
plt.ylabel('Price')
plt.show()
```

```{python}
print(X_train.columns)
```
```{python}
import pandas as pd
categorical_cols = ['enginesize', 'horsepower', 'peakrpm']
X_train_encoded = pd.get_dummies(X_train, columns=categorical_cols)
X_test_encoded = pd.get_dummies(X_test, columns=categorical_cols)
```

```{python}
for column in X_train.columns:
    unique_values = X_train[column].nunique()
    print(f"Column '{column}' has {unique_values} unique values.")
```

```{python}
from sklearn.linear_model import LinearRegression
regression=LinearRegression()
```
```{python}
for col in categorical_cols:
    print(f"Unique values in '{col}': {X_train[col].unique()}")
```

```{python}
print(X_train.dtypes)
```
```{python}
X_train_encoded = pd.get_dummies(X_train, columns=categorical_cols)
```

```{python}
X_train_encoded = pd.get_dummies(X_train, columns=categorical_cols)
X_test_encoded = pd.get_dummies(X_test, columns=categorical_cols)
```

```{python}
non_numeric_cols_train = X_train_encoded.select_dtypes(exclude=['float64', 'int64']).columns
non_numeric_cols_test = X_test_encoded.select_dtypes(exclude=['float64', 'int64']).columns
print("Non-numeric columns in X_train_encoded:", non_numeric_cols_train)
print("Non-numeric columns in X_test_encoded:", non_numeric_cols_test)
```

```{python}
from sklearn.preprocessing import LabelEncoder
label_encoder_train = LabelEncoder()
for col in non_numeric_cols_train:
    X_train_encoded[col] = label_encoder_train.fit_transform(X_train_encoded[col])

label_encoder_test = LabelEncoder()
for col in non_numeric_cols_test:
    X_test_encoded[col] = label_encoder_test.fit_transform(X_test_encoded[col])
```

```{python}
for col in non_numeric_cols_train:
    print(f"Unique values in {col}: {X_train_encoded[col].unique()}")
```

```{python}
for col in non_numeric_cols_test:
    print(f"Unique values in {col}: {X_test_encoded[col].unique()}")
```

```{python}
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
label_encoder = LabelEncoder()

for col in categorical_cols:
    X_train[col] = label_encoder.fit_transform(X_train[col])

onehot_encoder = OneHotEncoder(drop='first', sparse=False)

X_train_encoded = pd.get_dummies(X_train, columns=categorical_cols, drop_first=True)
X_test_encoded = pd.get_dummies(X_test, columns=categorical_cols, drop_first=True)

X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='outer', axis=1, fill_value=0)

regression.fit(X_train_encoded, y_train)
```

```{python}
from sklearn.model_selection import cross_val_score
validation_score=cross_val_score(regression,X_train,y_train,scoring='neg_mean_squared_error',
                             cv=3)
```

```{python}
np.mean(validation_score)
```

```{python}
y_pred = regression.predict(X_test_encoded)
y_pred
```
```{python}
from sklearn.metrics import mean_absolute_error,mean_squared_error
mse=mean_squared_error(y_test,y_pred)
mae=mean_absolute_error(y_test,y_pred)
rmse=np.sqrt(mse)
print(mse)
print(mae)
print(rmse)
```

```{python}
from sklearn.metrics import r2_score
score=r2_score(y_test,y_pred)
print(score)
print(1 - (1-score)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1))
```

```{python}
plt.scatter(y_test,y_pred)
```

```{python}
residuals=y_test-y_pred
print(residuals)
```

```{python}
sns.displot(residuals,kind='kde')
```

```{python}
plt.scatter(y_pred,residuals)
```

```{python}
sns.histplot(y_pred, kde=True, color='skyblue')
plt.title('Distribution of Predicted Prices')
plt.xlabel('Predicted Prices')
plt.show()
```

```{=html}
<h3 >3.b)Non-Linear Regression</h3>
    <h3>What is a Non-Linear Regression?</h3>
    <p>Non-Linear regression is a type of polynomial regression. It is a method to model a non-linear relationship between the dependent and independent variables. It is used in place when the data shows a curvy trend and linear regression would not produce
        very accurate results when compared to non-linear regression. This is because in linear regression it is pre-assumed that the data is linear. </p>
    <h3>How does a Non-Linear Regression work?</h3>
    <p>observe closely then we will realize that to evolve from linear regression to non-linear regression. We are just supposed to add the higher-order terms of the dependent features in the feature space. This is sometimes also known as feature engineering
        but not exactly.</p>
    <h3>Applications of Non-Linear Regression</h3>
    <p> that most of the real-world data is non-linear and hence non-linear regression techniques are far better than linear regression techniques. Non-Linear regression techniques help to get a robust model whose predictions are reliable and as per the trend
        followed by the data in history. Tasks related to exponential growth or decay of a population, financial forecasting, and logistic pricing model were all successfully accomplished by the Non-Linear Regression techniques.</p>
```
```{python}
from sklearn.ensemble import GradientBoostingRegressor
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, GridSearchCV
import warnings
import seaborn as sns
import matplotlib.pyplot as plt
warnings.filterwarnings('ignore')
```

```{python}
df = pd.read_csv('miami-housing.csv')
df.head()
```

```{python}
df.columns
```
```{python}
df.info()
```
```{python}
df.describe()
```
```{python}
df.shape
```
```{python}
df.isnull()
```
```{python}
df.dropna(inplace =True)
df.isnull().sum()
```
```{python}
df.duplicated().any()
```
```{python}
df.corr()
```
```{python}
df.drop(['LONGITUDE', 'WATER_DIST'], axis=1,inplace=True)
```
```{python}
low_corr_features = ['PARCELNO', 'age', 'avno60plus', 'month_sold']
df.drop(low_corr_features, axis=1,inplace=True)
```
```{python}
df.corr()
```
```{python}
plt.figure(figsize=(8, 6))
sns.histplot(df['CNTR_DIST'], bins=30, kde=True)
plt.title('Distribution of CNTR_DIST')
plt.show()
```
```{python}
plt.figure(figsize=(8, 6))
sns.regplot(x='OCEAN_DIST', y='SALE_PRC', data=df)
plt.title('Regression plot of OCEAN_DIST vs. SALE_PRC')
plt.show()
```
```{python}
plt.figure(figsize=(8, 6))
sns.scatterplot(x='LND_SQFOOT', y='SALE_PRC', data=df)
plt.title('Scatter plot of LND_SQFOOT vs. SALE_PRC')
plt.show()
```
```{python}
plt.figure(figsize=(8, 6))
sns.histplot(df['SALE_PRC'], bins=30, kde=True)
plt.title('Distribution of SALE_PRC')
plt.show()
```
```{python}
sns.set(style="darkgrid")
color_palette = "viridis"
sns.relplot(x='CNTR_DIST', y='SALE_PRC', sizes=(1, 100), hue='TOT_LVG_AREA', palette=color_palette, size='SALE_PRC', data=df)
plt.xlabel('Distance to Miami Central Business')
plt.ylabel('Sale Price')
plt.ticklabel_format(style='plain', axis='y')
plt.xticks(fontsize=8)
plt.show()
```
```{python}

sns.scatterplot(x='HWY_DIST', y='SALE_PRC', data=df)
plt.title('Scatter plot of HWY_DIST vs. SALE_PRC')
plt.show()
```
```{python}

sns.histplot(df['RAIL_DIST'], bins=30, kde=True)
plt.title('Distribution of RAIL_DIST')
plt.show()
```
```{python}
features = ['LATITUDE', 'LONGITUDE', 'LND_SQFOOT', 'TOT_LVG_AREA', 'SPEC_FEAT_VAL', 'RAIL_DIST', 'OCEAN_DIST', 'WATER_DIST', 'CNTR_DIST', 'SUBCNTR_DI', 'HWY_DIST', 'age', 'avno60plus', 'month_sold', 'structure_quality']
target = 'SALE_PRC'
```

```{python}
df = pd.read_csv('miami-housing.csv')
X = df[features]
y = df[target]
```



```{python}
correlation_matrix = df[features + [target]].corr()
plt.figure(figsize=(10, 6))
sns.heatmap(correlation_matrix,  cmap="coolwarm", fmt=".2f")
for i in range(len(correlation_matrix)):
    for j in range(len(correlation_matrix)):
        plt.text(j + 0.5, i + 0.5, f"{correlation_matrix.iloc[i, j]:.2f}", ha='center', va='center', fontsize=10)
plt.title("Correlation Heatmap")
plt.show()
```

```{python}

sns.histplot(df[target], kde=True)
plt.title("Distribution of Sale Price")
plt.xlabel("Sale Price")
plt.show()
```

```{python}
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)
model = gb_model.fit(X_train, y_train)
y_pred = model.predict(X_test)
r2_score(y_pred,y_test)
```

```{python}
fea_importances = model.feature_importances_

fea_importance_df = pd.DataFrame({'Feature': features, 'Importance': fea_importances})
```

```{python}

fea_importance_df = fea_importance_df.sort_values(by='Importance', ascending=False)
```

```{python}

plt.barh(fea_importance_df['Feature'], fea_importance_df['Importance'])
plt.xlabel('Importance')
plt.title('Feature Importance')
plt.show()
```

```{python}
%matplotlib inline
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
chosen_feature = 'LATITUDE'
y = df['SALE_PRC']
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_features.fit_transform(X[[chosen_feature]])
lin_reg = LinearRegression()
lin_reg.fit(X_poly, y)
plt.scatter(X[chosen_feature], y, color='r', label='Original Data')
X_line = np.linspace(X[chosen_feature].min(), X[chosen_feature].max(), 100).reshape(-1, 1)
X_line_poly = poly_features.transform(X_line)
y_line_pred = lin_reg.predict(X_line_poly)
plt.plot(X_line, y_line_pred, color='b', label='Polynomial Regression')
plt.xlabel(chosen_feature)
plt.ylabel('SALE_PRC')
plt.legend()
plt.show()
```

```{python}
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)
```

```{python}
from sklearn.linear_model import LinearRegression

y = df['SALE_PRC']
num_features = X.shape[1]
fig, axes = plt.subplots(nrows=num_features, ncols=1, figsize=(10, 2*num_features))

for i, feature in enumerate(X.columns):
    X_single_feature = X[feature].values.reshape(-1, 1)
    lin_reg = LinearRegression()
    lin_reg.fit(X_single_feature, y)
    axes[i].scatter(X[feature], y, color='r', label='Original Data')
    X_line = np.linspace(X[feature].min(), X[feature].max(), 100).reshape(-1, 1)
    y_line_pred = lin_reg.predict(X_line)
    axes[i].plot(X_line, y_line_pred, color='b', label='Simple Linear Regression')
    axes[i].set_xlabel(feature)
    axes[i].set_ylabel('SALE_PRC')
    axes[i].legend()
plt.show()
```

```{python}
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_features.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)
poly_reg_model = LinearRegression()
poly_reg_model.fit(X_train, y_train)
accuracy = poly_reg_model.score(X_test, y_test)
print(f"Polynomial Regression Model Accuracy: {accuracy}")
```

```{python}
X_poly.round(2)
```

```{python}
print(poly_reg_model.coef_.round(2))
```

```{python}
print(poly_reg_model.intercept_)
```



```{python}
degree = 2
fig, axes = plt.subplots(nrows=num_features, ncols=1, figsize=(10, 2*num_features))
fig.tight_layout(h_pad=4)
for i, feature in enumerate(X.columns):
    X_single_feature = X[feature].values.reshape(-1, 1)
    poly_features = PolynomialFeatures(degree=degree, include_bias=False)
    X_poly = poly_features.fit_transform(X_single_feature)
    poly_reg = LinearRegression()
    poly_reg.fit(X_poly, y)
    axes[i].scatter(X[feature], y, color='r', label='Original Data')
    X_line = np.linspace(X[feature].min(), X[feature].max(), 100).reshape(-1, 1)
    X_line_poly = poly_features.transform(X_line)
    y_line_pred = poly_reg.predict(X_line_poly)
    axes[i].plot(X_line, y_line_pred, color='b', label=f'Polynomial Regression (Degree {degree})')
    axes[i].set_xlabel(feature)
    axes[i].set_ylabel('SALE_PRC')
    axes[i].legend()

plt.show()
```

```{python}
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_features.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)
poly_reg_model = LinearRegression()
poly_reg_model.fit(X_train, y_train)
accuracy = poly_reg_model.score(X_test, y_test)
print(f"Polynomial Regression Model Accuracy: {accuracy}")
```

```{python}
num_features = X.shape[1]
degree = 3
fig, axes = plt.subplots(nrows=num_features, ncols=1, figsize=(10, 2*num_features))
fig.tight_layout(h_pad=4)
for i, feature in enumerate(X.columns):
    X_single_feature = X[feature].values.reshape(-1, 1)
    poly_features = PolynomialFeatures(degree=degree, include_bias=True) 
    X_poly = poly_features.fit_transform(X_single_feature)
    poly_reg = LinearRegression()
    poly_reg.fit(X_poly, y)
    axes[i].scatter(X[feature], y, color='r', label='Original Data')
    X_line = np.linspace(X[feature].min(), X[feature].max(), 100).reshape(-1, 1)
    X_line_poly = poly_features.transform(X_line)
    y_line_pred = poly_reg.predict(X_line_poly)
    axes[i].plot(X_line, y_line_pred, color='b', label=f'Polynomial Regression (Degree {degree})')
    axes[i].set_xlabel(feature)
    axes[i].set_ylabel('SALE_PRC')
    axes[i].legend()
plt.show()
```

```{python}
from sklearn.preprocessing import PolynomialFeatures
X = df[features]
degree = 3
poly_features = PolynomialFeatures(degree=degree, include_bias=False)
X_poly = poly_features.fit_transform(X)
poly_feature = poly_features.get_feature_names_out(X.columns)
X_poly_df = pd.DataFrame(X_poly, columns=poly_feature)
print(X_poly_df.head())
```

```{python}
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import confusion_matrix, roc_auc_score
import lightgbm as lgb
import xgboost as xgb
from datetime import datetime

df = pd.read_csv('miami-housing.csv')
thresholdPrice = 500000
df['Above_Threshold'] = (df['SALE_PRC'] > thresholdPrice).astype(int)
df = df.drop(['PARCELNO'], axis=1)
y = df['Above_Threshold'].values
labelencoder = LabelEncoder()
Y = labelencoder.fit_transform(y)   
X = df.drop(labels=['Above_Threshold', 'SALE_PRC'], axis=1)  
feature_names = np.array(X.columns)
scaler = StandardScaler()
scaler.fit(X)
X = scaler.transform(X)
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

dtrain_lgb = lgb.Dataset(X_train, label=y_train)

lgbm_params = {
    'learning_rate': 0.05,
    'boosting_type': 'gbdt',
    'objective': 'binary',
    'metric': ['auc', 'binary_logloss'],
    'num_leaves': 100,
    'max_depth': 10
}
startLgb = datetime.now()
clf_lgb = lgb.train(lgbm_params, dtrain_lgb, 50)
stopLgb = datetime.now()
executionTime_lgb = stopLgb - startLgb
ypred_lgb = clf_lgb.predict(X_test)
ypred_lgb = (ypred_lgb >= 0.5).astype(int)
dtrain_xgb = xgb.DMatrix(X_train, label=y_train)
parameters_xgb = {
    'max_depth': 10,
    'objective': 'binary:logistic',
    'eval_metric': 'auc',
    'learning_rate': 0.05
}
startXgb = datetime.now()
xg = xgb.train(parameters_xgb, dtrain_xgb, 50)
stopXgb = datetime.now()
executionTime_xgb = stopXgb - startXgb
dtest_xgb = xgb.DMatrix(X_test)
ypred_xgb = xg.predict(dtest_xgb)
ypred_xgb = (ypred_xgb >= 0.5).astype(int)
cm_xgb = confusion_matrix(y_test, ypred_xgb)
sns.heatmap(cm_xgb,fmt='g',cmap="crest" )
for i in range(len(cm_xgb)):
    for j in range(len(cm_xgb[0])):
        plt.text(j + 0.5, i + 0.5, str(cm_xgb[i, j]), ha='center', va='center', fontsize=16, color='white')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('XGBoost Confusion Matrix')
plt.show() 
print("_________________________________________________")
print("LGBM execution time is: ", executionTime_lgb )
print("XGBoost execution time is: ", executionTime_xgb)
print("_________________________________________________")
print("Accuracy with LGBM = ", np.mean(ypred_lgb == y_test))
print("Accuracy with XGBoost = ", np.mean(ypred_xgb == y_test))
print("___________________________________________________")
print("AUC score with LGBM is: ", roc_auc_score(y_test, ypred_lgb))
print("AUC score with XGBoost is: ", roc_auc_score(y_test,ypred_xgb))
```
