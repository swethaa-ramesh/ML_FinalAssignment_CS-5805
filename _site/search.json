[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS 5805-Final Term Project",
    "section": "",
    "text": "Machine Learning (CS 5805) \nSubmitted by: Swethaa Shanmugam Ramesh\n\n\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly/Outlier detection\n\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\nSwethaa\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nSwethaa\n\n\n\n\n\n\n  \n\n\n\n\nLinear and nonlinear regression\n\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nSwethaa\n\n\n\n\n\n\n  \n\n\n\n\nProbability theory and random variables\n\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2023\n\n\nSwethaa\n\n\n\n\n\n\n  \n\n\n\n\nClasification\n\n\n\n\n\n\n\n\n\n\n\n\nNov 5, 2023\n\n\nSwethaa\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Clustering - Copy/index.html",
    "href": "posts/Clustering - Copy/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering \nContent\n    \n        Introduction\n        What is Clustering\n        Why Clustering\n        Types of Clustering Methods/ Algorithms\n    \n    Introduction\n    It is basically a type of unsupervised learning method. An unsupervised learning method is a method in which we draw references from datasets consisting of input data without labeled responses. Generally, it is used as a process to find meaningful\n        structure, explanatory underlying processes, generative features, and groupings inherent in a set of examples. \n    What is Clustering\n    Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group and dissimilar to the data points in other groups. It is basically\n        a collection of objects on the basis of similarity and dissimilarity between them. \n    Clustering is a type of unsupervised machine learning technique where the goal is to group similar data points together based on certain features or characteristics. The objective of clustering is to find natural groupings or patterns within a dataset\n        without the need for labeled output.\n    Why Clustering\n    When working with large datasets, an efficient way to analyze them is to first divide the data into logical groupings, aka clusters. This way, you could extract value from a large set of unstructured data. It helps you to glance through the data to\n        pull out some patterns or structures before going deeper into analyzing the data for specific findings\n    Clustering is very much important as it determines the intrinsic grouping among the unlabelled data present. There are no criteria for good clustering. It depends on the user, and what criteria they may use which satisfy their need\n    \n        \n            Pattern Recognition:\n            Clustering helps identify natural groupings or patterns within data that may not be immediately apparent. It allows for the discovery of inherent structures and relationships.\n        \n        \n            Data Exploration and Understanding:\n            Insight into Data Distribution: Clustering provides insights into how data points are distributed and grouped in a dataset. This is valuable for exploratory data analysis and understanding the underlying structure.\n        \n        Segmentation and Customer Profiling:\n            In business and marketing, clustering is often used for customer segmentation. It helps identify groups of customers with similar behavior, preferences, or purchasing patterns, enabling targeted marketing strategies.\n        \n\n        \n            Anomaly Detection:\n            Identification of Outliers: Clustering can be used to identify unusual patterns or outliers in a dataset. Data points that do not conform to the patterns of their assigned clusters may be considered anomalies.\n        \n    \n    Types of Clustering Methods/ Algorithms\n    \n            \n        Clustering Methods/ Algorithms\n            Method\n            Description\n            Advantages\n            Disadvantages\n        \n        \n            K-Means Clustering\n            Partitioning\n            Divides the dataset into a specified number (k) of clusters. \n            Simple and computationally efficient.\n            Sensitive to initial cluster centroids.\n        \n        \n            Hierarchical Clustering\n            Agglomerative (bottom-up) or divisive (top-down)\n            Builds a tree-like hierarchy of clusters\n            Provides a hierarchy of clusters, visualized using dendrogram.\n            Computationally more intensive.\n        \n        \n            DBSCAN\n            Density-based\n            Forms clusters based on the density of data points\n            Can discover clusters of arbitrary shapes and handles noise well\n            Sensitive to parameter settings\n        \n        \n            Mean Shift\n            Centroid-based\n            Iteratively shifts centroids towards the mode of the data distribution\n            Can find irregularly shaped clusters and adapt to varying densities\n            Computationally expensive\n        \n        \n            Gaussian Mixture Model (GMM)\n            Probabilistic\n            the data points are generated from a mixture of several Gaussian distributions\n            Can model complex data distributions and provide probabilistic cluster assignments\n            Sensitive to the initial parameters.\n        \n        \n            Mean Shift\n            Centroid-based\n            Iteratively shifts centroids towards the mode of the data distribution. The resulting clusters are regions of high data density\n            Can find irregularly shaped clusters and adapt to varying densities\n            Computationally expensive\n        \n          \n            Agglomerative Clustering\n            Hierarchical, bottom-up\n            Starts with individual data points as separate clusters and iteratively merges the closest clusters until a stopping criterion is met\n            Can handle different shapes and sizes of clusters\n            Computationally more intensive\n        \n    \n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\ndf = pd.read_csv('IMDB.csv')\n\n\ndf.head()\n\n\n\n\n\n\n\n\nName\nYear\nEpisodes\nType\nRating\nImage-src\nDescription\nName-href\n\n\n\n\n0\n1. Breaking Bad\n2008–2013\n62 eps\nTV-MA\n9.5\nhttps://m.media-amazon.com/images/M/MV5BYmQ4YW...\nA chemistry teacher diagnosed with inoperable ...\nhttps://www.imdb.com/title/tt0903747/?ref_=cht...\n\n\n1\n2. Planet Earth II\n2016\n6 eps\nTV-G\n9.5\nhttps://m.media-amazon.com/images/M/MV5BMGZmYm...\nDavid Attenborough returns with a new wildlife...\nhttps://www.imdb.com/title/tt5491994/?ref_=cht...\n\n\n2\n3. Planet Earth\n2006\n11 eps\nTV-PG\n9.4\nhttps://m.media-amazon.com/images/M/MV5BMzMyYj...\nA documentary series on the wildlife found on ...\nhttps://www.imdb.com/title/tt0795176/?ref_=cht...\n\n\n3\n4. Band of Brothers\n2001\n10 eps\nTV-MA\n9.4\nhttps://m.media-amazon.com/images/M/MV5BMTI3OD...\nThe story of Easy Company of the U.S. Army 101...\nhttps://www.imdb.com/title/tt0185906/?ref_=cht...\n\n\n4\n5. Chernobyl\n2019\n5 eps\nTV-MA\n9.4\nhttps://m.media-amazon.com/images/M/MV5BNTdkN2...\nIn April 1986, an explosion at the Chernobyl n...\nhttps://www.imdb.com/title/tt7366338/?ref_=cht...\n\n\n\n\n\n\n\n\ndf.columns\n\nIndex(['Name', 'Year', 'Episodes', 'Type', 'Rating', 'Image-src',\n       'Description', 'Name-href'],\n      dtype='object')\n\n\n\ndf['Year']\n\n0      2008–2013\n1           2016\n2           2006\n3           2001\n4           2019\n         ...    \n245        2009–\n246    2002–2015\n247    2009–2013\n248    2014–2015\n249    2017–2019\nName: Year, Length: 250, dtype: object\n\n\n\ndf.dropna(inplace=True)\n\n\ndf.isnull().sum()\n\nName           0\nYear           0\nEpisodes       0\nType           0\nRating         0\nImage-src      0\nDescription    0\nName-href      0\ndtype: int64\n\n\n\nplt.figure(figsize=(10, 6))\nsns.countplot(x='Type', data=df, palette='viridis')\nplt.title('Distribution of TV Shows by Genres')\nplt.show()\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n\n\n\n\ntop_rated_shows = df.sort_values(by='Rating', ascending=False).head(10)\nplt.figure(figsize=(12, 6))\nsns.barplot(x='Rating', y='Name', data=top_rated_shows, palette='Blues_r')\nplt.title('Top-rated TV Shows and Their IMDb Ratings')\nplt.xlabel('IMDb Rating')\nplt.ylabel('TV Show Name')\nplt.show()\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n\n\n\n\nplt.figure(figsize=(12, 6))\nsns.swarmplot(x='Type', y='Rating', data=df, palette='dark', size=8)\nplt.title('IMDb Ratings Distribution by TV Show Type')\nplt.xlabel('TV Show Type')\nplt.ylabel('IMDb Rating')\nplt.show()\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_5676\\3663582833.py:2: FutureWarning:\n\nPassing `palette` without assigning `hue` is deprecated.\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\categorical.py:3544: UserWarning:\n\n50.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\categorical.py:3544: UserWarning:\n\n23.4% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\categorical.py:3544: UserWarning:\n\n40.3% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n\n\n\n\n\n\n\nsns.pairplot(df[['Year', 'Episodes', 'Rating', 'Type']], hue='Type', palette='Set1')\nplt.suptitle('Pair Plot of TV Show Data with Type Hue', y=1.02)\nplt.show()\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n\n\n\n\n\n\nfig, ax = plt.subplots(1, figsize = (30,8))\nax = sns.scatterplot(x='Year', y='Rating', data=df, hue='Type', palette='Set1', alpha=0.7)\nax.grid()\nfig.autofmt_xdate()\nplt.xticks(rotation = 90, ha = 'right',\n           fontsize = 10)\nplt.xlim(0, 178)\nplt.title('Correlation between Release Year and IMDb Ratings')\nplt.xlabel('Release Year')\nplt.ylabel('IMDb Rating')\nplt.legend(title='TV Show Type')\nplt.show()\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n\n\n\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ntop_rated_descriptions = \" \".join(df['Description'])\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate(top_rated_descriptions)\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud of Top-rated TV Show Descriptions')\nplt.show()\n\n\n\n\n\nnumerical_features = df[['Year', 'Episodes', 'Rating']]\n\n\ndf['Year'] = df['Year'].astype(str)\ndf.loc[:, 'Year'] = df['Year'].str.split('–').str[0].astype(int)\n\n\ndf.loc[:, 'Episodes'] = pd.to_numeric(df['Episodes'].str.extract('(\\d+)')[0], errors='coerce')\n\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\nfeatures = df[['Year', 'Episodes', 'Rating']]\nfeatures = features.dropna()\nscaler = StandardScaler()\nfeatures_scaled = scaler.fit_transform(features)\nnum_clusters = 3\nkmeans = KMeans(n_clusters=num_clusters, random_state=42)\ndf['Cluster'] = kmeans.fit_predict(features_scaled)\nprint(df[['Name', 'Cluster']])\nplt.scatter(features_scaled[:, 0], features_scaled[:, 1], c=df['Cluster'], cmap='viridis')\nplt.xlabel('Year')\nplt.ylabel('Episodes')\nplt.title('K-Means Clustering')\nplt.show()\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\n                               Name  Cluster\n0                   1. Breaking Bad        1\n1                2. Planet Earth II        1\n2                   3. Planet Earth        1\n3               4. Band of Brothers        1\n4                      5. Chernobyl        1\n..                              ...      ...\n240                    241. Gintama        2\n241                  242. Queer Eye        0\n242  243. The Angry Video Game Nerd        2\n243  244. Alfred Hitchcock Presents        2\n244               245. The Night Of        0\n\n[245 rows x 2 columns]\n\n\n\n\n\n\nX = df[['Year', 'Episodes', 'Rating']].dropna()\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=0)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\n\nplt.plot(range(1, 11), wcss, marker='o')\nplt.title('The Elbow Method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS')\nplt.show()\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\n\n\n\n\nnum_clusters = 4\nclusterer = KMeans(n_clusters=num_clusters, random_state=10)\ncluster_labels = clusterer.fit_predict(X)\nprint(\"Cluster Labels:\")\nprint(cluster_labels)\ndf['Cluster'] = cluster_labels\nprint(\"Data with Cluster Labels:\")\nprint(df[['Name', 'Year', 'Episodes', 'Rating', 'Cluster']])\n\nCluster Labels:\n[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1\n 0 1 0 0 0 0 0 0 0 3 0 0 3 0 0 0 1 1 0 0 1 0 1 1 0 0 0 0 0 0 2 0 0 3 0 0 3\n 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0\n 0 3 0 0 0 1 0 0 0 0 1 0 0 3 0 1 0 1 1 3 0 0 0 0 0 0 0 0 0 3 0 0 0 1 0 1 0\n 0 0 1 1 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 3 0 0 1 1 0 1 0 1 0 0 0 3 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1\n 0 0 0 0 3 0 0 0 1 1 0 0 0 0 1 0 0 0 3 0 1 3 0]\nData with Cluster Labels:\n                               Name  Year Episodes  Rating  Cluster\n0                   1. Breaking Bad  2008       62     9.5        0\n1                2. Planet Earth II  2016        6     9.5        0\n2                   3. Planet Earth  2006       11     9.4        0\n3               4. Band of Brothers  2001       10     9.4        0\n4                      5. Chernobyl  2019        5     9.4        0\n..                              ...   ...      ...     ...      ...\n240                    241. Gintama  2005      375     8.7        3\n241                  242. Queer Eye  2018       60     8.5        0\n242  243. The Angry Video Game Nerd  2004      225     8.5        1\n243  244. Alfred Hitchcock Presents  1955      268     8.5        3\n244               245. The Night Of  2016        8     8.4        0\n\n[245 rows x 5 columns]\n\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\n\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nimport matplotlib.cm as cm\nrange_n_clusters = [2, 3, 4, 5, 6]\nfor n_clusters in range_n_clusters:\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    \n    ax1.set_xlim([-0.1, 1])\n    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(\"For n_clusters =\", n_clusters,\n          \"The average silhouette_score is :\", silhouette_avg)\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n    y_lower = 10\n    for i in range(n_clusters):\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n        ith_cluster_silhouette_values.sort()\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n        color = cm.nipy_spectral(float(i) / n_clusters)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n        y_lower = y_upper + 10  \n    ax1.set_title(\"The silhouette plot \")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n    ax1.set_yticks([])  \n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n    ax2.scatter(X.iloc[:, 0], X.iloc[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n                c=colors, edgecolor='k')\n\n    centers = clusterer.cluster_centers_\n    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n                c=\"white\", alpha=1, s=200, edgecolor='k')\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n                    s=50, edgecolor='k')\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                  \"with n_clusters = %d\" % n_clusters),\n                 fontsize=14, fontweight='bold')\n\nplt.show()\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\nFor n_clusters = 2 The average silhouette_score is : 0.7841282051476728\nFor n_clusters = 3 The average silhouette_score is : 0.7443994662641097\nFor n_clusters = 4 The average silhouette_score is : 0.6346534773168201\nFor n_clusters = 5 The average silhouette_score is : 0.5596258728689598\nFor n_clusters = 6 The average silhouette_score is : 0.5557296368153766"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome - Copy/index.html",
    "href": "posts/welcome - Copy/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/Outliner/index.html",
    "href": "posts/Outliner/index.html",
    "title": "Anomaly/Outlier detection",
    "section": "",
    "text": "Outlier Detection\n    An outlier is an object that deviates significantly from the rest of the objects. They can be caused by measurement or execution error. The analysis of outlier data is referred to as outlier analysis or outlier mining.\n    Outlier detection, also known as anomaly detection, is a technique in machine learning that focuses on identifying data points that deviate significantly from the majority of the dataset. Outliers are observations that exhibit unusual behavior compared\n        to the rest of the data and can have a significant impact on the results of statistical analysis and machine learning models.\n    Detecting Outlier\n    In the K-Means clustering technique, each cluster has a mean value. Objects belong to the cluster whose mean value is closest to it. In order to identify the Outlier, firstly we need to initialize the threshold value such that any distance of anydata\n        point greater than it from its nearest cluster identifies it as an outlier for our purpose. Then we need to find the distance of the test data to each cluster mean. Now, if the distance between the test data and the closest cluster to it is greater\n        than the threshold value then we will classify the test data as an outlier\n    Approaches for Outlier Detection\n    1.Statistical Methods\n    \n        Z-Score: Measures how many standard deviations a data point is from the mean. Data points with a high absolute Z-score are considered outliers\n        IQR (Interquartile Range): Defines a range based on the quartiles of the data. Data points outside this range are considered outliers\n    \n    2.Distance Based Methods\n    \n        Euclidean Distance: Measures the straight-line distance between data points. Points that are far from the center or cluster of data may be outliers\n        Mahalanobis Distance: Accounts for correlations between variables and is particularly useful for high-dimensional datasets\n\n        DBSCAN (Density-Based Spatial Clustering of Applications with Noise): Clusters data points based on their density. Outliers are points that do not belong to any cluster\n\n        LOF (Local Outlier Factor): Computes the local density deviation of a data point with respect to its neighbors. Points with low local density compared to neighbors are considered outliers\n    \n    3.Clustering-Based Methods\n    \n        K-Means: Outliers may be points that do not belong to any cluster or belong to a small cluster\n        Hierarchical Clustering: Outliers can be identified by observing small or singleton clusters in the hierarchical structure\n    \n    4.Isolation Forest\n    \n        Constructs an ensemble of isolation trees to isolate outliers. Outliers are expected to have shorter paths in the tree structures\n    \n    5.One-Class SVM (Support Vector Machine)\n    \n        Trains a model on the normal data and identifies outliers as instances lying far from the decision boundary\n    \n    Steps for Outlier Detection\n    \n        Data Exploration: Understand the distribution and characteristics of the dataset.\n        Feature Engineering:Choose relevant features and transformations that enhance outlier detection.\n        Select Detection Method:Choose an appropriate outlier detection method based on the dataset characteristics and the desired sensitivity to outliers.\n        Set Threshold:Define a threshold beyond which data points are considered outliers. The choice of threshold depends on the specific problem.\n        Evaluate and Validate:Evaluate the performance of the outlier detection method using appropriate metrics. Cross-validation may be used to ensure generalization.\n    \n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom IPython.display import Image\nfrom sklearn.preprocessing import StandardScaler\nsns.set(style=\"darkgrid\", palette=\"pastel\", color_codes=True)\nsns.set_context('talk')\n\n\ndf = pd.read_csv('insurance.csv')\n\n\ndf.head(5)\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520\n\n\n\n\n\n\n\n\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nfig = make_subplots(rows=1, cols=len(df.select_dtypes(exclude='object').columns), shared_yaxes=False)\nfor i, col in enumerate(df.select_dtypes(exclude='object').columns):\n    fig.add_trace(go.Box(y=df[col], name=col), row=1, col=i+1)\n\nfor i, col in enumerate(df.select_dtypes(exclude='object').columns):\n    fig.update_xaxes(title_text=col, row=1, col=i+1)\nfig.update_layout(title_text='Box Plots for Numerical Features',title_x=0.5, showlegend=False,width=1000,  height=500 )\nfig.show()\n\n\n                                                \n\n\n\ndef out_iqr(df , column):\n    global lower,upper\n    q25, q75 = np.quantile(df[column], 0.25), np.quantile(df[column], 0.75)\n    iqr = q75 - q25\n    cutOff = iqr * 1.5\n    lower, upper = q25 - cutOff, q75 + cutOff\n    print('The IQR is',iqr)\n    print('The lower bound value is', lower)\n    print('The upper bound value is', upper)\n    df1 = df[df[column] &gt; upper]\n    df2 = df[df[column] &lt; lower]\n    return print('Total number of outliers are', df1.shape[0]+ df2.shape[0])\n\n\nout_iqr(df,'age')\n\nThe IQR is 24.0\nThe lower bound value is -9.0\nThe upper bound value is 87.0\nTotal number of outliers are 0\n\n\n\nimport plotly.express as px\nfig = make_subplots(rows=1, cols=len(df.select_dtypes(exclude='object').columns), shared_yaxes=False)\nfor i, col in enumerate(df.select_dtypes(exclude='object').columns):\n    fig.add_trace(go.Histogram(x=df[col], name=col, ), row=1, col=i+1)\nfor i, col in enumerate(df.select_dtypes(exclude='object').columns):\n    fig.update_xaxes(title_text=col, row=1, col=i+1)\nfig.update_layout(\n    title_text='Histograms for Numerical Features',\n    title_x=0.5,\n    showlegend=False,\n    width=1000,  \n    height=500  \n)\nfig.show()\n\n\n                                                \n\n\n\nfrom collections import Counter\ndef IQR_method(df, n, features):\n    outlier_list = []\n    for column in features:\n        Q1 = np.percentile(df[column], 25)\n        Q3 = np.percentile(df[column], 75)\n        IQR = Q3 - Q1\n        outlier_step = 1.5 * IQR\n        outlier_list_column = df[(df[column] &lt; Q1 - outlier_step) | (df[column] &gt; Q3 + outlier_step)].index\n        outlier_list.extend(outlier_list_column)\n    outlier_list = Counter(outlier_list)\n    multiple_outliers = [k for k, v in outlier_list.items() if v &gt; n]\n    total_outliers = sum(v for k, v in outlier_list.items() if v &gt; n)\n    print('Total number of outliers is:', total_outliers)\n    return multiple_outliers\n\n\nfeature_list = ['age', 'bmi', 'children', 'charges']\nOutliers_IQR = IQR_method(df, 1, feature_list)\ndf_out = df.drop(Outliers_IQR, axis=0).reset_index(drop=True)\n\nTotal number of outliers is: 6\n\n\n\nfeatures = ['age', 'bmi', 'children', 'charges']\nplt.figure(figsize=(16,10))\nfor i, feature in enumerate(features, 1):\n    plt.subplot(2, len(features), i)\n    sns.histplot(df[feature], bins=30, kde=True)\n    plt.title(f'{feature}  Before Dropping Outliers')\nfor i, feature in enumerate(features, 1):\n    plt.subplot(2, len(features), i + len(features))\n    sns.histplot(df_out[feature], bins=30, kde=True)\n    plt.title(f'{feature}  After Dropping Outliers')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nfrom collections import Counter\ndef StDev_method(df, n, features):\n    outlier_indices = []\n    for column in features:\n        mean = df[column].mean()\n        std = df[column].std()\n        cutOff= std * 3\n        outlier_listColumn = df[(df[column] &lt; mean - cutOff) | (df[column] &gt; mean +cutOff)].index\n        outlier_indices.extend(outlier_listColumn)\n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = [k for k, v in outlier_indices.items() if v &gt; n]\n    total_outliers = sum(v for k, v in outlier_indices.items() if v &gt; n)\n    print('Total number of outliers is:', total_outliers)\n    return multiple_outliers\n\n\nOutliers_StDev = StDev_method(df,1,feature_list)\ndf_out2 = df.drop(Outliers_StDev, axis = 0).reset_index(drop=True)\n\nTotal number of outliers is: 0\n\n\n\ndata_mean, data_std = df['charges'].mean(), df['charges'].std()\ncutOff = data_std * 3\nlower, upper = data_mean - cutOff, data_mean + cutOff\nprint('The lower bound value is:', lower)\nprint('The upper bound value is:', upper)\nplt.figure(figsize=(10, 6))\nsns.histplot(x='charges', data=df, bins=70, kde=True)\nplt.axvspan(xmin=lower, xmax=df['charges'].min(), alpha=0.2, color='red', label='Outlier Bound')\nplt.axvspan(xmin=upper, xmax=df['charges'].max(), alpha=0.2, color='red')\nplt.title('Histogram of Charges with Outlier Bound')\nplt.xlabel('Charges')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n\nThe lower bound value is: -23059.611444940747\nThe upper bound value is: 49600.45597522326\n\n\n\n\n\n\nplt.figure(figsize=(16, 8))\nfor i, feature in enumerate(features, 1):\n    plt.subplot(2, len(features), i)\n    sns.scatterplot(x=range(len(df)), y=df[feature], alpha=0.5)\n    plt.title(f'{feature} Before Dropping Outliers')\nfor i, feature in enumerate(features, 1):\n    plt.subplot(2, len(features), i + len(features))\n    sns.scatterplot(x=range(len(df_out2)), y=df_out2[feature], alpha=0.5)\n    plt.title(f'{feature} After Dropping Outliers')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nfrom collections import Counter\ndef z_score_method(df, n, features, threshold=3):\n    outlier_list = []\n    for column in features:\n        mean = df[column].mean()\n        std = df[column].std()\n        zScore = abs((df[column] - mean) / std)\n        outlier_list_column = df[zScore &gt; threshold].index\n        outlier_list.extend(outlier_list_column)\n    outlier_list = Counter(outlier_list)\n    multiple_outliers = [k for k, v in outlier_list.items() if v &gt; n]\n    total_outliers = sum(v for k, v in outlier_list.items() if v &gt; n)\n    print('Total number of outliers is:', total_outliers)\n    return multiple_outliers\n\n\nOutliers_z_score = z_score_method(df,1,feature_list)\ndf_out3 = df.drop(Outliers_z_score, axis = 0).reset_index(drop=True)\n\nTotal number of outliers is: 0\n\n\n\nplt.figure(figsize=(16 , 10))\nfor i, feature in enumerate(features, 1):\n    plt.subplot(3, len(features), i)\n    sns.histplot(df[feature], bins=30, kde=True)\n    plt.title(f'{feature} Before Dropping Outliers')\n    plt.subplot(3, len(features), i + len(features))\n    sns.histplot(df_out[feature], bins=30, kde=True)\n    plt.title(f'{feature} After Dropping Outliers')\n    plt.subplot(3, len(features), i + 2 * len(features))\n    data_mean = df[feature].mean()\n    data_std = df[feature].std()\n    zScore = abs((df[feature] - data_mean) / data_std)\n    sns.scatterplot(x=range(len(df)), y= zScore, alpha=0.5)\n    plt.axhline(y=3, color='red', linestyle='--', label='Threshold')\n    plt.title(f'{feature} Z-scores')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nfrom scipy.stats import median_abs_deviation\ndef z_scoremod_method(df, n, features):\n    outlier_list = []\n    for column in features:\n        data_mean = df[column].mean()\n        threshold = 3\n        MAD = median_abs_deviation(df[column])\n        mod_z_score = abs(0.6745 * (df[column] - data_mean) / MAD)\n        outlier_list_column = df[mod_z_score &gt; threshold].index\n        outlier_list.extend(outlier_list_column)\n    outlier_list = Counter(outlier_list)\n    multiple_outliers = [k for k, v in outlier_list.items() if v &gt; n]\n    total_outliers = sum(v for k, v in outlier_list.items() if v &gt; n)\n    print('Total number of outliers is:', total_outliers)\n    return multiple_outliers\n\n\nOutliers_z_score = z_scoremod_method(df,1,feature_list)\ndf_out4 = df.drop(Outliers_z_score, axis = 0).reset_index(drop=True)\n\nTotal number of outliers is: 2\n\n\n\nplt.figure(figsize=(16, 12))\nfor i, feature in enumerate(features, 1):\n    plt.subplot(3, len(features), i)\n    sns.histplot(df[feature], bins=30, kde=True)\n    plt.title(f'{feature} Before Dropping Outliers')\noutliers_z_scoremod = z_scoremod_method(df, 1, features)\ndf_out_z_scoremod = df.drop(outliers_z_scoremod, axis=0).reset_index(drop=True)\nfor i, feature in enumerate(features, 1):\n    plt.subplot(3, len(features), i + len(features))\n    sns.histplot(df_out_z_scoremod[feature], bins=30, kde=True)\n    plt.title(f'{feature}  Z-score Modified Method')\nplt.tight_layout()\nplt.show()\n\nTotal number of outliers is: 2\n\n\n\n\n\n\nfrom sklearn.ensemble import IsolationForest\nfeatures = ['age', 'bmi', 'children', 'charges']\nclf = IsolationForest(contamination=0.05, random_state=42)  \noutliers = clf.fit_predict(df[features])\noutlier_mask = outliers == -1\nplt.figure(figsize=(20, 8))\nfor i, feature in enumerate(features, 1):\n    plt.subplot(2, len(features), i)\n    sns.histplot(df[feature], bins=30, kde=True)\n    plt.title(f'{feature} Before Dropping Outliers')\nfor i, feature in enumerate(features, 1):\n    plt.subplot(2, len(features), i + len(features))\n    sns.scatterplot(x=range(len(df)), y=df[feature], hue=outlier_mask, palette={True: 'red', False: 'blue'})\n    plt.title(f'{feature} with Outliers Highlighted')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import DBSCAN\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(df[features])\ndbscan = DBSCAN(eps=0.5, min_samples=10)\nlabel = dbscan.fit_predict(scaled_data)\nnclusters = len(set(label)) - (1 if -1 in label else 0)\nprint('The number of clusters in the dataset is:', nclusters)\nprint(pd.Series(label).value_counts())\nfor i, feature1 in enumerate(features):\n    for j, feature2 in enumerate(features):\n        plt.figure(figsize=(10, 6))\n        sns.scatterplot(x=df[feature1], y=df[feature2], hue=label, palette='viridis', alpha=0.7)\n        plt.title(f'{feature1} vs {feature2}')\n        plt.legend(fontsize=\"small\", bbox_to_anchor=(1.05, 1), loc='upper left')\n        plt.show()\n\nThe number of clusters in the dataset is: 10\n-1    433\n 2    417\n 3    206\n 4    136\n 1     61\n 6     22\n 0     19\n 5     13\n 7     11\n 8     10\n 9     10\nName: count, dtype: int64"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Clasification",
    "section": "",
    "text": "Classification\n\n    What is Classification?\n    Classification is a type of supervised machine learning task where the goal is to predict the categorical class labels of new instances based on past observations. In classification, the algorithm is trained on a labeled dataset, where each example\n        has an associated class label, and the model learns to map input features to specific categories.\n    Classification algorithms used in machine learning utilize input training data for the purpose of predicting the likelihood or probability that the data that follows will fall into one of the predetermined categories. One of the most common applications\n        of classification is for filtering emails into “spam” or “non-spam”, as used by today’s top email service providers\n     Binary Classification vs. Multi-Class Classification:\n    \n        Binary Classification\n            In binary classification, there are two possible classes or outcomes (e.g., spam or not spam, positive or negative)\n        \n        Multi-Class Classification\n            In multi-class classification, there are more than two classes (e.g., classifying animals into categories like cats, dogs, and birds)\n        \n    \n    Types of Classification Algorithms\n    \n        Linear Classifiers\n            Algorithms that create a linear decision boundary, such as Logistic Regression or Linear Support Vector Machines (SVM)\n        \n        Non-linear Classifiers\n            Algorithms that can capture non-linear relationships, such as Decision Trees, Random Forests, k-Nearest Neighbors (k-NN), and Support Vector Machines with non-linear kernels\n        \n        Ensemble Methods\n            Techniques that combine multiple base classifiers to improve overall performance, such as AdaBoost, Gradient Boosting, and Random Forests\n        \n        Neural Networks\n            Deep learning models, such as artificial neural networks, are powerful for complex tasks but require a larger amount of data and computational resources\n        \n    \n    \n    Training a Classification Model\n    \n        Dataset\n            A labeled dataset is required, where each instance is associated with a class label\n        \n        Feature Extraction\n            Features are extracted from the input data. These can be numeric or categorical variables\n        \n        Model Training\n        The algorithm learns the mapping between features and class labels using the labeled training data\n    \n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\ndf = pd.read_csv('creditcard.csv')\n\n\ndf.head()\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n...\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\nClass\n\n\n\n\n0\n0.0\n-1.359807\n-0.072781\n2.536347\n1.378155\n-0.338321\n0.462388\n0.239599\n0.098698\n0.363787\n...\n-0.018307\n0.277838\n-0.110474\n0.066928\n0.128539\n-0.189115\n0.133558\n-0.021053\n149.62\n0\n\n\n1\n0.0\n1.191857\n0.266151\n0.166480\n0.448154\n0.060018\n-0.082361\n-0.078803\n0.085102\n-0.255425\n...\n-0.225775\n-0.638672\n0.101288\n-0.339846\n0.167170\n0.125895\n-0.008983\n0.014724\n2.69\n0\n\n\n2\n1.0\n-1.358354\n-1.340163\n1.773209\n0.379780\n-0.503198\n1.800499\n0.791461\n0.247676\n-1.514654\n...\n0.247998\n0.771679\n0.909412\n-0.689281\n-0.327642\n-0.139097\n-0.055353\n-0.059752\n378.66\n0\n\n\n3\n1.0\n-0.966272\n-0.185226\n1.792993\n-0.863291\n-0.010309\n1.247203\n0.237609\n0.377436\n-1.387024\n...\n-0.108300\n0.005274\n-0.190321\n-1.175575\n0.647376\n-0.221929\n0.062723\n0.061458\n123.50\n0\n\n\n4\n2.0\n-1.158233\n0.877737\n1.548718\n0.403034\n-0.407193\n0.095921\n0.592941\n-0.270533\n0.817739\n...\n-0.009431\n0.798278\n-0.137458\n0.141267\n-0.206010\n0.502292\n0.219422\n0.215153\n69.99\n0\n\n\n\n\n5 rows × 31 columns\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n...\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\nClass\n\n\n\n\ncount\n284807.000000\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n...\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n284807.000000\n284807.000000\n\n\nmean\n94813.859575\n1.168375e-15\n3.416908e-16\n-1.379537e-15\n2.074095e-15\n9.604066e-16\n1.487313e-15\n-5.556467e-16\n1.213481e-16\n-2.406331e-15\n...\n1.654067e-16\n-3.568593e-16\n2.578648e-16\n4.473266e-15\n5.340915e-16\n1.683437e-15\n-3.660091e-16\n-1.227390e-16\n88.349619\n0.001727\n\n\nstd\n47488.145955\n1.958696e+00\n1.651309e+00\n1.516255e+00\n1.415869e+00\n1.380247e+00\n1.332271e+00\n1.237094e+00\n1.194353e+00\n1.098632e+00\n...\n7.345240e-01\n7.257016e-01\n6.244603e-01\n6.056471e-01\n5.212781e-01\n4.822270e-01\n4.036325e-01\n3.300833e-01\n250.120109\n0.041527\n\n\nmin\n0.000000\n-5.640751e+01\n-7.271573e+01\n-4.832559e+01\n-5.683171e+00\n-1.137433e+02\n-2.616051e+01\n-4.355724e+01\n-7.321672e+01\n-1.343407e+01\n...\n-3.483038e+01\n-1.093314e+01\n-4.480774e+01\n-2.836627e+00\n-1.029540e+01\n-2.604551e+00\n-2.256568e+01\n-1.543008e+01\n0.000000\n0.000000\n\n\n25%\n54201.500000\n-9.203734e-01\n-5.985499e-01\n-8.903648e-01\n-8.486401e-01\n-6.915971e-01\n-7.682956e-01\n-5.540759e-01\n-2.086297e-01\n-6.430976e-01\n...\n-2.283949e-01\n-5.423504e-01\n-1.618463e-01\n-3.545861e-01\n-3.171451e-01\n-3.269839e-01\n-7.083953e-02\n-5.295979e-02\n5.600000\n0.000000\n\n\n50%\n84692.000000\n1.810880e-02\n6.548556e-02\n1.798463e-01\n-1.984653e-02\n-5.433583e-02\n-2.741871e-01\n4.010308e-02\n2.235804e-02\n-5.142873e-02\n...\n-2.945017e-02\n6.781943e-03\n-1.119293e-02\n4.097606e-02\n1.659350e-02\n-5.213911e-02\n1.342146e-03\n1.124383e-02\n22.000000\n0.000000\n\n\n75%\n139320.500000\n1.315642e+00\n8.037239e-01\n1.027196e+00\n7.433413e-01\n6.119264e-01\n3.985649e-01\n5.704361e-01\n3.273459e-01\n5.971390e-01\n...\n1.863772e-01\n5.285536e-01\n1.476421e-01\n4.395266e-01\n3.507156e-01\n2.409522e-01\n9.104512e-02\n7.827995e-02\n77.165000\n0.000000\n\n\nmax\n172792.000000\n2.454930e+00\n2.205773e+01\n9.382558e+00\n1.687534e+01\n3.480167e+01\n7.330163e+01\n1.205895e+02\n2.000721e+01\n1.559499e+01\n...\n2.720284e+01\n1.050309e+01\n2.252841e+01\n4.584549e+00\n7.519589e+00\n3.517346e+00\n3.161220e+01\n3.384781e+01\n25691.160000\n1.000000\n\n\n\n\n8 rows × 31 columns\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 284807 entries, 0 to 284806\nData columns (total 31 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   Time    284807 non-null  float64\n 1   V1      284807 non-null  float64\n 2   V2      284807 non-null  float64\n 3   V3      284807 non-null  float64\n 4   V4      284807 non-null  float64\n 5   V5      284807 non-null  float64\n 6   V6      284807 non-null  float64\n 7   V7      284807 non-null  float64\n 8   V8      284807 non-null  float64\n 9   V9      284807 non-null  float64\n 10  V10     284807 non-null  float64\n 11  V11     284807 non-null  float64\n 12  V12     284807 non-null  float64\n 13  V13     284807 non-null  float64\n 14  V14     284807 non-null  float64\n 15  V15     284807 non-null  float64\n 16  V16     284807 non-null  float64\n 17  V17     284807 non-null  float64\n 18  V18     284807 non-null  float64\n 19  V19     284807 non-null  float64\n 20  V20     284807 non-null  float64\n 21  V21     284807 non-null  float64\n 22  V22     284807 non-null  float64\n 23  V23     284807 non-null  float64\n 24  V24     284807 non-null  float64\n 25  V25     284807 non-null  float64\n 26  V26     284807 non-null  float64\n 27  V27     284807 non-null  float64\n 28  V28     284807 non-null  float64\n 29  Amount  284807 non-null  float64\n 30  Class   284807 non-null  int64  \ndtypes: float64(30), int64(1)\nmemory usage: 67.4 MB\n\n\n\ndf.shape\n\n(284807, 31)\n\n\n\ndf.columns\n\nIndex(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n       'Class'],\n      dtype='object')\n\n\n\ndf.dropna(inplace=True)\n\n\ndf.isnull().sum()\n\nTime      0\nV1        0\nV2        0\nV3        0\nV4        0\nV5        0\nV6        0\nV7        0\nV8        0\nV9        0\nV10       0\nV11       0\nV12       0\nV13       0\nV14       0\nV15       0\nV16       0\nV17       0\nV18       0\nV19       0\nV20       0\nV21       0\nV22       0\nV23       0\nV24       0\nV25       0\nV26       0\nV27       0\nV28       0\nAmount    0\nClass     0\ndtype: int64\n\n\n\ndf['Class'].unique()\n\narray([0, 1], dtype=int64)\n\n\n\ncorrelation = df.corr()\nsns.heatmap(correlation)\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nsns.countplot(x='Class', data=df)\nplt.xlabel('Class (0: Not Fraud, 1: Fraud)')\nplt.ylabel('Count')\nplt.title('Credit Card Fraud Detection - Class Distribution')\nplt.show()\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n\n\n\n\nplt.figure(figsize=(10, 6))\nsns.histplot(x='Time', data=df, hue='Class', bins=30, kde=True)\nplt.xlabel('Transaction Time')\nplt.ylabel('Count')\nplt.title('Credit Card Fraud Detection - Transaction Time Distribution by Class')\nplt.show()\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 6))\nsns.barplot(x='Class', y='Amount', data=df, palette='Purples')\nplt.xlabel('Class (0: Not Fraud, 1: Fraud)')\nplt.ylabel('Transaction Amount')\nplt.title('Distribution of Transaction Amounts by Class')\nplt.show()\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 6))\nsns.barplot(x='Class', y='Time', data=df, palette='Purples')\nplt.xlabel('Class (0: Not Fraud, 1: Fraud)')\nplt.ylabel('Transaction Time')\nplt.title('Distribution of Transaction Times by Class')\nplt.show()\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 6))\nsns.barplot(x='Class', y='Amount', data=df, estimator=np.mean, palette='Blues')\nplt.xlabel('Class (0: Not Fraud, 1: Fraud)')\nplt.ylabel('Average Transaction Amount')\nplt.title('Average Transaction Amount by Class')\nplt.show()\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n\n\n\n\ndf['Day_of_Week'] = pd.to_datetime(df['Time'], unit='s').dt.day_name()\nplt.figure(figsize=(12, 6))\nsns.countplot(x='Day_of_Week', data=df[df['Class'] == 1], order=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], palette='Greens')\nplt.xlabel('Day of the Week')\nplt.ylabel('Count of Fraudulent Transactions')\nplt.title('Count of Fraudulent Transactions by Day of the Week')\nplt.show()\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n\n\n\n\nsns.stripplot(x='Class', y='Time', data=df, palette='viridis')\nplt.xlabel('Class (0: Not Fraud, 1: Fraud)')\nplt.ylabel('Transaction Time')\nplt.title('Distribution of Transaction Times by Class')\nplt.show()\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_17660\\78416919.py:1: FutureWarning:\n\nPassing `palette` without assigning `hue` is deprecated.\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n\n\n\n\n\n\nsns.boxplot(x=df['Time'], palette='Blues')\nplt.title('Boxplot of Transaction Times')\nplt.xlabel('Transaction Time')\nplt.show()\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n\n\n\n\nsns.lmplot(x='Amount', y='V25', data=df, hue='Class', palette='Set1', markers=['o', 's'], scatter_kws={'s': 50})\nplt.title('lmplot with Amount and V25')\nplt.show()\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n\n\n\n\nsns.lmplot(x='Amount', y='V1', data=df, hue='Class', palette='Set1', markers=['o', 's'], scatter_kws={'s': 50})\nplt.title('lmplot with Amount and V1')\nplt.show()\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n\n\n\n\n\n\nfrom sklearn.decomposition import PCA\nX = df.drop(columns='Class')\ny = df['Class']\n\n\nX.head()\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n...\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\nDay_of_Week\n\n\n\n\n0\n0.0\n-1.359807\n-0.072781\n2.536347\n1.378155\n-0.338321\n0.462388\n0.239599\n0.098698\n0.363787\n...\n-0.018307\n0.277838\n-0.110474\n0.066928\n0.128539\n-0.189115\n0.133558\n-0.021053\n149.62\nThursday\n\n\n1\n0.0\n1.191857\n0.266151\n0.166480\n0.448154\n0.060018\n-0.082361\n-0.078803\n0.085102\n-0.255425\n...\n-0.225775\n-0.638672\n0.101288\n-0.339846\n0.167170\n0.125895\n-0.008983\n0.014724\n2.69\nThursday\n\n\n2\n1.0\n-1.358354\n-1.340163\n1.773209\n0.379780\n-0.503198\n1.800499\n0.791461\n0.247676\n-1.514654\n...\n0.247998\n0.771679\n0.909412\n-0.689281\n-0.327642\n-0.139097\n-0.055353\n-0.059752\n378.66\nThursday\n\n\n3\n1.0\n-0.966272\n-0.185226\n1.792993\n-0.863291\n-0.010309\n1.247203\n0.237609\n0.377436\n-1.387024\n...\n-0.108300\n0.005274\n-0.190321\n-1.175575\n0.647376\n-0.221929\n0.062723\n0.061458\n123.50\nThursday\n\n\n4\n2.0\n-1.158233\n0.877737\n1.548718\n0.403034\n-0.407193\n0.095921\n0.592941\n-0.270533\n0.817739\n...\n-0.009431\n0.798278\n-0.137458\n0.141267\n-0.206010\n0.502292\n0.219422\n0.215153\n69.99\nThursday\n\n\n\n\n5 rows × 31 columns\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\n\nfrom sklearn.preprocessing import StandardScaler\nX_train_numeric = X_train.select_dtypes(exclude=['object'])\nscaler = StandardScaler()\nscaler.fit(X_train_numeric)\nX_train_scaled = scaler.transform(X_train_numeric)\nX_train_scaled\n\narray([[ 0.4628655 , -0.76417848, -0.58517942, ..., -0.05427856,\n         0.47278134,  0.27637606],\n       [ 0.99884641, -0.43199832,  0.8362486 , ..., -0.21155863,\n        -0.17561255, -0.20845219],\n       [-1.06243719, -0.5473776 ,  0.36358059, ..., -0.1751781 ,\n         0.27717173, -0.30005801],\n       ...,\n       [-0.31423311, -0.07216301,  0.59345235, ..., -0.29402614,\n        -0.59027941, -0.32887389],\n       [-0.1428877 , -1.49506753,  1.40403542, ...,  1.21908694,\n         1.01135271, -0.34027614],\n       [-0.38613248,  0.62850772, -0.46466388, ...,  0.00552523,\n         0.11653329,  0.09409522]])\n\n\n\nX_train_encoded = pd.get_dummies(X_train)\nscaler.fit(X_train_encoded)\nX_train_scaled = scaler.transform(X_train_encoded)\nX_train_scaled\n\narray([[ 0.4628655 , -0.76417848, -0.58517942, ...,  0.27637606,\n         1.0191673 , -1.0191673 ],\n       [ 0.99884641, -0.43199832,  0.8362486 , ..., -0.20845219,\n         1.0191673 , -1.0191673 ],\n       [-1.06243719, -0.5473776 ,  0.36358059, ..., -0.30005801,\n        -0.98119318,  0.98119318],\n       ...,\n       [-0.31423311, -0.07216301,  0.59345235, ..., -0.32887389,\n        -0.98119318,  0.98119318],\n       [-0.1428877 , -1.49506753,  1.40403542, ..., -0.34027614,\n         1.0191673 , -1.0191673 ],\n       [-0.38613248,  0.62850772, -0.46466388, ...,  0.09409522,\n        -0.98119318,  0.98119318]])\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport pandas as pd\nclf = LogisticRegression(random_state=0, solver='sag', max_iter=1000)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\nnumeric_features = X.select_dtypes(include=['float64']).columns\ncategorical_features = X.select_dtypes(include=['object']).columns\nnumeric_transformer = Pipeline(steps=[\n    ('scaler', StandardScaler())\n])\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000))])\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(f'Accuracy: {accuracy:.4f}')\nprint('\\nConfusion Matrix:')\nprint(conf_matrix)\n\nAccuracy: 0.9992\n\nConfusion Matrix:\n[[113718     14]\n [    74    117]]\n\n\n\nclf.score(X_test, y_test)\n\n0.9992275484318356\n\n\n\nclf.score(X_train, y_train)\n\n0.9991982865569626\n\n\n\nfrom sklearn.metrics import classification_report\ntarget_names = ['not_fraud', 'fraud']\nprint(classification_report(y_test, y_pred, target_names=target_names))\n\n              precision    recall  f1-score   support\n\n   not_fraud       1.00      1.00      1.00    113732\n       fraud       0.89      0.61      0.73       191\n\n    accuracy                           1.00    113923\n   macro avg       0.95      0.81      0.86    113923\nweighted avg       1.00      1.00      1.00    113923\n\n\n\n\nfrom sklearn.metrics import log_loss\ny_pred_proba = clf.predict_proba(X_test)[:, 1]\nloss = log_loss(y_test, y_pred_proba)\nprint(f'Log Loss: {loss:.4f}')\n\nLog Loss: 0.0038\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.naive_bayes import GaussianNB\nX_train_numeric, X_test_numeric, y_train, y_test = train_test_split(X_train_numeric, y_train, test_size=0.4, random_state=42)\nscaler = StandardScaler()\nX_train_scaled_numeric = scaler.fit_transform(X_train_numeric)\nX_test_scaled_numeric = scaler.transform(X_test_numeric)\nclf = GaussianNB()\nclf.fit(X_train_scaled_numeric, y_train)\ny_pred = clf.predict(X_test_scaled_numeric)\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclassification_report_str = classification_report(y_test, y_pred)\nprint(f'Accuracy: {accuracy:.4f}')\nprint('\\nConfusion Matrix:')\nprint(conf_matrix)\nprint('\\nClassification Report:')\nprint(classification_report_str)\n\nAccuracy: 0.9774\n\nConfusion Matrix:\n[[66706  1530]\n [   15   103]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      0.98      0.99     68236\n           1       0.06      0.87      0.12       118\n\n    accuracy                           0.98     68354\n   macro avg       0.53      0.93      0.55     68354\nweighted avg       1.00      0.98      0.99     68354\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport pandas as pd\nX_train_numeric, X_test_numeric, y_train, y_test = train_test_split(X_train_numeric, y_train, test_size=0.4, random_state=42)\nscaler = StandardScaler()\nX_train_scaled_numeric = scaler.fit_transform(X_train_numeric)\nX_test_scaled_numeric = scaler.transform(X_test_numeric)\ndt_classifier = DecisionTreeClassifier(random_state=0)\ndt_classifier.fit(X_train_scaled_numeric, y_train)\ny_pred_dt = dt_classifier.predict(X_test_scaled_numeric)\naccuracy_dt = accuracy_score(y_test, y_pred_dt)\nconf_matrix_dt = confusion_matrix(y_test, y_pred_dt)\nclassification_report_dt = classification_report(y_test, y_pred_dt)\nprint(f'Decision Tree Classifier:')\nprint(f'Accuracy: {accuracy_dt:.4f}')\nprint('\\nConfusion Matrix:')\nprint(conf_matrix_dt)\nprint('\\nClassification Report:')\nprint(classification_report_dt)\n\nDecision Tree Classifier:\nAccuracy: 0.9987\n\nConfusion Matrix:\n[[40919    34]\n [   20    39]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     40953\n           1       0.53      0.66      0.59        59\n\n    accuracy                           1.00     41012\n   macro avg       0.77      0.83      0.80     41012\nweighted avg       1.00      1.00      1.00     41012\n\n\n\n\nfrom sklearn.svm import SVC\nX_train_numeric, X_test_numeric, y_train, y_test = train_test_split(X_train_numeric, y_train, test_size=0.4, random_state=42)\nscaler = StandardScaler()\nX_train_scaled_numeric = scaler.fit_transform(X_train_numeric)\nX_test_scaled_numeric = scaler.transform(X_test_numeric)\nsvm_classifier = SVC(random_state=0)\nsvm_classifier.fit(X_train_scaled_numeric, y_train)\ny_pred_svm = svm_classifier.predict(X_test_scaled_numeric)\naccuracy_svm = accuracy_score(y_test, y_pred_svm)\nconf_matrix_svm = confusion_matrix(y_test, y_pred_svm)\nclassification_report_svm = classification_report(y_test, y_pred_svm)\nprint(f'SVM Classifier:')\nprint(f'Accuracy: {accuracy_svm:.4f}')\nprint('\\nConfusion Matrix:')\nprint(conf_matrix_svm)\nprint('\\nClassification Report:')\nprint(classification_report_svm)\n\nSVM Classifier:\nAccuracy: 0.9990\n\nConfusion Matrix:\n[[24568     0]\n [   25    15]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     24568\n           1       1.00      0.38      0.55        40\n\n    accuracy                           1.00     24608\n   macro avg       1.00      0.69      0.77     24608\nweighted avg       1.00      1.00      1.00     24608\n\n\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\nX_train_numeric, X_test_numeric, y_train, y_test = train_test_split(X_train_numeric, y_train, test_size=0.4, random_state=42)\nscaler = StandardScaler()\nX_train_scaled_numeric = scaler.fit_transform(X_train_numeric)\nX_test_scaled_numeric = scaler.transform(X_test_numeric)\nknn_classifier = KNeighborsClassifier()\nknn_classifier.fit(X_train_scaled_numeric, y_train)\ny_pred_knn = knn_classifier.predict(X_test_scaled_numeric)\naccuracy_knn = accuracy_score(y_test, y_pred_knn)\nconf_matrix_knn = confusion_matrix(y_test, y_pred_knn)\nclassification_report_knn = classification_report(y_test, y_pred_knn)\nprint(f'K-Nearest Neighbors Classifier:')\nprint(f'Accuracy: {accuracy_knn:.4f}')\nprint('\\nConfusion Matrix:')\nprint(conf_matrix_knn)\nprint('\\nClassification Report:')\nprint(classification_report_knn)\n\nK-Nearest Neighbors Classifier:\nAccuracy: 0.9988\n\nConfusion Matrix:\n[[14725     3]\n [   15    21]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     14728\n           1       0.88      0.58      0.70        36\n\n    accuracy                           1.00     14764\n   macro avg       0.94      0.79      0.85     14764\nweighted avg       1.00      1.00      1.00     14764\n\n\n\n\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Dense,Activation,Flatten\nfrom tensorflow.keras import Sequential\n\nWARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n\n\n\n\nmodel = Sequential()\nmodel.add(Flatten(input_shape=(X_train_scaled_numeric.shape[1],)))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nearlystop = EarlyStopping(monitor='val_loss', patience=2, verbose=0, mode='min')\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.fit(X_train_scaled_numeric, y_train, epochs=10, validation_data=(X_test_scaled_numeric, y_test), callbacks=[earlystop])\ny_pred_probs = model.predict(X_test_scaled_numeric)\ny_pred_nn = (y_pred_probs &gt; 0.5).astype(int)  \naccuracy_nn = accuracy_score(y_test, y_pred_nn)\nconf_matrix_nn = confusion_matrix(y_test, y_pred_nn)\nclassification_report_nn = classification_report(y_test, y_pred_nn)\nprint(f'Neural Network Classifier:')\nprint(f'Accuracy: {accuracy_nn:.4f}')\nprint('\\nConfusion Matrix:')\nprint(conf_matrix_nn)\nprint('\\nClassification Report:')\nprint(classification_report_nn)\n\nWARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nWARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\nEpoch 1/10\nWARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n\nWARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n\n  1/693 [..............................] - ETA: 8:09 - loss: 0.6898 - accuracy: 0.4375\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 48/693 [=&gt;............................] - ETA: 0s - loss: 0.4806 - accuracy: 0.8477  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b100/693 [===&gt;..........................] - ETA: 0s - loss: 0.3245 - accuracy: 0.9262\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b127/693 [====&gt;.........................] - ETA: 0s - loss: 0.2715 - accuracy: 0.9417\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b172/693 [======&gt;.......................] - ETA: 0s - loss: 0.2149 - accuracy: 0.9564\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b225/693 [========&gt;.....................] - ETA: 0s - loss: 0.1730 - accuracy: 0.9663\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b273/693 [==========&gt;...................] - ETA: 0s - loss: 0.1466 - accuracy: 0.9718\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b306/693 [============&gt;.................] - ETA: 0s - loss: 0.1348 - accuracy: 0.9745\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b350/693 [==============&gt;...............] - ETA: 0s - loss: 0.1187 - accuracy: 0.9777\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b404/693 [================&gt;.............] - ETA: 0s - loss: 0.1041 - accuracy: 0.9804\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b454/693 [==================&gt;...........] - ETA: 0s - loss: 0.0931 - accuracy: 0.9825\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b501/693 [====================&gt;.........] - ETA: 0s - loss: 0.0852 - accuracy: 0.9839\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b552/693 [======================&gt;.......] - ETA: 0s - loss: 0.0783 - accuracy: 0.9853\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b607/693 [=========================&gt;....] - ETA: 0s - loss: 0.0719 - accuracy: 0.9865\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b662/693 [===========================&gt;..] - ETA: 0s - loss: 0.0661 - accuracy: 0.9876\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b693/693 [==============================] - 2s 2ms/step - loss: 0.0637 - accuracy: 0.9881 - val_loss: 0.0086 - val_accuracy: 0.9988\nEpoch 2/10\n  1/693 [..............................] - ETA: 1s - loss: 0.0016 - accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 60/693 [=&gt;............................] - ETA: 0s - loss: 0.0040 - accuracy: 0.9995\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b118/693 [====&gt;.........................] - ETA: 0s - loss: 0.0027 - accuracy: 0.9997\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b181/693 [======&gt;.......................] - ETA: 0s - loss: 0.0039 - accuracy: 0.9995\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b239/693 [=========&gt;....................] - ETA: 0s - loss: 0.0038 - accuracy: 0.9992\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b296/693 [===========&gt;..................] - ETA: 0s - loss: 0.0041 - accuracy: 0.9990\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b352/693 [==============&gt;...............] - ETA: 0s - loss: 0.0037 - accuracy: 0.9991\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b410/693 [================&gt;.............] - ETA: 0s - loss: 0.0043 - accuracy: 0.9991\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b468/693 [===================&gt;..........] - ETA: 0s - loss: 0.0047 - accuracy: 0.9991\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b527/693 [=====================&gt;........] - ETA: 0s - loss: 0.0054 - accuracy: 0.9989\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b585/693 [========================&gt;.....] - ETA: 0s - loss: 0.0060 - accuracy: 0.9988\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b642/693 [==========================&gt;...] - ETA: 0s - loss: 0.0056 - accuracy: 0.9989\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b693/693 [==============================] - 1s 1ms/step - loss: 0.0055 - accuracy: 0.9989 - val_loss: 0.0074 - val_accuracy: 0.9988\nEpoch 3/10\n  1/693 [..............................] - ETA: 0s - loss: 2.7365e-04 - accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 64/693 [=&gt;............................] - ETA: 0s - loss: 0.0033 - accuracy: 0.9995    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b125/693 [====&gt;.........................] - ETA: 0s - loss: 0.0019 - accuracy: 0.9998\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b186/693 [=======&gt;......................] - ETA: 0s - loss: 0.0015 - accuracy: 0.9998\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b244/693 [=========&gt;....................] - ETA: 0s - loss: 0.0021 - accuracy: 0.9995\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b304/693 [============&gt;.................] - ETA: 0s - loss: 0.0020 - accuracy: 0.9995\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b366/693 [==============&gt;...............] - ETA: 0s - loss: 0.0020 - accuracy: 0.9994\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b427/693 [=================&gt;............] - ETA: 0s - loss: 0.0021 - accuracy: 0.9993\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b463/693 [===================&gt;..........] - ETA: 0s - loss: 0.0021 - accuracy: 0.9993\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b529/693 [=====================&gt;........] - ETA: 0s - loss: 0.0025 - accuracy: 0.9993\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b593/693 [========================&gt;.....] - ETA: 0s - loss: 0.0026 - accuracy: 0.9993\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b657/693 [===========================&gt;..] - ETA: 0s - loss: 0.0032 - accuracy: 0.9992\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b693/693 [==============================] - 1s 1ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.0081 - val_accuracy: 0.9985\nEpoch 4/10\n  1/693 [..............................] - ETA: 0s - loss: 2.7800e-04 - accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 64/693 [=&gt;............................] - ETA: 0s - loss: 0.0025 - accuracy: 0.9985    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b127/693 [====&gt;.........................] - ETA: 0s - loss: 0.0020 - accuracy: 0.9988\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b193/693 [=======&gt;......................] - ETA: 0s - loss: 0.0022 - accuracy: 0.9990\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b256/693 [==========&gt;...................] - ETA: 0s - loss: 0.0036 - accuracy: 0.9990\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b321/693 [============&gt;.................] - ETA: 0s - loss: 0.0035 - accuracy: 0.9989\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b384/693 [===============&gt;..............] - ETA: 0s - loss: 0.0030 - accuracy: 0.9991\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b448/693 [==================&gt;...........] - ETA: 0s - loss: 0.0031 - accuracy: 0.9991\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b513/693 [=====================&gt;........] - ETA: 0s - loss: 0.0033 - accuracy: 0.9990\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b579/693 [========================&gt;.....] - ETA: 0s - loss: 0.0032 - accuracy: 0.9990\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b646/693 [==========================&gt;...] - ETA: 0s - loss: 0.0030 - accuracy: 0.9991\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b693/693 [==============================] - 1s 1ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0080 - val_accuracy: 0.9988\n  1/462 [..............................] - ETA: 28s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 78/462 [====&gt;.........................] - ETA: 0s \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b155/462 [=========&gt;....................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b235/462 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b311/462 [===================&gt;..........] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b389/462 [========================&gt;.....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b462/462 [==============================] - 0s 647us/step\nNeural Network Classifier:\nAccuracy: 0.9988\n\nConfusion Matrix:\n[[14722     6]\n [   12    24]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     14728\n           1       0.80      0.67      0.73        36\n\n    accuracy                           1.00     14764\n   macro avg       0.90      0.83      0.86     14764\nweighted avg       1.00      1.00      1.00     14764\n\n\n\n\nhistory = model.fit(X_train_scaled_numeric, y_train, epochs=10, validation_data=(X_test_scaled_numeric, y_test), callbacks=[earlystop])\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(['Train', 'Validation'], loc='upper right')\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['Train', 'Validation'], loc='lower right')\nplt.tight_layout()\nplt.show()\n\nEpoch 1/10\n  1/693 [..............................] - ETA: 2s - loss: 0.0012 - accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 63/693 [=&gt;............................] - ETA: 0s - loss: 0.0014 - accuracy: 0.9995\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b128/693 [====&gt;.........................] - ETA: 0s - loss: 0.0029 - accuracy: 0.9993\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b193/693 [=======&gt;......................] - ETA: 0s - loss: 0.0037 - accuracy: 0.9990\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b258/693 [==========&gt;...................] - ETA: 0s - loss: 0.0028 - accuracy: 0.9993\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b323/693 [============&gt;.................] - ETA: 0s - loss: 0.0024 - accuracy: 0.9994\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b387/693 [===============&gt;..............] - ETA: 0s - loss: 0.0021 - accuracy: 0.9994\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b451/693 [==================&gt;...........] - ETA: 0s - loss: 0.0023 - accuracy: 0.9994\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b513/693 [=====================&gt;........] - ETA: 0s - loss: 0.0025 - accuracy: 0.9993\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b573/693 [=======================&gt;......] - ETA: 0s - loss: 0.0026 - accuracy: 0.9993\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b633/693 [==========================&gt;...] - ETA: 0s - loss: 0.0026 - accuracy: 0.9993\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b693/693 [==============================] - 1s 1ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0090 - val_accuracy: 0.9986\nEpoch 2/10\n  1/693 [..............................] - ETA: 0s - loss: 1.5978e-04 - accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65/693 [=&gt;............................] - ETA: 0s - loss: 0.0018 - accuracy: 0.9986    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b128/693 [====&gt;.........................] - ETA: 0s - loss: 0.0016 - accuracy: 0.9990\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b194/693 [=======&gt;......................] - ETA: 0s - loss: 0.0016 - accuracy: 0.9992\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b259/693 [==========&gt;...................] - ETA: 0s - loss: 0.0013 - accuracy: 0.9994\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b325/693 [=============&gt;................] - ETA: 0s - loss: 0.0013 - accuracy: 0.9994\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b389/693 [===============&gt;..............] - ETA: 0s - loss: 0.0013 - accuracy: 0.9994\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b455/693 [==================&gt;...........] - ETA: 0s - loss: 0.0016 - accuracy: 0.9995\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b520/693 [=====================&gt;........] - ETA: 0s - loss: 0.0020 - accuracy: 0.9992\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b584/693 [========================&gt;.....] - ETA: 0s - loss: 0.0024 - accuracy: 0.9990\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b648/693 [===========================&gt;..] - ETA: 0s - loss: 0.0023 - accuracy: 0.9991\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b693/693 [==============================] - 1s 1ms/step - loss: 0.0022 - accuracy: 0.9991 - val_loss: 0.0095 - val_accuracy: 0.9988\nEpoch 3/10\n  1/693 [..............................] - ETA: 1s - loss: 6.3921e-05 - accuracy: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 60/693 [=&gt;............................] - ETA: 0s - loss: 0.0026 - accuracy: 0.9990    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b120/693 [====&gt;.........................] - ETA: 0s - loss: 0.0017 - accuracy: 0.9995\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b178/693 [======&gt;.......................] - ETA: 0s - loss: 0.0017 - accuracy: 0.9995\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b243/693 [=========&gt;....................] - ETA: 0s - loss: 0.0013 - accuracy: 0.9996\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b308/693 [============&gt;.................] - ETA: 0s - loss: 0.0018 - accuracy: 0.9996\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b373/693 [===============&gt;..............] - ETA: 0s - loss: 0.0016 - accuracy: 0.9996\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b438/693 [=================&gt;............] - ETA: 0s - loss: 0.0019 - accuracy: 0.9994\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b502/693 [====================&gt;.........] - ETA: 0s - loss: 0.0017 - accuracy: 0.9995\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b567/693 [=======================&gt;......] - ETA: 0s - loss: 0.0016 - accuracy: 0.9996\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b632/693 [==========================&gt;...] - ETA: 0s - loss: 0.0019 - accuracy: 0.9995\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b693/693 [==============================] - 1s 1ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.0099 - val_accuracy: 0.9988\n\n\n\n\n\n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport pandas as pd\n\nX, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_train, y_train)\ny_train_series = pd.Series(y_train)\nprint(\"Original Training Set Class Distribution:\")\nprint(y_train_series.value_counts())\nprint(\"\\nResampled Training Set Class Distribution:\")\nprint(pd.Series(y_resampled).value_counts())\nprint(\"\\nTest Set Class Distribution:\")\nprint(pd.Series(y_test).value_counts())\nscaler = StandardScaler()\nX_resampled_scaled = scaler.fit_transform(X_resampled)\nX_test_scaled = scaler.transform(X_test)\nknn_classifier = KNeighborsClassifier()\nknn_classifier.fit(X_resampled_scaled, y_resampled)\ny_pred_knn = knn_classifier.predict(X_test_scaled)\naccuracy_knn = accuracy_score(y_test, y_pred_knn)\nconf_matrix_knn = confusion_matrix(y_test, y_pred_knn)\nclassification_report_knn = classification_report(y_test, y_pred_knn)\nprint(f'\\nK-Nearest Neighbors Classifier:')\nprint(f'Accuracy: {accuracy_knn:.4f}')\nprint('\\nConfusion Matrix:')\nprint(conf_matrix_knn)\nprint('\\nClassification Report:')\nprint(classification_report_knn)\n\nOriginal Training Set Class Distribution:\n1    627\n0     73\nName: count, dtype: int64\n\nResampled Training Set Class Distribution:\n1    627\n0    627\nName: count, dtype: int64\n\nTest Set Class Distribution:\n1    273\n0     27\nName: count, dtype: int64\n\nK-Nearest Neighbors Classifier:\nAccuracy: 0.9267\n\nConfusion Matrix:\n[[ 24   3]\n [ 19 254]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.56      0.89      0.69        27\n           1       0.99      0.93      0.96       273\n\n    accuracy                           0.93       300\n   macro avg       0.77      0.91      0.82       300\nweighted avg       0.95      0.93      0.93       300\n\n\n\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport pandas as pd\n\nX, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0, n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X_train, y_train)\ny_resampled_series = pd.Series(y_resampled)\nprint(\"Resampled Training Set Class Distribution:\")\nprint(y_resampled_series.value_counts())\nscaler = StandardScaler()\nX_resampled_scaled = scaler.fit_transform(X_resampled)\nX_test_scaled = scaler.transform(X_test)\nknn_classifier = KNeighborsClassifier()\nknn_classifier.fit(X_resampled_scaled, y_resampled)\ny_pred_knn = knn_classifier.predict(X_test_scaled)\naccuracy_knn = accuracy_score(y_test, y_pred_knn)\nconf_matrix_knn = confusion_matrix(y_test, y_pred_knn)\nclassification_report_knn = classification_report(y_test, y_pred_knn)\nprint(f'\\nK-Nearest Neighbors Classifier:')\nprint(f'Accuracy: {accuracy_knn:.4f}')\nprint('\\nConfusion Matrix:')\nprint(conf_matrix_knn)\nprint('\\nClassification Report:')\nprint(classification_report_knn)\n\nResampled Training Set Class Distribution:\n0    73\n1    73\nName: count, dtype: int64\n\nK-Nearest Neighbors Classifier:\nAccuracy: 0.9200\n\nConfusion Matrix:\n[[ 27   0]\n [ 24 249]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.53      1.00      0.69        27\n           1       1.00      0.91      0.95       273\n\n    accuracy                           0.92       300\n   macro avg       0.76      0.96      0.82       300\nweighted avg       0.96      0.92      0.93       300"
  },
  {
    "objectID": "posts/Linear and nonlinear regression/index.html",
    "href": "posts/Linear and nonlinear regression/index.html",
    "title": "Linear and nonlinear regression",
    "section": "",
    "text": "a)Linear Regression\n \n    Index\n    \n        What is Linear Regression\n        Types of Linear Regression\n        Training a Linear Regression Model\n        Finding the best fit line\n    \n    What is Linear Regression\n    Linear regression is one of the easiest and most popular Machine Learning algorithms. It is a statistical method that is used for predictive analysis. Linear regression makes predictions for continuous/real or numeric variables such as sales, salary,\n        age, product price, etc.\n    Mathematically, we can represent a linear regression as:\n    y= a0+a1x+ ε\n    \n        Y= Dependent Variable (Target Variable)\n        X= Independent Variable (predictor Variable)\n        a0= intercept of the line (Gives an additional degree of freedom)\n        a1 = Linear regression coefficient (scale factor to each input value).\n        ε = random error\n    \n     The values for x and y variables are training datasets for Linear Regression model representation.\n    Types of Linear Regression\n    \n         Simple Linear Regression\n            If a single independent variable is used to predict the value of a numerical dependent variable, then such a Linear Regression algorithm is called Simple Linear Regression.\n             Y=β 0+β 1⋅X+ε\n            \n                \n                    Y is the dependent variable.\n                \n                X is the independent variable.\n                β 0is the intercept (where the line crosses theY-axis).\n                β 1 is the slop (the change in Y for a one-unitchange in X).\n                ε is the error term, representing the variability in Y that is not explained by the linear relationship withX\n            \n        \n         Multiple Linear Regression\n            If more than one independent variable is used to predict the value of a numerical dependent variable, then such a Linear Regression algorithm is called Multiple Linear Regression.\n             Y=β 0+β 1⋅X 1 +β 2⋅X 2 +…+β n⋅X n+ε \n            \n                \n                    Y is the dependent variable.\n                \n                X 1,X 2,…,Xn are the independent variables\n                β 0is the intercept \n                β1,β 2,…,β nare the coefficients representing the impact of each independent variable.\n                ε is the error term\n            \n        \n    \n    Training a Linear Regression Model\n    The process of training a linear regression model involves finding the values of the coefficients (β0,β 1,…,βn) that minimize the sum of squared differences between the predicted and actual values in the training data.\n    Finding the best fit line\n    goal is to find the best fit line that means the error between predicted values and actual values should be minimized. The best fit line will have the least error. The different values for weights or the coefficient of lines (a0, a1) gives a different\n        line of regression, so we need to calculate the best values for a0 and a1 to find the best fit line, so to calculate this we use cost function.\n    Continuous random variable\n    A continuous random variable has two main characteristics:\n    the set of its possible values is uncountable;\nwe compute the probability that its value will belong to a given interval by integrating a function called probability density function.\n    Common continuous distributions\n    The next table contains some examples of continuous distributions that are frequently encountered in probability theory and statistics.\n    \n\n\n\nName of the continuous distribution\nSupport\n\n\nUniform\nAll the real numbers in the interval [0,1]\n\n\nNormal\nThe whole set of real numbers\n\n\nChi-square\nThe set of all non-negative real numbers\n\n\n\n\n    \n\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nimport warnings\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings('ignore')\n\n\ndf = pd.read_csv('CarPrice_Assignment.csv')\ndf.head()\n\n\n\n\n\n\n\n\ncar_ID\nsymboling\nCarName\nfueltype\naspiration\ndoornumber\ncarbody\ndrivewheel\nenginelocation\nwheelbase\n...\nenginesize\nfuelsystem\nboreratio\nstroke\ncompressionratio\nhorsepower\npeakrpm\ncitympg\nhighwaympg\nprice\n\n\n\n\n0\n1\n3\nalfa-romero giulia\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n...\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n13495.0\n\n\n1\n2\n3\nalfa-romero stelvio\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n...\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n16500.0\n\n\n2\n3\n1\nalfa-romero Quadrifoglio\ngas\nstd\ntwo\nhatchback\nrwd\nfront\n94.5\n...\n152\nmpfi\n2.68\n3.47\n9.0\n154\n5000\n19\n26\n16500.0\n\n\n3\n4\n2\naudi 100 ls\ngas\nstd\nfour\nsedan\nfwd\nfront\n99.8\n...\n109\nmpfi\n3.19\n3.40\n10.0\n102\n5500\n24\n30\n13950.0\n\n\n4\n5\n2\naudi 100ls\ngas\nstd\nfour\nsedan\n4wd\nfront\n99.4\n...\n136\nmpfi\n3.19\n3.40\n8.0\n115\n5500\n18\n22\n17450.0\n\n\n\n\n5 rows × 26 columns\n\n\n\n\ndf.columns\n\nIndex(['car_ID', 'symboling', 'CarName', 'fueltype', 'aspiration',\n       'doornumber', 'carbody', 'drivewheel', 'enginelocation', 'wheelbase',\n       'carlength', 'carwidth', 'carheight', 'curbweight', 'enginetype',\n       'cylindernumber', 'enginesize', 'fuelsystem', 'boreratio', 'stroke',\n       'compressionratio', 'horsepower', 'peakrpm', 'citympg', 'highwaympg',\n       'price'],\n      dtype='object')\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 205 entries, 0 to 204\nData columns (total 26 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   car_ID            205 non-null    int64  \n 1   symboling         205 non-null    int64  \n 2   CarName           205 non-null    object \n 3   fueltype          205 non-null    object \n 4   aspiration        205 non-null    object \n 5   doornumber        205 non-null    object \n 6   carbody           205 non-null    object \n 7   drivewheel        205 non-null    object \n 8   enginelocation    205 non-null    object \n 9   wheelbase         205 non-null    float64\n 10  carlength         205 non-null    float64\n 11  carwidth          205 non-null    float64\n 12  carheight         205 non-null    float64\n 13  curbweight        205 non-null    int64  \n 14  enginetype        205 non-null    object \n 15  cylindernumber    205 non-null    object \n 16  enginesize        205 non-null    int64  \n 17  fuelsystem        205 non-null    object \n 18  boreratio         205 non-null    float64\n 19  stroke            205 non-null    float64\n 20  compressionratio  205 non-null    float64\n 21  horsepower        205 non-null    int64  \n 22  peakrpm           205 non-null    int64  \n 23  citympg           205 non-null    int64  \n 24  highwaympg        205 non-null    int64  \n 25  price             205 non-null    float64\ndtypes: float64(8), int64(8), object(10)\nmemory usage: 41.8+ KB\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\ncar_ID\nsymboling\nwheelbase\ncarlength\ncarwidth\ncarheight\ncurbweight\nenginesize\nboreratio\nstroke\ncompressionratio\nhorsepower\npeakrpm\ncitympg\nhighwaympg\nprice\n\n\n\n\ncount\n205.000000\n205.000000\n205.000000\n205.000000\n205.000000\n205.000000\n205.000000\n205.000000\n205.000000\n205.000000\n205.000000\n205.000000\n205.000000\n205.000000\n205.000000\n205.000000\n\n\nmean\n103.000000\n0.834146\n98.756585\n174.049268\n65.907805\n53.724878\n2555.565854\n126.907317\n3.329756\n3.255415\n10.142537\n104.117073\n5125.121951\n25.219512\n30.751220\n13276.710571\n\n\nstd\n59.322565\n1.245307\n6.021776\n12.337289\n2.145204\n2.443522\n520.680204\n41.642693\n0.270844\n0.313597\n3.972040\n39.544167\n476.985643\n6.542142\n6.886443\n7988.852332\n\n\nmin\n1.000000\n-2.000000\n86.600000\n141.100000\n60.300000\n47.800000\n1488.000000\n61.000000\n2.540000\n2.070000\n7.000000\n48.000000\n4150.000000\n13.000000\n16.000000\n5118.000000\n\n\n25%\n52.000000\n0.000000\n94.500000\n166.300000\n64.100000\n52.000000\n2145.000000\n97.000000\n3.150000\n3.110000\n8.600000\n70.000000\n4800.000000\n19.000000\n25.000000\n7788.000000\n\n\n50%\n103.000000\n1.000000\n97.000000\n173.200000\n65.500000\n54.100000\n2414.000000\n120.000000\n3.310000\n3.290000\n9.000000\n95.000000\n5200.000000\n24.000000\n30.000000\n10295.000000\n\n\n75%\n154.000000\n2.000000\n102.400000\n183.100000\n66.900000\n55.500000\n2935.000000\n141.000000\n3.580000\n3.410000\n9.400000\n116.000000\n5500.000000\n30.000000\n34.000000\n16503.000000\n\n\nmax\n205.000000\n3.000000\n120.900000\n208.100000\n72.300000\n59.800000\n4066.000000\n326.000000\n3.940000\n4.170000\n23.000000\n288.000000\n6600.000000\n49.000000\n54.000000\n45400.000000\n\n\n\n\n\n\n\n\ndf.shape\n\n(205, 26)\n\n\n\ndf.isnull()\n\n\n\n\n\n\n\n\ncar_ID\nsymboling\nCarName\nfueltype\naspiration\ndoornumber\ncarbody\ndrivewheel\nenginelocation\nwheelbase\n...\nenginesize\nfuelsystem\nboreratio\nstroke\ncompressionratio\nhorsepower\npeakrpm\ncitympg\nhighwaympg\nprice\n\n\n\n\n0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n200\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n201\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n202\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n203\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n204\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n...\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n205 rows × 26 columns\n\n\n\n\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')\n\n&lt;Axes: &gt;\n\n\n\n\n\n\ndf.dropna(inplace =True)\ndf.isnull().sum()\n\ncar_ID              0\nsymboling           0\nCarName             0\nfueltype            0\naspiration          0\ndoornumber          0\ncarbody             0\ndrivewheel          0\nenginelocation      0\nwheelbase           0\ncarlength           0\ncarwidth            0\ncarheight           0\ncurbweight          0\nenginetype          0\ncylindernumber      0\nenginesize          0\nfuelsystem          0\nboreratio           0\nstroke              0\ncompressionratio    0\nhorsepower          0\npeakrpm             0\ncitympg             0\nhighwaympg          0\nprice               0\ndtype: int64\n\n\n\ndf.duplicated().any()\n\nFalse\n\n\n\ndf.describe(include=object)\n\n\n\n\n\n\n\n\nCarName\nfueltype\naspiration\ndoornumber\ncarbody\ndrivewheel\nenginelocation\nenginetype\ncylindernumber\nfuelsystem\n\n\n\n\ncount\n205\n205\n205\n205\n205\n205\n205\n205\n205\n205\n\n\nunique\n147\n2\n2\n2\n5\n3\n2\n7\n7\n8\n\n\ntop\ntoyota corona\ngas\nstd\nfour\nsedan\nfwd\nfront\nohc\nfour\nmpfi\n\n\nfreq\n6\n185\n168\n115\n96\n120\n202\n148\n159\n94\n\n\n\n\n\n\n\n\ndf = df.select_dtypes(include=['float64', 'int64'])\ndf.corr()\n\n\n\n\n\n\n\n\ncar_ID\nsymboling\nwheelbase\ncarlength\ncarwidth\ncarheight\ncurbweight\nenginesize\nboreratio\nstroke\ncompressionratio\nhorsepower\npeakrpm\ncitympg\nhighwaympg\nprice\n\n\n\n\ncar_ID\n1.000000\n-0.151621\n0.129729\n0.170636\n0.052387\n0.255960\n0.071962\n-0.033930\n0.260064\n-0.160824\n0.150276\n-0.015006\n-0.203789\n0.015940\n0.011255\n-0.109093\n\n\nsymboling\n-0.151621\n1.000000\n-0.531954\n-0.357612\n-0.232919\n-0.541038\n-0.227691\n-0.105790\n-0.130051\n-0.008735\n-0.178515\n0.070873\n0.273606\n-0.035823\n0.034606\n-0.079978\n\n\nwheelbase\n0.129729\n-0.531954\n1.000000\n0.874587\n0.795144\n0.589435\n0.776386\n0.569329\n0.488750\n0.160959\n0.249786\n0.353294\n-0.360469\n-0.470414\n-0.544082\n0.577816\n\n\ncarlength\n0.170636\n-0.357612\n0.874587\n1.000000\n0.841118\n0.491029\n0.877728\n0.683360\n0.606454\n0.129533\n0.158414\n0.552623\n-0.287242\n-0.670909\n-0.704662\n0.682920\n\n\ncarwidth\n0.052387\n-0.232919\n0.795144\n0.841118\n1.000000\n0.279210\n0.867032\n0.735433\n0.559150\n0.182942\n0.181129\n0.640732\n-0.220012\n-0.642704\n-0.677218\n0.759325\n\n\ncarheight\n0.255960\n-0.541038\n0.589435\n0.491029\n0.279210\n1.000000\n0.295572\n0.067149\n0.171071\n-0.055307\n0.261214\n-0.108802\n-0.320411\n-0.048640\n-0.107358\n0.119336\n\n\ncurbweight\n0.071962\n-0.227691\n0.776386\n0.877728\n0.867032\n0.295572\n1.000000\n0.850594\n0.648480\n0.168790\n0.151362\n0.750739\n-0.266243\n-0.757414\n-0.797465\n0.835305\n\n\nenginesize\n-0.033930\n-0.105790\n0.569329\n0.683360\n0.735433\n0.067149\n0.850594\n1.000000\n0.583774\n0.203129\n0.028971\n0.809769\n-0.244660\n-0.653658\n-0.677470\n0.874145\n\n\nboreratio\n0.260064\n-0.130051\n0.488750\n0.606454\n0.559150\n0.171071\n0.648480\n0.583774\n1.000000\n-0.055909\n0.005197\n0.573677\n-0.254976\n-0.584532\n-0.587012\n0.553173\n\n\nstroke\n-0.160824\n-0.008735\n0.160959\n0.129533\n0.182942\n-0.055307\n0.168790\n0.203129\n-0.055909\n1.000000\n0.186110\n0.080940\n-0.067964\n-0.042145\n-0.043931\n0.079443\n\n\ncompressionratio\n0.150276\n-0.178515\n0.249786\n0.158414\n0.181129\n0.261214\n0.151362\n0.028971\n0.005197\n0.186110\n1.000000\n-0.204326\n-0.435741\n0.324701\n0.265201\n0.067984\n\n\nhorsepower\n-0.015006\n0.070873\n0.353294\n0.552623\n0.640732\n-0.108802\n0.750739\n0.809769\n0.573677\n0.080940\n-0.204326\n1.000000\n0.131073\n-0.801456\n-0.770544\n0.808139\n\n\npeakrpm\n-0.203789\n0.273606\n-0.360469\n-0.287242\n-0.220012\n-0.320411\n-0.266243\n-0.244660\n-0.254976\n-0.067964\n-0.435741\n0.131073\n1.000000\n-0.113544\n-0.054275\n-0.085267\n\n\ncitympg\n0.015940\n-0.035823\n-0.470414\n-0.670909\n-0.642704\n-0.048640\n-0.757414\n-0.653658\n-0.584532\n-0.042145\n0.324701\n-0.801456\n-0.113544\n1.000000\n0.971337\n-0.685751\n\n\nhighwaympg\n0.011255\n0.034606\n-0.544082\n-0.704662\n-0.677218\n-0.107358\n-0.797465\n-0.677470\n-0.587012\n-0.043931\n0.265201\n-0.770544\n-0.054275\n0.971337\n1.000000\n-0.697599\n\n\nprice\n-0.109093\n-0.079978\n0.577816\n0.682920\n0.759325\n0.119336\n0.835305\n0.874145\n0.553173\n0.079443\n0.067984\n0.808139\n-0.085267\n-0.685751\n-0.697599\n1.000000\n\n\n\n\n\n\n\n\ncorrelation_matrix = df[['carlength', 'carwidth', 'curbweight']].corr()\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix,  cmap='coolwarm', center=0)\nfor i in range(len(correlation_matrix)):\n    for j in range(len(correlation_matrix)):\n        plt.text(j + 0.5, i + 0.5, f\"{correlation_matrix.iloc[i, j]:.2f}\", ha='center', va='center', fontsize=10)\nplt.title('Correlation Matrix')\nplt.show()\n\n\n\n\n\ndf = df.drop(['symboling', 'car_ID', ], axis=1)\nprint(df.head())\n\n   wheelbase  carlength  carwidth  carheight  curbweight  enginesize  \\\n0       88.6      168.8      64.1       48.8        2548         130   \n1       88.6      168.8      64.1       48.8        2548         130   \n2       94.5      171.2      65.5       52.4        2823         152   \n3       99.8      176.6      66.2       54.3        2337         109   \n4       99.4      176.6      66.4       54.3        2824         136   \n\n   boreratio  stroke  compressionratio  horsepower  peakrpm  citympg  \\\n0       3.47    2.68               9.0         111     5000       21   \n1       3.47    2.68               9.0         111     5000       21   \n2       2.68    3.47               9.0         154     5000       19   \n3       3.19    3.40              10.0         102     5500       24   \n4       3.19    3.40               8.0         115     5500       18   \n\n   highwaympg    price  \n0          27  13495.0  \n1          27  16500.0  \n2          26  16500.0  \n3          30  13950.0  \n4          22  17450.0  \n\n\n\ndf.shape\n\n(205, 14)\n\n\n\ndf.columns\n\nIndex(['wheelbase', 'carlength', 'carwidth', 'carheight', 'curbweight',\n       'enginesize', 'boreratio', 'stroke', 'compressionratio', 'horsepower',\n       'peakrpm', 'citympg', 'highwaympg', 'price'],\n      dtype='object')\n\n\n\nX = df.drop('price', axis=1)\ny = df['price']\n\n\nX.head()\n\n\n\n\n\n\n\n\nwheelbase\ncarlength\ncarwidth\ncarheight\ncurbweight\nenginesize\nboreratio\nstroke\ncompressionratio\nhorsepower\npeakrpm\ncitympg\nhighwaympg\n\n\n\n\n0\n88.6\n168.8\n64.1\n48.8\n2548\n130\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n\n\n1\n88.6\n168.8\n64.1\n48.8\n2548\n130\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n\n\n2\n94.5\n171.2\n65.5\n52.4\n2823\n152\n2.68\n3.47\n9.0\n154\n5000\n19\n26\n\n\n3\n99.8\n176.6\n66.2\n54.3\n2337\n109\n3.19\n3.40\n10.0\n102\n5500\n24\n30\n\n\n4\n99.4\n176.6\n66.4\n54.3\n2824\n136\n3.19\n3.40\n8.0\n115\n5500\n18\n22\n\n\n\n\n\n\n\n\ny\n\n0      13495.0\n1      16500.0\n2      16500.0\n3      13950.0\n4      17450.0\n        ...   \n200    16845.0\n201    19045.0\n202    21485.0\n203    22470.0\n204    22625.0\nName: price, Length: 205, dtype: float64\n\n\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(df.corr(), cmap='coolwarm', fmt=\".2f\", linewidths=.5)\nfor i in range(len(df.corr())):\n    for j in range(len(df.corr())):\n        plt.text(j + 0.5, i + 0.5, f\"{df.corr().iloc[i, j]:.2f}\", ha='center', va='center', fontsize=10)\nplt.title('Correlation Heatmap')\nplt.show()\n\n\n\n\n\nsns.pairplot(df)\n\n\n\n\n\nplt.figure(figsize=(8, 6))\nsns.histplot(df['enginesize'], kde=True, color='skyblue')\nplt.title('Distribution of Engine Size')\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(8, 6))\nsns.histplot(df['wheelbase'], kde=True)\nplt.title('Distribution of Wheelbase')\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(8, 6))\nsns.histplot(df['carlength'], kde=True)\nplt.title('Distribution of Car Length')\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(8, 6))\nsns.histplot(df['carlength'], kde=True)\nplt.title('Distribution of Car Length')\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(8, 6))\nsns.histplot(df['carheight'], kde=True)\nplt.title('Distribution of Car Height')\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(8, 6))\nsns.histplot(df['curbweight'], kde=True)\nplt.title('Distribution of Curb Weight')\nplt.show()\n\n\n\n\n\nplt.scatter(df['enginesize'], df['price'])\nplt.xlabel('Engine Size')\nplt.ylabel('Price')\nplt.title('Scatter Plot between Engine Size and Price')\nplt.show()\n\n\n\n\n\nsns.scatterplot(x='horsepower', y='price', data=df)\nplt.title('Horsepower vs. Price (Scatter Plot)')\nplt.show()\n\n\n\n\n\nsns.histplot(df['price'], kde=True, color='skyblue')\nplt.title('Distribution of Car Prices with KDE')\nplt.show()\n\n\n\n\n\nsns.histplot(df['citympg'], kde=True, bins=20, color='skyblue')\nplt.title('Distribution of City MPG')\nplt.xlabel('City MPG')\nplt.show()\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=42)\n\n\nsns.regplot(x='wheelbase', y='price', data=df, scatter_kws={'s': 20}, line_kws={'color': 'red'})\nplt.title('Wheelbase vs. Price')\nplt.xlabel('Wheelbase')\nplt.ylabel('Price')\nplt.show()\n\n\n\n\n\nsns.regplot(x='carlength', y='price', data=df, scatter_kws={'s': 20}, line_kws={'color': 'red'})\nplt.title('Car Length vs. Price')\nplt.xlabel('Car Length')\nplt.ylabel('Price')\nplt.show()\n\n\n\n\n\nsns.regplot(x='curbweight', y='price', data=df, scatter_kws={'s': 20}, line_kws={'color': 'red'})\nplt.title('Curb Weight vs. Price')\nplt.xlabel('Curb Weight')\nplt.ylabel('Price')\nplt.show()\n\n\n\n\n\nsns.regplot(x='enginesize', y='price', data=df, scatter_kws={'s': 20}, line_kws={'color': 'red'})\nplt.title('Engine Size vs. Price')\nplt.xlabel('Engine Size')\nplt.ylabel('Price')\nplt.show()\n\n\n\n\n\nsns.regplot(x='horsepower', y='price', data=df, scatter_kws={'s': 20}, line_kws={'color': 'red'})\nplt.title('Horsepower vs. Price')\nplt.xlabel('Horsepower')\nplt.ylabel('Price')\nplt.show()\n\n\n\n\n\nsns.regplot(x='citympg', y='price', data=df, scatter_kws={'s': 20}, line_kws={'color': 'red'})\nplt.title('City MPG vs. Price')\nplt.xlabel('City MPG')\nplt.ylabel('Price')\nplt.show()\n\n\n\n\n\nprint(X_train.columns)\n\nIndex(['wheelbase', 'carlength', 'carwidth', 'carheight', 'curbweight',\n       'enginesize', 'boreratio', 'stroke', 'compressionratio', 'horsepower',\n       'peakrpm', 'citympg', 'highwaympg'],\n      dtype='object')\n\n\n\nimport pandas as pd\ncategorical_cols = ['enginesize', 'horsepower', 'peakrpm']\nX_train_encoded = pd.get_dummies(X_train, columns=categorical_cols)\nX_test_encoded = pd.get_dummies(X_test, columns=categorical_cols)\n\n\nfor column in X_train.columns:\n    unique_values = X_train[column].nunique()\n    print(f\"Column '{column}' has {unique_values} unique values.\")\n\nColumn 'wheelbase' has 46 unique values.\nColumn 'carlength' has 63 unique values.\nColumn 'carwidth' has 41 unique values.\nColumn 'carheight' has 47 unique values.\nColumn 'curbweight' has 132 unique values.\nColumn 'enginesize' has 41 unique values.\nColumn 'boreratio' has 36 unique values.\nColumn 'stroke' has 35 unique values.\nColumn 'compressionratio' has 30 unique values.\nColumn 'horsepower' has 53 unique values.\nColumn 'peakrpm' has 23 unique values.\nColumn 'citympg' has 27 unique values.\nColumn 'highwaympg' has 28 unique values.\n\n\n\nfrom sklearn.linear_model import LinearRegression\nregression=LinearRegression()\n\n\nfor col in categorical_cols:\n    print(f\"Unique values in '{col}': {X_train[col].unique()}\")\n\nUnique values in 'enginesize': [103 122  97 136 146  70 194  92 108 181 140 110 164 151  98 121  91 120\n 156  90 141 134 152 119 130  79 109 183 145 258 111 326 161 131 132 209\n 234 171  80 203 304]\nUnique values in 'horsepower': [ 55  92  69 110 116 101 207  68  82 160 175  73  76 121 143 100  70  95\n 112 145 102 114 142 154  90  88  84 162 115  60  97  62 111  52  85 123\n 106  86 176  56  78 200 262 156 140 182 155  64 135 288 152 184 161]\nUnique values in 'peakrpm': [4800 4200 5200 5500 6000 5900 5000 4500 4250 4400 6600 5800 5400 5600\n 4900 4150 5250 5100 4350 4750 4650 5300 5750]\n\n\n\nprint(X_train.dtypes)\n\nwheelbase           float64\ncarlength           float64\ncarwidth            float64\ncarheight           float64\ncurbweight            int64\nenginesize            int64\nboreratio           float64\nstroke              float64\ncompressionratio    float64\nhorsepower            int64\npeakrpm               int64\ncitympg               int64\nhighwaympg            int64\ndtype: object\n\n\n\nX_train_encoded = pd.get_dummies(X_train, columns=categorical_cols)\n\n\nX_train_encoded = pd.get_dummies(X_train, columns=categorical_cols)\nX_test_encoded = pd.get_dummies(X_test, columns=categorical_cols)\n\n\nnon_numeric_cols_train = X_train_encoded.select_dtypes(exclude=['float64', 'int64']).columns\nnon_numeric_cols_test = X_test_encoded.select_dtypes(exclude=['float64', 'int64']).columns\nprint(\"Non-numeric columns in X_train_encoded:\", non_numeric_cols_train)\nprint(\"Non-numeric columns in X_test_encoded:\", non_numeric_cols_test)\n\nNon-numeric columns in X_train_encoded: Index(['enginesize_70', 'enginesize_79', 'enginesize_80', 'enginesize_90',\n       'enginesize_91', 'enginesize_92', 'enginesize_97', 'enginesize_98',\n       'enginesize_103', 'enginesize_108',\n       ...\n       'peakrpm_5250', 'peakrpm_5300', 'peakrpm_5400', 'peakrpm_5500',\n       'peakrpm_5600', 'peakrpm_5750', 'peakrpm_5800', 'peakrpm_5900',\n       'peakrpm_6000', 'peakrpm_6600'],\n      dtype='object', length=117)\nNon-numeric columns in X_test_encoded: Index(['enginesize_61', 'enginesize_70', 'enginesize_90', 'enginesize_92',\n       'enginesize_97', 'enginesize_98', 'enginesize_108', 'enginesize_110',\n       'enginesize_120', 'enginesize_121', 'enginesize_122', 'enginesize_131',\n       'enginesize_134', 'enginesize_136', 'enginesize_140', 'enginesize_141',\n       'enginesize_146', 'enginesize_152', 'enginesize_156', 'enginesize_171',\n       'enginesize_173', 'enginesize_181', 'enginesize_183', 'enginesize_209',\n       'enginesize_308', 'horsepower_48', 'horsepower_52', 'horsepower_56',\n       'horsepower_58', 'horsepower_62', 'horsepower_68', 'horsepower_69',\n       'horsepower_70', 'horsepower_72', 'horsepower_73', 'horsepower_82',\n       'horsepower_84', 'horsepower_86', 'horsepower_88', 'horsepower_92',\n       'horsepower_94', 'horsepower_95', 'horsepower_97', 'horsepower_101',\n       'horsepower_102', 'horsepower_110', 'horsepower_111', 'horsepower_114',\n       'horsepower_116', 'horsepower_120', 'horsepower_123', 'horsepower_134',\n       'horsepower_145', 'horsepower_152', 'horsepower_160', 'horsepower_161',\n       'horsepower_182', 'horsepower_184', 'peakrpm_4150', 'peakrpm_4200',\n       'peakrpm_4350', 'peakrpm_4400', 'peakrpm_4500', 'peakrpm_4800',\n       'peakrpm_5000', 'peakrpm_5100', 'peakrpm_5200', 'peakrpm_5250',\n       'peakrpm_5400', 'peakrpm_5500', 'peakrpm_5800', 'peakrpm_6000'],\n      dtype='object')\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder_train = LabelEncoder()\nfor col in non_numeric_cols_train:\n    X_train_encoded[col] = label_encoder_train.fit_transform(X_train_encoded[col])\n\nlabel_encoder_test = LabelEncoder()\nfor col in non_numeric_cols_test:\n    X_test_encoded[col] = label_encoder_test.fit_transform(X_test_encoded[col])\n\n\nfor col in non_numeric_cols_train:\n    print(f\"Unique values in {col}: {X_train_encoded[col].unique()}\")\n\nUnique values in enginesize_70: [0 1]\nUnique values in enginesize_79: [0 1]\nUnique values in enginesize_80: [0 1]\nUnique values in enginesize_90: [0 1]\nUnique values in enginesize_91: [0 1]\nUnique values in enginesize_92: [0 1]\nUnique values in enginesize_97: [0 1]\nUnique values in enginesize_98: [0 1]\nUnique values in enginesize_103: [1 0]\nUnique values in enginesize_108: [0 1]\nUnique values in enginesize_109: [0 1]\nUnique values in enginesize_110: [0 1]\nUnique values in enginesize_111: [0 1]\nUnique values in enginesize_119: [0 1]\nUnique values in enginesize_120: [0 1]\nUnique values in enginesize_121: [0 1]\nUnique values in enginesize_122: [0 1]\nUnique values in enginesize_130: [0 1]\nUnique values in enginesize_131: [0 1]\nUnique values in enginesize_132: [0 1]\nUnique values in enginesize_134: [0 1]\nUnique values in enginesize_136: [0 1]\nUnique values in enginesize_140: [0 1]\nUnique values in enginesize_141: [0 1]\nUnique values in enginesize_145: [0 1]\nUnique values in enginesize_146: [0 1]\nUnique values in enginesize_151: [0 1]\nUnique values in enginesize_152: [0 1]\nUnique values in enginesize_156: [0 1]\nUnique values in enginesize_161: [0 1]\nUnique values in enginesize_164: [0 1]\nUnique values in enginesize_171: [0 1]\nUnique values in enginesize_181: [0 1]\nUnique values in enginesize_183: [0 1]\nUnique values in enginesize_194: [0 1]\nUnique values in enginesize_203: [0 1]\nUnique values in enginesize_209: [0 1]\nUnique values in enginesize_234: [0 1]\nUnique values in enginesize_258: [0 1]\nUnique values in enginesize_304: [0 1]\nUnique values in enginesize_326: [0 1]\nUnique values in horsepower_52: [0 1]\nUnique values in horsepower_55: [1 0]\nUnique values in horsepower_56: [0 1]\nUnique values in horsepower_60: [0 1]\nUnique values in horsepower_62: [0 1]\nUnique values in horsepower_64: [0 1]\nUnique values in horsepower_68: [0 1]\nUnique values in horsepower_69: [0 1]\nUnique values in horsepower_70: [0 1]\nUnique values in horsepower_73: [0 1]\nUnique values in horsepower_76: [0 1]\nUnique values in horsepower_78: [0 1]\nUnique values in horsepower_82: [0 1]\nUnique values in horsepower_84: [0 1]\nUnique values in horsepower_85: [0 1]\nUnique values in horsepower_86: [0 1]\nUnique values in horsepower_88: [0 1]\nUnique values in horsepower_90: [0 1]\nUnique values in horsepower_92: [0 1]\nUnique values in horsepower_95: [0 1]\nUnique values in horsepower_97: [0 1]\nUnique values in horsepower_100: [0 1]\nUnique values in horsepower_101: [0 1]\nUnique values in horsepower_102: [0 1]\nUnique values in horsepower_106: [0 1]\nUnique values in horsepower_110: [0 1]\nUnique values in horsepower_111: [0 1]\nUnique values in horsepower_112: [0 1]\nUnique values in horsepower_114: [0 1]\nUnique values in horsepower_115: [0 1]\nUnique values in horsepower_116: [0 1]\nUnique values in horsepower_121: [0 1]\nUnique values in horsepower_123: [0 1]\nUnique values in horsepower_135: [0 1]\nUnique values in horsepower_140: [0 1]\nUnique values in horsepower_142: [0 1]\nUnique values in horsepower_143: [0 1]\nUnique values in horsepower_145: [0 1]\nUnique values in horsepower_152: [0 1]\nUnique values in horsepower_154: [0 1]\nUnique values in horsepower_155: [0 1]\nUnique values in horsepower_156: [0 1]\nUnique values in horsepower_160: [0 1]\nUnique values in horsepower_161: [0 1]\nUnique values in horsepower_162: [0 1]\nUnique values in horsepower_175: [0 1]\nUnique values in horsepower_176: [0 1]\nUnique values in horsepower_182: [0 1]\nUnique values in horsepower_184: [0 1]\nUnique values in horsepower_200: [0 1]\nUnique values in horsepower_207: [0 1]\nUnique values in horsepower_262: [0 1]\nUnique values in horsepower_288: [0 1]\nUnique values in peakrpm_4150: [0 1]\nUnique values in peakrpm_4200: [0 1]\nUnique values in peakrpm_4250: [0 1]\nUnique values in peakrpm_4350: [0 1]\nUnique values in peakrpm_4400: [0 1]\nUnique values in peakrpm_4500: [0 1]\nUnique values in peakrpm_4650: [0 1]\nUnique values in peakrpm_4750: [0 1]\nUnique values in peakrpm_4800: [1 0]\nUnique values in peakrpm_4900: [0 1]\nUnique values in peakrpm_5000: [0 1]\nUnique values in peakrpm_5100: [0 1]\nUnique values in peakrpm_5200: [0 1]\nUnique values in peakrpm_5250: [0 1]\nUnique values in peakrpm_5300: [0 1]\nUnique values in peakrpm_5400: [0 1]\nUnique values in peakrpm_5500: [0 1]\nUnique values in peakrpm_5600: [0 1]\nUnique values in peakrpm_5750: [0 1]\nUnique values in peakrpm_5800: [0 1]\nUnique values in peakrpm_5900: [0 1]\nUnique values in peakrpm_6000: [0 1]\nUnique values in peakrpm_6600: [0 1]\n\n\n\nfor col in non_numeric_cols_test:\n    print(f\"Unique values in {col}: {X_test_encoded[col].unique()}\")\n\nUnique values in enginesize_61: [0 1]\nUnique values in enginesize_70: [0 1]\nUnique values in enginesize_90: [0 1]\nUnique values in enginesize_92: [0 1]\nUnique values in enginesize_97: [0 1]\nUnique values in enginesize_98: [0 1]\nUnique values in enginesize_108: [0 1]\nUnique values in enginesize_110: [0 1]\nUnique values in enginesize_120: [0 1]\nUnique values in enginesize_121: [0 1]\nUnique values in enginesize_122: [0 1]\nUnique values in enginesize_131: [0 1]\nUnique values in enginesize_134: [0 1]\nUnique values in enginesize_136: [0 1]\nUnique values in enginesize_140: [0 1]\nUnique values in enginesize_141: [0 1]\nUnique values in enginesize_146: [0 1]\nUnique values in enginesize_152: [0 1]\nUnique values in enginesize_156: [0 1]\nUnique values in enginesize_171: [0 1]\nUnique values in enginesize_173: [0 1]\nUnique values in enginesize_181: [0 1]\nUnique values in enginesize_183: [0 1]\nUnique values in enginesize_209: [1 0]\nUnique values in enginesize_308: [0 1]\nUnique values in horsepower_48: [0 1]\nUnique values in horsepower_52: [0 1]\nUnique values in horsepower_56: [0 1]\nUnique values in horsepower_58: [0 1]\nUnique values in horsepower_62: [0 1]\nUnique values in horsepower_68: [0 1]\nUnique values in horsepower_69: [0 1]\nUnique values in horsepower_70: [0 1]\nUnique values in horsepower_72: [0 1]\nUnique values in horsepower_73: [0 1]\nUnique values in horsepower_82: [0 1]\nUnique values in horsepower_84: [0 1]\nUnique values in horsepower_86: [0 1]\nUnique values in horsepower_88: [0 1]\nUnique values in horsepower_92: [0 1]\nUnique values in horsepower_94: [0 1]\nUnique values in horsepower_95: [0 1]\nUnique values in horsepower_97: [0 1]\nUnique values in horsepower_101: [0 1]\nUnique values in horsepower_102: [0 1]\nUnique values in horsepower_110: [0 1]\nUnique values in horsepower_111: [0 1]\nUnique values in horsepower_114: [0 1]\nUnique values in horsepower_116: [0 1]\nUnique values in horsepower_120: [0 1]\nUnique values in horsepower_123: [0 1]\nUnique values in horsepower_134: [0 1]\nUnique values in horsepower_145: [0 1]\nUnique values in horsepower_152: [0 1]\nUnique values in horsepower_160: [0 1]\nUnique values in horsepower_161: [0 1]\nUnique values in horsepower_182: [1 0]\nUnique values in horsepower_184: [0 1]\nUnique values in peakrpm_4150: [0 1]\nUnique values in peakrpm_4200: [0 1]\nUnique values in peakrpm_4350: [0 1]\nUnique values in peakrpm_4400: [0 1]\nUnique values in peakrpm_4500: [0 1]\nUnique values in peakrpm_4800: [0 1]\nUnique values in peakrpm_5000: [0 1]\nUnique values in peakrpm_5100: [0 1]\nUnique values in peakrpm_5200: [0 1]\nUnique values in peakrpm_5250: [0 1]\nUnique values in peakrpm_5400: [1 0]\nUnique values in peakrpm_5500: [0 1]\nUnique values in peakrpm_5800: [0 1]\nUnique values in peakrpm_6000: [0 1]\n\n\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabel_encoder = LabelEncoder()\n\nfor col in categorical_cols:\n    X_train[col] = label_encoder.fit_transform(X_train[col])\n\nonehot_encoder = OneHotEncoder(drop='first', sparse=False)\n\nX_train_encoded = pd.get_dummies(X_train, columns=categorical_cols, drop_first=True)\nX_test_encoded = pd.get_dummies(X_test, columns=categorical_cols, drop_first=True)\n\nX_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='outer', axis=1, fill_value=0)\n\nregression.fit(X_train_encoded, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nfrom sklearn.model_selection import cross_val_score\nvalidation_score=cross_val_score(regression,X_train,y_train,scoring='neg_mean_squared_error',\n                             cv=3)\n\n\nnp.mean(validation_score)\n\n-17619779.964257453\n\n\n\ny_pred = regression.predict(X_test_encoded)\ny_pred\n\narray([ 17663.40920114,   2843.49700864,   -681.0197125 ,  20031.97260338,\n        23256.22526061,  -2078.06829731,  11404.65164046,   8303.95517846,\n        34989.85861164,  -1974.8074486 ,    672.29083674,   7942.30385746,\n        26485.26793673,  -1324.44214526,  31219.07729321,   3691.87875078,\n        -5254.23835884,   -389.77808369,   1982.84977136,  34302.79230783,\n         4217.87918551,  15554.97366416,  -1635.32099728, -14421.58919858,\n        -3894.410904  ,  18993.50244592,  10010.82968533,  28157.3938917 ,\n        -2754.23702629,  27383.42715415,  21693.84435239,  -4089.13068506,\n         3223.28302425,  32429.56926617,  -6839.56578167,  21720.24707413,\n        34377.53936716,   9197.98033432,    136.89843489,    723.50606337,\n        32982.56263849,  11575.29609297,  49483.4797775 ,   3077.1128973 ,\n        -2818.66172925,  -8107.23857484,  -4089.13068506,  32626.79986009,\n        25793.48112021,  -1219.70262027,   -172.88303499,   8578.30732609])\n\n\n\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nmse=mean_squared_error(y_test,y_pred)\nmae=mean_absolute_error(y_test,y_pred)\nrmse=np.sqrt(mse)\nprint(mse)\nprint(mae)\nprint(rmse)\n\n177304634.11961922\n11133.618219594775\n13315.57862503989\n\n\n\nfrom sklearn.metrics import r2_score\nscore=r2_score(y_test,y_pred)\nprint(score)\nprint(1 - (1-score)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1))\n\n-1.6205416455966546\n-2.51704273487972\n\n\n\nplt.scatter(y_test,y_pred)\n\n&lt;matplotlib.collections.PathCollection at 0x1eb154bed10&gt;\n\n\n\n\n\n\nresiduals=y_test-y_pred\nprint(residuals)\n\n15     13096.590799\n9      15015.669991\n100    10230.019713\n132    -8181.972603\n68      4991.774739\n95      9877.068297\n159    -3616.651640\n162      954.044822\n147   -24791.858612\n182     9749.807449\n191    12622.709163\n164      295.696143\n65     -8205.267937\n175    11312.442145\n73      9740.922707\n152     2796.121249\n18     10405.238359\n82     13018.778084\n86      6206.150229\n143   -24342.792308\n60      4277.120814\n101    -2055.973664\n98      9884.320997\n30     20900.589199\n25     10586.410904\n16     22321.497554\n168     -371.829685\n195   -14742.393892\n97     10753.237026\n194   -14443.427154\n67      3858.155648\n120    10318.130685\n154     4674.716976\n202   -10944.569266\n79     14528.565782\n69      6455.752926\n145   -23118.539367\n55      1747.019666\n45      8779.601565\n84     13765.493937\n146   -25519.562638\n66      6768.703907\n111   -33903.479777\n153     3840.887103\n96     10317.661729\n38     17202.238575\n24     10318.130685\n139   -25573.799860\n112    -8893.481120\n29     14183.702620\n19      6467.883035\n178     7979.692674\nName: price, dtype: float64\n\n\n\nsns.displot(residuals,kind='kde')\n\n\n\n\n\nplt.scatter(y_pred,residuals)\n\n&lt;matplotlib.collections.PathCollection at 0x1eb13db5510&gt;\n\n\n\n\n\n\nsns.histplot(y_pred, kde=True, color='skyblue')\nplt.title('Distribution of Predicted Prices')\nplt.xlabel('Predicted Prices')\nplt.show()\n\n\n\n\n3.b)Non-Linear Regression\n    What is a Non-Linear Regression?\n    Non-Linear regression is a type of polynomial regression. It is a method to model a non-linear relationship between the dependent and independent variables. It is used in place when the data shows a curvy trend and linear regression would not produce\n        very accurate results when compared to non-linear regression. This is because in linear regression it is pre-assumed that the data is linear. \n    How does a Non-Linear Regression work?\n    observe closely then we will realize that to evolve from linear regression to non-linear regression. We are just supposed to add the higher-order terms of the dependent features in the feature space. This is sometimes also known as feature engineering\n        but not exactly.\n    Applications of Non-Linear Regression\n     that most of the real-world data is non-linear and hence non-linear regression techniques are far better than linear regression techniques. Non-Linear regression techniques help to get a robust model whose predictions are reliable and as per the trend\n        followed by the data in history. Tasks related to exponential growth or decay of a population, financial forecasting, and logistic pricing model were all successfully accomplished by the Non-Linear Regression techniques.\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nimport warnings\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings('ignore')\n\n\ndf = pd.read_csv('miami-housing.csv')\ndf.head()\n\n\n\n\n\n\n\n\nLATITUDE\nLONGITUDE\nPARCELNO\nSALE_PRC\nLND_SQFOOT\nTOT_LVG_AREA\nSPEC_FEAT_VAL\nRAIL_DIST\nOCEAN_DIST\nWATER_DIST\nCNTR_DIST\nSUBCNTR_DI\nHWY_DIST\nage\navno60plus\nmonth_sold\nstructure_quality\n\n\n\n\n0\n25.891031\n-80.160561\n622280070620\n440000.0\n9375\n1753\n0\n2815.9\n12811.4\n347.6\n42815.3\n37742.2\n15954.9\n67\n0\n8\n4\n\n\n1\n25.891324\n-80.153968\n622280100460\n349000.0\n9375\n1715\n0\n4359.1\n10648.4\n337.8\n43504.9\n37340.5\n18125.0\n63\n0\n9\n4\n\n\n2\n25.891334\n-80.153740\n622280100470\n800000.0\n9375\n2276\n49206\n4412.9\n10574.1\n297.1\n43530.4\n37328.7\n18200.5\n61\n0\n2\n4\n\n\n3\n25.891765\n-80.152657\n622280100530\n988000.0\n12450\n2058\n10033\n4585.0\n10156.5\n0.0\n43797.5\n37423.2\n18514.4\n63\n0\n9\n4\n\n\n4\n25.891825\n-80.154639\n622280100200\n755000.0\n12800\n1684\n16681\n4063.4\n10836.8\n326.6\n43599.7\n37550.8\n17903.4\n42\n0\n7\n4\n\n\n\n\n\n\n\n\ndf.columns\n\nIndex(['LATITUDE', 'LONGITUDE', 'PARCELNO', 'SALE_PRC', 'LND_SQFOOT',\n       'TOT_LVG_AREA', 'SPEC_FEAT_VAL', 'RAIL_DIST', 'OCEAN_DIST',\n       'WATER_DIST', 'CNTR_DIST', 'SUBCNTR_DI', 'HWY_DIST', 'age',\n       'avno60plus', 'month_sold', 'structure_quality'],\n      dtype='object')\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 13932 entries, 0 to 13931\nData columns (total 17 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   LATITUDE           13932 non-null  float64\n 1   LONGITUDE          13932 non-null  float64\n 2   PARCELNO           13932 non-null  int64  \n 3   SALE_PRC           13932 non-null  float64\n 4   LND_SQFOOT         13932 non-null  int64  \n 5   TOT_LVG_AREA       13932 non-null  int64  \n 6   SPEC_FEAT_VAL      13932 non-null  int64  \n 7   RAIL_DIST          13932 non-null  float64\n 8   OCEAN_DIST         13932 non-null  float64\n 9   WATER_DIST         13932 non-null  float64\n 10  CNTR_DIST          13932 non-null  float64\n 11  SUBCNTR_DI         13932 non-null  float64\n 12  HWY_DIST           13932 non-null  float64\n 13  age                13932 non-null  int64  \n 14  avno60plus         13932 non-null  int64  \n 15  month_sold         13932 non-null  int64  \n 16  structure_quality  13932 non-null  int64  \ndtypes: float64(9), int64(8)\nmemory usage: 1.8 MB\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nLATITUDE\nLONGITUDE\nPARCELNO\nSALE_PRC\nLND_SQFOOT\nTOT_LVG_AREA\nSPEC_FEAT_VAL\nRAIL_DIST\nOCEAN_DIST\nWATER_DIST\nCNTR_DIST\nSUBCNTR_DI\nHWY_DIST\nage\navno60plus\nmonth_sold\nstructure_quality\n\n\n\n\ncount\n13932.000000\n13932.000000\n1.393200e+04\n1.393200e+04\n13932.000000\n13932.000000\n13932.000000\n13932.000000\n13932.000000\n13932.000000\n13932.000000\n13932.000000\n13932.000000\n13932.000000\n13932.000000\n13932.000000\n13932.000000\n\n\nmean\n25.728811\n-80.327475\n2.356496e+12\n3.999419e+05\n8620.879917\n2058.044574\n9562.493468\n8348.548715\n31690.993798\n11960.285235\n68490.327132\n41115.047265\n7723.770693\n30.669251\n0.014930\n6.655828\n3.513997\n\n\nstd\n0.140633\n0.089199\n1.199290e+12\n3.172147e+05\n6070.088742\n813.538535\n13890.967782\n6178.027333\n17595.079468\n11932.992369\n32008.474808\n22161.825935\n6068.936108\n21.153068\n0.121276\n3.301523\n1.097444\n\n\nmin\n25.434333\n-80.542172\n1.020008e+11\n7.200000e+04\n1248.000000\n854.000000\n0.000000\n10.500000\n236.100000\n0.000000\n3825.600000\n1462.800000\n90.200000\n0.000000\n0.000000\n1.000000\n1.000000\n\n\n25%\n25.620056\n-80.403278\n1.079160e+12\n2.350000e+05\n5400.000000\n1470.000000\n810.000000\n3299.450000\n18079.350000\n2675.850000\n42823.100000\n23996.250000\n2998.125000\n14.000000\n0.000000\n4.000000\n2.000000\n\n\n50%\n25.731810\n-80.338911\n3.040300e+12\n3.100000e+05\n7500.000000\n1877.500000\n2765.500000\n7106.300000\n28541.750000\n6922.600000\n65852.400000\n41109.900000\n6159.750000\n26.000000\n0.000000\n7.000000\n4.000000\n\n\n75%\n25.852269\n-80.258019\n3.060170e+12\n4.280000e+05\n9126.250000\n2471.000000\n12352.250000\n12102.600000\n44310.650000\n19200.000000\n89358.325000\n53949.375000\n10854.200000\n46.000000\n0.000000\n9.000000\n4.000000\n\n\nmax\n25.974382\n-80.119746\n3.660170e+12\n2.650000e+06\n57064.000000\n6287.000000\n175020.000000\n29621.500000\n75744.900000\n50399.800000\n159976.500000\n110553.800000\n48167.300000\n96.000000\n1.000000\n12.000000\n5.000000\n\n\n\n\n\n\n\n\ndf.shape\n\n(13932, 17)\n\n\n\ndf.isnull()\n\n\n\n\n\n\n\n\nLATITUDE\nLONGITUDE\nPARCELNO\nSALE_PRC\nLND_SQFOOT\nTOT_LVG_AREA\nSPEC_FEAT_VAL\nRAIL_DIST\nOCEAN_DIST\nWATER_DIST\nCNTR_DIST\nSUBCNTR_DI\nHWY_DIST\nage\navno60plus\nmonth_sold\nstructure_quality\n\n\n\n\n0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n13927\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n13928\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n13929\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n13930\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n13931\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n13932 rows × 17 columns\n\n\n\n\ndf.dropna(inplace =True)\ndf.isnull().sum()\n\nLATITUDE             0\nLONGITUDE            0\nPARCELNO             0\nSALE_PRC             0\nLND_SQFOOT           0\nTOT_LVG_AREA         0\nSPEC_FEAT_VAL        0\nRAIL_DIST            0\nOCEAN_DIST           0\nWATER_DIST           0\nCNTR_DIST            0\nSUBCNTR_DI           0\nHWY_DIST             0\nage                  0\navno60plus           0\nmonth_sold           0\nstructure_quality    0\ndtype: int64\n\n\n\ndf.duplicated().any()\n\nFalse\n\n\n\ndf.corr()\n\n\n\n\n\n\n\n\nLATITUDE\nLONGITUDE\nPARCELNO\nSALE_PRC\nLND_SQFOOT\nTOT_LVG_AREA\nSPEC_FEAT_VAL\nRAIL_DIST\nOCEAN_DIST\nWATER_DIST\nCNTR_DIST\nSUBCNTR_DI\nHWY_DIST\nage\navno60plus\nmonth_sold\nstructure_quality\n\n\n\n\nLATITUDE\n1.000000\n0.721232\n-0.165487\n0.047701\n-0.077481\n-0.193972\n-0.007634\n-0.172382\n0.242735\n-0.423396\n-0.717348\n-0.195823\n-0.113443\n0.416967\n0.081366\n-0.023634\n0.391989\n\n\nLONGITUDE\n0.721232\n1.000000\n-0.432816\n0.195274\n0.018242\n-0.181007\n-0.009372\n-0.303155\n-0.457477\n-0.764256\n-0.791968\n-0.380220\n-0.216406\n0.488757\n0.059416\n-0.010859\n0.132893\n\n\nPARCELNO\n-0.165487\n-0.432816\n1.000000\n-0.204068\n0.071381\n0.102439\n0.055152\n0.223387\n0.289232\n0.295951\n0.419933\n0.243888\n0.018247\n-0.270718\n-0.160925\n0.011129\n0.044652\n\n\nSALE_PRC\n0.047701\n0.195274\n-0.204068\n1.000000\n0.363077\n0.667301\n0.497500\n-0.077009\n-0.274675\n-0.127938\n-0.271425\n-0.370078\n0.231877\n-0.123408\n-0.027026\n0.000325\n0.383995\n\n\nLND_SQFOOT\n-0.077481\n0.018242\n0.071381\n0.363077\n1.000000\n0.437472\n0.390707\n-0.083901\n-0.161579\n-0.055093\n-0.023181\n-0.159094\n0.130488\n0.101244\n-0.005899\n0.005926\n-0.006686\n\n\nTOT_LVG_AREA\n-0.193972\n-0.181007\n0.102439\n0.667301\n0.437472\n1.000000\n0.506064\n0.075486\n-0.050141\n0.148343\n0.136526\n-0.044882\n0.229497\n-0.340606\n-0.056545\n0.002517\n0.173422\n\n\nSPEC_FEAT_VAL\n-0.007634\n-0.009372\n0.055152\n0.497500\n0.390707\n0.506064\n1.000000\n-0.021965\n-0.055155\n0.013923\n-0.048817\n-0.151916\n0.153770\n-0.098780\n-0.008879\n-0.014012\n0.188030\n\n\nRAIL_DIST\n-0.172382\n-0.303155\n0.223387\n-0.077009\n-0.083901\n0.075486\n-0.021965\n1.000000\n0.258966\n0.162313\n0.444494\n0.485468\n-0.092495\n-0.234515\n-0.116955\n0.010560\n-0.074075\n\n\nOCEAN_DIST\n0.242735\n-0.457477\n0.289232\n-0.274675\n-0.161579\n-0.050141\n-0.055155\n0.258966\n1.000000\n0.490764\n0.245396\n0.425869\n0.093500\n-0.159409\n0.035215\n-0.012723\n0.209497\n\n\nWATER_DIST\n-0.423396\n-0.764256\n0.295951\n-0.127938\n-0.055093\n0.148343\n0.013923\n0.162313\n0.490764\n1.000000\n0.526952\n0.195280\n0.400233\n-0.330578\n-0.096339\n0.010556\n-0.034343\n\n\nCNTR_DIST\n-0.717348\n-0.791968\n0.419933\n-0.271425\n-0.023181\n0.136526\n-0.048817\n0.444494\n0.245396\n0.526952\n1.000000\n0.766387\n0.076484\n-0.548287\n-0.130857\n0.023096\n-0.330588\n\n\nSUBCNTR_DI\n-0.195823\n-0.380220\n0.243888\n-0.370078\n-0.159094\n-0.044882\n-0.151916\n0.485468\n0.425869\n0.195280\n0.766387\n1.000000\n-0.093982\n-0.385278\n-0.073202\n0.016334\n-0.248656\n\n\nHWY_DIST\n-0.113443\n-0.216406\n0.018247\n0.231877\n0.130488\n0.229497\n0.153770\n-0.092495\n0.093500\n0.400233\n0.076484\n-0.093982\n1.000000\n-0.120505\n-0.019788\n-0.004547\n0.193529\n\n\nage\n0.416967\n0.488757\n-0.270718\n-0.123408\n0.101244\n-0.340606\n-0.098780\n-0.234515\n-0.159409\n-0.330578\n-0.548287\n-0.385278\n-0.120505\n1.000000\n0.110325\n-0.038904\n0.009253\n\n\navno60plus\n0.081366\n0.059416\n-0.160925\n-0.027026\n-0.005899\n-0.056545\n-0.008879\n-0.116955\n0.035215\n-0.096339\n-0.130857\n-0.073202\n-0.019788\n0.110325\n1.000000\n-0.020870\n0.096050\n\n\nmonth_sold\n-0.023634\n-0.010859\n0.011129\n0.000325\n0.005926\n0.002517\n-0.014012\n0.010560\n-0.012723\n0.010556\n0.023096\n0.016334\n-0.004547\n-0.038904\n-0.020870\n1.000000\n-0.011023\n\n\nstructure_quality\n0.391989\n0.132893\n0.044652\n0.383995\n-0.006686\n0.173422\n0.188030\n-0.074075\n0.209497\n-0.034343\n-0.330588\n-0.248656\n0.193529\n0.009253\n0.096050\n-0.011023\n1.000000\n\n\n\n\n\n\n\n\ndf.drop(['LONGITUDE', 'WATER_DIST'], axis=1,inplace=True)\n\n\nlow_corr_features = ['PARCELNO', 'age', 'avno60plus', 'month_sold']\ndf.drop(low_corr_features, axis=1,inplace=True)\n\n\ndf.corr()\n\n\n\n\n\n\n\n\nLATITUDE\nSALE_PRC\nLND_SQFOOT\nTOT_LVG_AREA\nSPEC_FEAT_VAL\nRAIL_DIST\nOCEAN_DIST\nCNTR_DIST\nSUBCNTR_DI\nHWY_DIST\nstructure_quality\n\n\n\n\nLATITUDE\n1.000000\n0.047701\n-0.077481\n-0.193972\n-0.007634\n-0.172382\n0.242735\n-0.717348\n-0.195823\n-0.113443\n0.391989\n\n\nSALE_PRC\n0.047701\n1.000000\n0.363077\n0.667301\n0.497500\n-0.077009\n-0.274675\n-0.271425\n-0.370078\n0.231877\n0.383995\n\n\nLND_SQFOOT\n-0.077481\n0.363077\n1.000000\n0.437472\n0.390707\n-0.083901\n-0.161579\n-0.023181\n-0.159094\n0.130488\n-0.006686\n\n\nTOT_LVG_AREA\n-0.193972\n0.667301\n0.437472\n1.000000\n0.506064\n0.075486\n-0.050141\n0.136526\n-0.044882\n0.229497\n0.173422\n\n\nSPEC_FEAT_VAL\n-0.007634\n0.497500\n0.390707\n0.506064\n1.000000\n-0.021965\n-0.055155\n-0.048817\n-0.151916\n0.153770\n0.188030\n\n\nRAIL_DIST\n-0.172382\n-0.077009\n-0.083901\n0.075486\n-0.021965\n1.000000\n0.258966\n0.444494\n0.485468\n-0.092495\n-0.074075\n\n\nOCEAN_DIST\n0.242735\n-0.274675\n-0.161579\n-0.050141\n-0.055155\n0.258966\n1.000000\n0.245396\n0.425869\n0.093500\n0.209497\n\n\nCNTR_DIST\n-0.717348\n-0.271425\n-0.023181\n0.136526\n-0.048817\n0.444494\n0.245396\n1.000000\n0.766387\n0.076484\n-0.330588\n\n\nSUBCNTR_DI\n-0.195823\n-0.370078\n-0.159094\n-0.044882\n-0.151916\n0.485468\n0.425869\n0.766387\n1.000000\n-0.093982\n-0.248656\n\n\nHWY_DIST\n-0.113443\n0.231877\n0.130488\n0.229497\n0.153770\n-0.092495\n0.093500\n0.076484\n-0.093982\n1.000000\n0.193529\n\n\nstructure_quality\n0.391989\n0.383995\n-0.006686\n0.173422\n0.188030\n-0.074075\n0.209497\n-0.330588\n-0.248656\n0.193529\n1.000000\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 6))\nsns.histplot(df['CNTR_DIST'], bins=30, kde=True)\nplt.title('Distribution of CNTR_DIST')\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(8, 6))\nsns.regplot(x='OCEAN_DIST', y='SALE_PRC', data=df)\nplt.title('Regression plot of OCEAN_DIST vs. SALE_PRC')\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x='LND_SQFOOT', y='SALE_PRC', data=df)\nplt.title('Scatter plot of LND_SQFOOT vs. SALE_PRC')\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(8, 6))\nsns.histplot(df['SALE_PRC'], bins=30, kde=True)\nplt.title('Distribution of SALE_PRC')\nplt.show()\n\n\n\n\n\nsns.set(style=\"darkgrid\")\ncolor_palette = \"viridis\"\nsns.relplot(x='CNTR_DIST', y='SALE_PRC', sizes=(1, 100), hue='TOT_LVG_AREA', palette=color_palette, size='SALE_PRC', data=df)\nplt.xlabel('Distance to Miami Central Business')\nplt.ylabel('Sale Price')\nplt.ticklabel_format(style='plain', axis='y')\nplt.xticks(fontsize=8)\nplt.show()\n\n\n\n\n\nsns.scatterplot(x='HWY_DIST', y='SALE_PRC', data=df)\nplt.title('Scatter plot of HWY_DIST vs. SALE_PRC')\nplt.show()\n\n\n\n\n\nsns.histplot(df['RAIL_DIST'], bins=30, kde=True)\nplt.title('Distribution of RAIL_DIST')\nplt.show()\n\n\n\n\n\nfeatures = ['LATITUDE', 'LONGITUDE', 'LND_SQFOOT', 'TOT_LVG_AREA', 'SPEC_FEAT_VAL', 'RAIL_DIST', 'OCEAN_DIST', 'WATER_DIST', 'CNTR_DIST', 'SUBCNTR_DI', 'HWY_DIST', 'age', 'avno60plus', 'month_sold', 'structure_quality']\ntarget = 'SALE_PRC'\n\n\ndf = pd.read_csv('miami-housing.csv')\nX = df[features]\ny = df[target]\n\n\ncorrelation_matrix = df[features + [target]].corr()\nplt.figure(figsize=(10, 6))\nsns.heatmap(correlation_matrix,  cmap=\"coolwarm\", fmt=\".2f\")\nfor i in range(len(correlation_matrix)):\n    for j in range(len(correlation_matrix)):\n        plt.text(j + 0.5, i + 0.5, f\"{correlation_matrix.iloc[i, j]:.2f}\", ha='center', va='center', fontsize=10)\nplt.title(\"Correlation Heatmap\")\nplt.show()\n\n\n\n\n\nsns.histplot(df[target], kde=True)\nplt.title(\"Distribution of Sale Price\")\nplt.xlabel(\"Sale Price\")\nplt.show()\n\n\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\ngb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\nmodel = gb_model.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nr2_score(y_pred,y_test)\n\n0.8775410609127824\n\n\n\nfea_importances = model.feature_importances_\n\nfea_importance_df = pd.DataFrame({'Feature': features, 'Importance': fea_importances})\n\n\nfea_importance_df = fea_importance_df.sort_values(by='Importance', ascending=False)\n\n\nplt.barh(fea_importance_df['Feature'], fea_importance_df['Importance'])\nplt.xlabel('Importance')\nplt.title('Feature Importance')\nplt.show()\n\n\n\n\n\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nchosen_feature = 'LATITUDE'\ny = df['SALE_PRC']\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly_features.fit_transform(X[[chosen_feature]])\nlin_reg = LinearRegression()\nlin_reg.fit(X_poly, y)\nplt.scatter(X[chosen_feature], y, color='r', label='Original Data')\nX_line = np.linspace(X[chosen_feature].min(), X[chosen_feature].max(), 100).reshape(-1, 1)\nX_line_poly = poly_features.transform(X_line)\ny_line_pred = lin_reg.predict(X_line_poly)\nplt.plot(X_line, y_line_pred, color='b', label='Polynomial Regression')\nplt.xlabel(chosen_feature)\nplt.ylabel('SALE_PRC')\nplt.legend()\nplt.show()\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n\n\nfrom sklearn.linear_model import LinearRegression\n\ny = df['SALE_PRC']\nnum_features = X.shape[1]\nfig, axes = plt.subplots(nrows=num_features, ncols=1, figsize=(10, 2*num_features))\n\nfor i, feature in enumerate(X.columns):\n    X_single_feature = X[feature].values.reshape(-1, 1)\n    lin_reg = LinearRegression()\n    lin_reg.fit(X_single_feature, y)\n    axes[i].scatter(X[feature], y, color='r', label='Original Data')\n    X_line = np.linspace(X[feature].min(), X[feature].max(), 100).reshape(-1, 1)\n    y_line_pred = lin_reg.predict(X_line)\n    axes[i].plot(X_line, y_line_pred, color='b', label='Simple Linear Regression')\n    axes[i].set_xlabel(feature)\n    axes[i].set_ylabel('SALE_PRC')\n    axes[i].legend()\nplt.show()\n\n\n\n\n\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly_features.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)\npoly_reg_model = LinearRegression()\npoly_reg_model.fit(X_train, y_train)\naccuracy = poly_reg_model.score(X_test, y_test)\nprint(f\"Polynomial Regression Model Accuracy: {accuracy}\")\n\nPolynomial Regression Model Accuracy: 0.881111708167996\n\n\n\nX_poly.round(2)\n\narray([[ 2.589e+01, -8.016e+01,  9.375e+03, ...,  6.400e+01,  3.200e+01,\n         1.600e+01],\n       [ 2.589e+01, -8.015e+01,  9.375e+03, ...,  8.100e+01,  3.600e+01,\n         1.600e+01],\n       [ 2.589e+01, -8.015e+01,  9.375e+03, ...,  4.000e+00,  8.000e+00,\n         1.600e+01],\n       ...,\n       [ 2.578e+01, -8.026e+01,  8.460e+03, ...,  4.900e+01,  2.800e+01,\n         1.600e+01],\n       [ 2.578e+01, -8.026e+01,  7.500e+03, ...,  6.400e+01,  3.200e+01,\n         1.600e+01],\n       [ 2.578e+01, -8.026e+01,  8.833e+03, ...,  1.210e+02,  4.400e+01,\n         1.600e+01]])\n\n\n\nprint(poly_reg_model.coef_.round(2))\n\n[ 1.64638752e+09  1.43501940e+08  1.99361000e+03  7.18771000e+03\n  5.72473000e+03  1.09326100e+04 -2.20423400e+04 -2.27902000e+03\n  5.99300000e+01 -8.40520000e+02 -1.25848900e+04 -1.22307374e+06\n -1.67500006e+06 -1.76007415e+06  2.15822777e+07  4.92283607e+06\n  2.37884737e+07 -7.24700000e+01 -2.26910000e+02 -2.41000000e+01\n -6.05400000e+01  7.95200000e+01  8.66300000e+01  5.84100000e+01\n -5.57000000e+00  4.27100000e+01  9.13476000e+03  1.16614383e+07\n -2.10318000e+03 -8.07204800e+04  4.66524507e+06  1.23000000e+00\n  1.33300000e+01  6.36000000e+01  1.16820000e+02 -2.49730000e+02\n -2.30000000e-01  1.92500000e+01 -1.20200000e+01 -1.43320000e+02\n -1.23242000e+04  3.73434037e+06 -2.26988600e+04  2.44413360e+05\n -0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00 -0.00000000e+00  0.00000000e+00\n  0.00000000e+00  1.00000000e-01 -2.84000000e+00  3.00000000e-02\n -3.70000000e-01  1.00000000e-02  0.00000000e+00 -0.00000000e+00\n -0.00000000e+00  0.00000000e+00 -0.00000000e+00  0.00000000e+00\n  0.00000000e+00 -2.56000000e+00 -8.56900000e+01 -1.20000000e-01\n  2.69200000e+01 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n -0.00000000e+00  0.00000000e+00 -0.00000000e+00  0.00000000e+00\n -4.00000000e-02 -2.83000000e+00  1.00000000e-02  3.50000000e-01\n -0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n  0.00000000e+00 -0.00000000e+00  1.00000000e-02  4.45000000e+00\n  3.00000000e-02  1.29000000e+00 -0.00000000e+00 -0.00000000e+00\n -0.00000000e+00  0.00000000e+00 -0.00000000e+00 -6.00000000e-02\n -1.37300000e+01  3.00000000e-02  6.00000000e-02 -0.00000000e+00\n  0.00000000e+00 -0.00000000e+00  0.00000000e+00  8.00000000e-02\n  4.09000000e+00  1.00000000e-02  7.20000000e-01 -0.00000000e+00\n  0.00000000e+00 -0.00000000e+00 -1.00000000e-02  4.62000000e+01\n -8.00000000e-02 -6.50000000e-01  0.00000000e+00  0.00000000e+00\n  3.00000000e-02 -3.48700000e+01  5.00000000e-02  1.60000000e-01\n  0.00000000e+00  4.00000000e-02 -1.87100000e+01 -7.00000000e-02\n  1.34000000e+00  9.35000000e+00  6.75860000e+02 -1.79900000e+01\n  1.14010000e+02  5.66658400e+05  1.42371000e+03 -2.88494500e+04\n -1.58070000e+02 -8.33910000e+02  1.79120600e+04]\n\n\n\nprint(poly_reg_model.intercept_)\n\n-15030137122.62008\n\n\n\ndegree = 2\nfig, axes = plt.subplots(nrows=num_features, ncols=1, figsize=(10, 2*num_features))\nfig.tight_layout(h_pad=4)\nfor i, feature in enumerate(X.columns):\n    X_single_feature = X[feature].values.reshape(-1, 1)\n    poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n    X_poly = poly_features.fit_transform(X_single_feature)\n    poly_reg = LinearRegression()\n    poly_reg.fit(X_poly, y)\n    axes[i].scatter(X[feature], y, color='r', label='Original Data')\n    X_line = np.linspace(X[feature].min(), X[feature].max(), 100).reshape(-1, 1)\n    X_line_poly = poly_features.transform(X_line)\n    y_line_pred = poly_reg.predict(X_line_poly)\n    axes[i].plot(X_line, y_line_pred, color='b', label=f'Polynomial Regression (Degree {degree})')\n    axes[i].set_xlabel(feature)\n    axes[i].set_ylabel('SALE_PRC')\n    axes[i].legend()\n\nplt.show()\n\n\n\n\n\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly_features.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)\npoly_reg_model = LinearRegression()\npoly_reg_model.fit(X_train, y_train)\naccuracy = poly_reg_model.score(X_test, y_test)\nprint(f\"Polynomial Regression Model Accuracy: {accuracy}\")\n\nPolynomial Regression Model Accuracy: 0.881111708167996\n\n\n\nnum_features = X.shape[1]\ndegree = 3\nfig, axes = plt.subplots(nrows=num_features, ncols=1, figsize=(10, 2*num_features))\nfig.tight_layout(h_pad=4)\nfor i, feature in enumerate(X.columns):\n    X_single_feature = X[feature].values.reshape(-1, 1)\n    poly_features = PolynomialFeatures(degree=degree, include_bias=True) \n    X_poly = poly_features.fit_transform(X_single_feature)\n    poly_reg = LinearRegression()\n    poly_reg.fit(X_poly, y)\n    axes[i].scatter(X[feature], y, color='r', label='Original Data')\n    X_line = np.linspace(X[feature].min(), X[feature].max(), 100).reshape(-1, 1)\n    X_line_poly = poly_features.transform(X_line)\n    y_line_pred = poly_reg.predict(X_line_poly)\n    axes[i].plot(X_line, y_line_pred, color='b', label=f'Polynomial Regression (Degree {degree})')\n    axes[i].set_xlabel(feature)\n    axes[i].set_ylabel('SALE_PRC')\n    axes[i].legend()\nplt.show()\n\n\n\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\nX = df[features]\ndegree = 3\npoly_features = PolynomialFeatures(degree=degree, include_bias=False)\nX_poly = poly_features.fit_transform(X)\npoly_feature = poly_features.get_feature_names_out(X.columns)\nX_poly_df = pd.DataFrame(X_poly, columns=poly_feature)\nprint(X_poly_df.head())\n\n    LATITUDE  LONGITUDE  LND_SQFOOT  TOT_LVG_AREA  SPEC_FEAT_VAL  RAIL_DIST  \\\n0  25.891031 -80.160561      9375.0        1753.0            0.0     2815.9   \n1  25.891324 -80.153968      9375.0        1715.0            0.0     4359.1   \n2  25.891334 -80.153740      9375.0        2276.0        49206.0     4412.9   \n3  25.891765 -80.152657     12450.0        2058.0        10033.0     4585.0   \n4  25.891825 -80.154639     12800.0        1684.0        16681.0     4063.4   \n\n   OCEAN_DIST  WATER_DIST  CNTR_DIST  SUBCNTR_DI  ...  avno60plus^3  \\\n0     12811.4       347.6    42815.3     37742.2  ...           0.0   \n1     10648.4       337.8    43504.9     37340.5  ...           0.0   \n2     10574.1       297.1    43530.4     37328.7  ...           0.0   \n3     10156.5         0.0    43797.5     37423.2  ...           0.0   \n4     10836.8       326.6    43599.7     37550.8  ...           0.0   \n\n   avno60plus^2 month_sold  avno60plus^2 structure_quality  \\\n0                      0.0                             0.0   \n1                      0.0                             0.0   \n2                      0.0                             0.0   \n3                      0.0                             0.0   \n4                      0.0                             0.0   \n\n   avno60plus month_sold^2  avno60plus month_sold structure_quality  \\\n0                      0.0                                      0.0   \n1                      0.0                                      0.0   \n2                      0.0                                      0.0   \n3                      0.0                                      0.0   \n4                      0.0                                      0.0   \n\n   avno60plus structure_quality^2  month_sold^3  \\\n0                             0.0         512.0   \n1                             0.0         729.0   \n2                             0.0           8.0   \n3                             0.0         729.0   \n4                             0.0         343.0   \n\n   month_sold^2 structure_quality  month_sold structure_quality^2  \\\n0                           256.0                           128.0   \n1                           324.0                           144.0   \n2                            16.0                            32.0   \n3                           324.0                           144.0   \n4                           196.0                           112.0   \n\n   structure_quality^3  \n0                 64.0  \n1                 64.0  \n2                 64.0  \n3                 64.0  \n4                 64.0  \n\n[5 rows x 815 columns]\n\n\n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom datetime import datetime\n\ndf = pd.read_csv('miami-housing.csv')\nthresholdPrice = 500000\ndf['Above_Threshold'] = (df['SALE_PRC'] &gt; thresholdPrice).astype(int)\ndf = df.drop(['PARCELNO'], axis=1)\ny = df['Above_Threshold'].values\nlabelencoder = LabelEncoder()\nY = labelencoder.fit_transform(y)   \nX = df.drop(labels=['Above_Threshold', 'SALE_PRC'], axis=1)  \nfeature_names = np.array(X.columns)\nscaler = StandardScaler()\nscaler.fit(X)\nX = scaler.transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\ndtrain_lgb = lgb.Dataset(X_train, label=y_train)\n\nlgbm_params = {\n    'learning_rate': 0.05,\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': ['auc', 'binary_logloss'],\n    'num_leaves': 100,\n    'max_depth': 10\n}\nstartLgb = datetime.now()\nclf_lgb = lgb.train(lgbm_params, dtrain_lgb, 50)\nstopLgb = datetime.now()\nexecutionTime_lgb = stopLgb - startLgb\nypred_lgb = clf_lgb.predict(X_test)\nypred_lgb = (ypred_lgb &gt;= 0.5).astype(int)\ndtrain_xgb = xgb.DMatrix(X_train, label=y_train)\nparameters_xgb = {\n    'max_depth': 10,\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'learning_rate': 0.05\n}\nstartXgb = datetime.now()\nxg = xgb.train(parameters_xgb, dtrain_xgb, 50)\nstopXgb = datetime.now()\nexecutionTime_xgb = stopXgb - startXgb\ndtest_xgb = xgb.DMatrix(X_test)\nypred_xgb = xg.predict(dtest_xgb)\nypred_xgb = (ypred_xgb &gt;= 0.5).astype(int)\ncm_xgb = confusion_matrix(y_test, ypred_xgb)\nsns.heatmap(cm_xgb,fmt='g',cmap=\"crest\" )\nfor i in range(len(cm_xgb)):\n    for j in range(len(cm_xgb[0])):\n        plt.text(j + 0.5, i + 0.5, str(cm_xgb[i, j]), ha='center', va='center', fontsize=16, color='white')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('XGBoost Confusion Matrix')\nplt.show() \nprint(\"_________________________________________________\")\nprint(\"LGBM execution time is: \", executionTime_lgb )\nprint(\"XGBoost execution time is: \", executionTime_xgb)\nprint(\"_________________________________________________\")\nprint(\"Accuracy with LGBM = \", np.mean(ypred_lgb == y_test))\nprint(\"Accuracy with XGBoost = \", np.mean(ypred_xgb == y_test))\nprint(\"___________________________________________________\")\nprint(\"AUC score with LGBM is: \", roc_auc_score(y_test, ypred_lgb))\nprint(\"AUC score with XGBoost is: \", roc_auc_score(y_test,ypred_xgb))\n\n[LightGBM] [Info] Number of positive: 2015, number of negative: 9130\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000558 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2921\n[LightGBM] [Info] Number of data points in the train set: 11145, number of used features: 15\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.180799 -&gt; initscore=-1.510946\n[LightGBM] [Info] Start training from score -1.510946\n_________________________________________________\nLGBM execution time is:  0:00:00.108216\nXGBoost execution time is:  0:00:00.233882\n_________________________________________________\nAccuracy with LGBM =  0.9551489056332975\nAccuracy with XGBoost =  0.9529960531036957\n___________________________________________________\nAUC score with LGBM is:  0.9180927441443535\nAUC score with XGBoost is:  0.9174731495161104"
  },
  {
    "objectID": "posts/Probability theory and random variables/index.html",
    "href": "posts/Probability theory and random variables/index.html",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "Probability Distribution\nContent\n\n    What is Probability Distribution?\n    Random Variables\n    Definition of Random Variable\n    Types of Random Variables\n\n\n    What is Probability Distribution?\n    In Probability Distribution, A Random Variable’s outcome is uncertain. Here, the outcome’s observation is known as Realization. It is a Function that maps Sample Space into a Real number space, known as State Space. They can be Discrete or Continuous.\n        Probability Distribution is basically the set of all possible outcomes of any random experiment or event. \n\n\n    Random Variables\n    Random Variable is a real-valued function whose domain is the sample space of the random experiment. It is represented as\n        X(sample space) = Real number. We need to learn the concept of Random Variables because sometimes we are just only interested in the probability of the event but also in the number of events associated with the random experiment.\n        The importance of random variables can be better understood by the following example:\n    Why do we need Random Variables?\n    \n         So now we flip our coin 3 times, and we want to answer some questions.\n    \n    \n        What is the probability of getting exactly 3 heads?\n        What is the probability of getting less than 3 heads?\n        What is the probability of getting more than 1 head?\n    \n    Then our general way of writing would be:\n    \n        P(Probability of getting exactly 3 heads when we flip a coin 3 times)\n        P(Probability of getting less than 3 heads when we flip a coin 3 times)\n        P(Probability of getting more than 1 head when we flip a coin 3 times) \n    \n    \n\n    Definition of Random Variable\n    A random variable is a real valued function whose domain is the sample space of a random experiment\n    Types of Random Variables in Probability Distribution\n    There are following two types of \n    \n        \n            Discrete Random Variables\n        \n        \n            Continuous Random Variables\n        \n    \n    Discrete Random Variables in Probability Distribution\n    In probability theory, a discrete random variable is a type of random variable that can take on a finite or countable number of distinct values. These values are often represented by integers or whole numbers, other than this they can also be represented\n        by other discrete values. \n    For example, the number of heads obtained after flipping a coin three times is a discrete random variable. The possible values of this variable are 0, 1, 2, or 3.\n     there are various other examples of random discrete variables.\n    \n        The number of cars that pass through a given intersection in an hour.\n        The number of defective items in a shipment of goods.\n        The number of red balls drawn in a sample of 10 balls taken from a jar containing both red and blue balls.\n    \n    Different Types of Probability Distributions\n    \n        Discrete Probability Distributions for discrete variables\n        Cumulative Probability Distribution for continuous variables\n    \n    \n\n\nimport warnings\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import Image\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import DBSCAN\nsns.set(style=\"darkgrid\", palette=\"pastel\", color_codes=True)\nsns.set_context('talk')\n\nThe dataset, 'Walmart_Store_sales.csv', is loaded into a Pandas DataFrame, providing a structured and manipulable format for the data.The first five rows of the dataset are displayed, offering an initial glimpse into the data's structure and contents. \n\ndf = pd.read_csv('Walmart_Store_sales.csv')\ndf.head(5)\n\n\n\n\n\n\n\n\nStore\nDate\nWeekly_Sales\nHoliday_Flag\nTemperature\nFuel_Price\nCPI\nUnemployment\n\n\n\n\n0\n1\n05-02-2010\n1643690.90\n0\n42.31\n2.572\n211.096358\n8.106\n\n\n1\n1\n12-02-2010\n1641957.44\n1\n38.51\n2.548\n211.242170\n8.106\n\n\n2\n1\n19-02-2010\n1611968.17\n0\n39.93\n2.514\n211.289143\n8.106\n\n\n3\n1\n26-02-2010\n1409727.59\n0\n46.63\n2.561\n211.319643\n8.106\n\n\n4\n1\n05-03-2010\n1554806.68\n0\n46.50\n2.625\n211.350143\n8.106\n\n\n\n\n\n\n\nThis provides an overview of the column names. \n\ndf.columns\n\nIndex(['Store', 'Date', 'Weekly_Sales', 'Holiday_Flag', 'Temperature',\n       'Fuel_Price', 'CPI', 'Unemployment'],\n      dtype='object')\n\n\nThis code command provides a concise summary of a DataFrame in Python, displaying information such as the number of non-null values, data types, and memory usage. \n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 6435 entries, 0 to 6434\nData columns (total 8 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   Store         6435 non-null   int64  \n 1   Date          6435 non-null   object \n 2   Weekly_Sales  6435 non-null   float64\n 3   Holiday_Flag  6435 non-null   int64  \n 4   Temperature   6435 non-null   float64\n 5   Fuel_Price    6435 non-null   float64\n 6   CPI           6435 non-null   float64\n 7   Unemployment  6435 non-null   float64\ndtypes: float64(5), int64(2), object(1)\nmemory usage: 402.3+ KB\n\n\nThis code retrieves the column names of a DataFrame in pandas, providing a quick overview of the dataset's features \nThis code provides a quick overview of the central tendency, dispersion, and distribution of the numerical data in the DataFrame. \n\ndf.describe()\n\n\n\n\n\n\n\n\nStore\nWeekly_Sales\nHoliday_Flag\nTemperature\nFuel_Price\nCPI\nUnemployment\n\n\n\n\ncount\n6435.000000\n6.435000e+03\n6435.000000\n6435.000000\n6435.000000\n6435.000000\n6435.000000\n\n\nmean\n23.000000\n1.046965e+06\n0.069930\n60.663782\n3.358607\n171.578394\n7.999151\n\n\nstd\n12.988182\n5.643666e+05\n0.255049\n18.444933\n0.459020\n39.356712\n1.875885\n\n\nmin\n1.000000\n2.099862e+05\n0.000000\n-2.060000\n2.472000\n126.064000\n3.879000\n\n\n25%\n12.000000\n5.533501e+05\n0.000000\n47.460000\n2.933000\n131.735000\n6.891000\n\n\n50%\n23.000000\n9.607460e+05\n0.000000\n62.670000\n3.445000\n182.616521\n7.874000\n\n\n75%\n34.000000\n1.420159e+06\n0.000000\n74.940000\n3.735000\n212.743293\n8.622000\n\n\nmax\n45.000000\n3.818686e+06\n1.000000\n100.140000\n4.468000\n227.232807\n14.313000\n\n\n\n\n\n\n\ndf.shape  It returns a tuple representing the shape of the DataFrame in the form (number of rows, number of columns) \n\ndf.shape\n\n(6435, 8)\n\n\ndf.isnull() is a useful tool for data cleaning and exploration to identify and handle missing values in a dataset.  \n\ndf.isnull()\n\n\n\n\n\n\n\n\nStore\nDate\nWeekly_Sales\nHoliday_Flag\nTemperature\nFuel_Price\nCPI\nUnemployment\n\n\n\n\n0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6430\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n6431\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n6432\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n6433\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n6434\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n6435 rows × 8 columns\n\n\n\ndropna is used to dropping rows with missing values  from the DataFrame.\n    .sum() checks and counts the number of missing values in each column of the DataFrame  \n\ndf.dropna(inplace =True)\ndf.isnull().sum()\n\nStore           0\nDate            0\nWeekly_Sales    0\nHoliday_Flag    0\nTemperature     0\nFuel_Price      0\nCPI             0\nUnemployment    0\ndtype: int64\n\n\nTo Check  if there are any duplicated rows in the DataFrame.  \n\ndf.duplicated().any()\n\nFalse\n\n\nThis code provides a visual representation of the distribution of the variable Weekly_Sales along with its fitted normal distribution.  \n\nfrom scipy.stats import norm\nvariable_of_interest = 'Weekly_Sales'\nplt.figure(figsize=(8, 6))\nsns.histplot(df[variable_of_interest], kde=True, bins=30, color='blue', stat='density')\nmu, std = df[variable_of_interest].mean(), df[variable_of_interest].std()\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100).round(2)\np = norm.pdf(x, mu, std).round(2)\nplt.plot(x, p, 'k', linewidth=2)\nprint(variable_of_interest,x,p)\nplt.xlabel(variable_of_interest)\nplt.ylabel('Probability Density')\nplt.title(f'Probability Density Function for {variable_of_interest}')\nplt.show()\n\nWeekly_Sales [  29551.24   69647.91  109744.58  149841.25  189937.92  230034.58\n  270131.25  310227.92  350324.59  390421.26  430517.93  470614.6\n  510711.27  550807.94  590904.6   631001.27  671097.94  711194.61\n  751291.28  791387.95  831484.62  871581.29  911677.96  951774.62\n  991871.29 1031967.96 1072064.63 1112161.3  1152257.97 1192354.64\n 1232451.31 1272547.98 1312644.64 1352741.31 1392837.98 1432934.65\n 1473031.32 1513127.99 1553224.66 1593321.33 1633418.   1673514.66\n 1713611.33 1753708.   1793804.67 1833901.34 1873998.01 1914094.68\n 1954191.35 1994288.02 2034384.68 2074481.35 2114578.02 2154674.69\n 2194771.36 2234868.03 2274964.7  2315061.37 2355158.04 2395254.7\n 2435351.37 2475448.04 2515544.71 2555641.38 2595738.05 2635834.72\n 2675931.39 2716028.06 2756124.72 2796221.39 2836318.06 2876414.73\n 2916511.4  2956608.07 2996704.74 3036801.41 3076898.08 3116994.74\n 3157091.41 3197188.08 3237284.75 3277381.42 3317478.09 3357574.76\n 3397671.43 3437768.1  3477864.76 3517961.43 3558058.1  3598154.77\n 3638251.44 3678348.11 3718444.78 3758541.45 3798638.12 3838734.78\n 3878831.45 3918928.12 3959024.79 3999121.46] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0.]\n\n\n\n\n\nThis code efficiently organizes subplots for each column, creates histograms with KDE, overlays PDFs based on normal distributions, and displays the visualizations for multiple columns in your DataFrame. Adjustments are made for cases where the number of columns is odd.  \n\nsns.set(style=\"whitegrid\")\ndf.drop('Date', axis=1, inplace=True)\nnum_columns = len(df.columns)\nnum_rows = (num_columns // 2) + (num_columns % 2)  \nnum_cols = 2\nfig, axes = plt.subplots(num_rows, num_cols, figsize=(8, 3 * num_rows))\nif num_rows == 1:\n    axes = axes.reshape(1, -1)\nfor i, column in enumerate(df.columns):\n    row_idx = i // num_cols\n    col_idx = i % num_cols\n    sns.histplot(df[column], kde=True, bins=30, color='blue', stat='density', ax=axes[row_idx, col_idx])\n    mu, std = df[column].mean(), df[column].std()\n    xmin, xmax = axes[row_idx, col_idx].get_xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    axes[row_idx, col_idx].plot(x, p, 'k', linewidth=2)\n    print(column )\n    print(x,p)\n    axes[row_idx, col_idx].set_xlabel(column)\n    axes[row_idx, col_idx].set_ylabel('Probability Density')\n    axes[row_idx, col_idx].set_title(f'PDF for {column}')\n\nif num_columns % 2 == 1:\n    fig.delaxes(axes[-1, -1])\nplt.tight_layout()\nplt.show()\n\nStore\n[-1.2        -0.71111111 -0.22222222  0.26666667  0.75555556  1.24444444\n  1.73333333  2.22222222  2.71111111  3.2         3.68888889  4.17777778\n  4.66666667  5.15555556  5.64444444  6.13333333  6.62222222  7.11111111\n  7.6         8.08888889  8.57777778  9.06666667  9.55555556 10.04444444\n 10.53333333 11.02222222 11.51111111 12.         12.48888889 12.97777778\n 13.46666667 13.95555556 14.44444444 14.93333333 15.42222222 15.91111111\n 16.4        16.88888889 17.37777778 17.86666667 18.35555556 18.84444444\n 19.33333333 19.82222222 20.31111111 20.8        21.28888889 21.77777778\n 22.26666667 22.75555556 23.24444444 23.73333333 24.22222222 24.71111111\n 25.2        25.68888889 26.17777778 26.66666667 27.15555556 27.64444444\n 28.13333333 28.62222222 29.11111111 29.6        30.08888889 30.57777778\n 31.06666667 31.55555556 32.04444444 32.53333333 33.02222222 33.51111111\n 34.         34.48888889 34.97777778 35.46666667 35.95555556 36.44444444\n 36.93333333 37.42222222 37.91111111 38.4        38.88888889 39.37777778\n 39.86666667 40.35555556 40.84444444 41.33333333 41.82222222 42.31111111\n 42.8        43.28888889 43.77777778 44.26666667 44.75555556 45.24444444\n 45.73333333 46.22222222 46.71111111 47.2       ] [0.00541385 0.00580306 0.00621145 0.00663916 0.00708628 0.00755281\n 0.00803865 0.00854362 0.00906746 0.0096098  0.01017016 0.01074795\n 0.01134248 0.01195295 0.01257845 0.01321793 0.01387027 0.01453418\n 0.01520832 0.01589119 0.01658121 0.0172767  0.01797588 0.01867687\n 0.01937772 0.0200764  0.02077083 0.02145885 0.02213827 0.02280686\n 0.02346238 0.02410257 0.02472517 0.02532794 0.02590868 0.0264652\n 0.0269954  0.02749724 0.02796875 0.02840806 0.02881343 0.0291832\n 0.02951587 0.02981006 0.03006456 0.0302783  0.03045039 0.03058009\n 0.03066687 0.03071035 0.03071035 0.03066687 0.03058009 0.03045039\n 0.0302783  0.03006456 0.02981006 0.02951587 0.0291832  0.02881343\n 0.02840806 0.02796875 0.02749724 0.0269954  0.0264652  0.02590868\n 0.02532794 0.02472517 0.02410257 0.02346238 0.02280686 0.02213827\n 0.02145885 0.02077083 0.0200764  0.01937772 0.01867687 0.01797588\n 0.0172767  0.01658121 0.01589119 0.01520832 0.01453418 0.01387027\n 0.01321793 0.01257845 0.01195295 0.01134248 0.01074795 0.01017016\n 0.0096098  0.00906746 0.00854362 0.00803865 0.00755281 0.00708628\n 0.00663916 0.00621145 0.00580306 0.00541385]\nWeekly_Sales\n[  29551.24         69647.90888889  109744.57777778  149841.24666667\n  189937.91555556  230034.58444444  270131.25333333  310227.92222222\n  350324.59111111  390421.26        430517.92888889  470614.59777778\n  510711.26666667  550807.93555556  590904.60444444  631001.27333333\n  671097.94222222  711194.61111111  751291.28        791387.94888889\n  831484.61777778  871581.28666667  911677.95555556  951774.62444444\n  991871.29333333 1031967.96222222 1072064.63111111 1112161.3\n 1152257.96888889 1192354.63777778 1232451.30666667 1272547.97555556\n 1312644.64444444 1352741.31333333 1392837.98222222 1432934.65111111\n 1473031.32       1513127.98888889 1553224.65777778 1593321.32666667\n 1633417.99555556 1673514.66444444 1713611.33333333 1753708.00222222\n 1793804.67111111 1833901.34       1873998.00888889 1914094.67777778\n 1954191.34666667 1994288.01555556 2034384.68444444 2074481.35333333\n 2114578.02222222 2154674.69111111 2194771.36       2234868.02888889\n 2274964.69777778 2315061.36666667 2355158.03555556 2395254.70444444\n 2435351.37333333 2475448.04222222 2515544.71111111 2555641.38\n 2595738.04888889 2635834.71777778 2675931.38666667 2716028.05555556\n 2756124.72444444 2796221.39333333 2836318.06222222 2876414.73111111\n 2916511.4        2956608.06888889 2996704.73777778 3036801.40666667\n 3076898.07555556 3116994.74444444 3157091.41333333 3197188.08222222\n 3237284.75111111 3277381.42       3317478.08888889 3357574.75777778\n 3397671.42666667 3437768.09555556 3477864.76444444 3517961.43333333\n 3558058.10222222 3598154.77111111 3638251.44       3678348.10888889\n 3718444.77777778 3758541.44666667 3798638.11555556 3838734.78444444\n 3878831.45333333 3918928.12222222 3959024.79111111 3999121.46      ] [1.39199576e-07 1.57821620e-07 1.78033976e-07 1.99823745e-07\n 2.23151138e-07 2.47947041e-07 2.74111062e-07 3.01510196e-07\n 3.29978205e-07 3.59315802e-07 3.89291739e-07 4.19644822e-07\n 4.50086885e-07 4.80306715e-07 5.09974861e-07 5.38749261e-07\n 5.66281556e-07 5.92223934e-07 6.16236337e-07 6.37993805e-07\n 6.57193757e-07 6.73562975e-07 6.86864061e-07 6.96901164e-07\n 7.03524775e-07 7.06635436e-07 7.06186218e-07 7.02183909e-07\n 6.94688834e-07 6.83813346e-07 6.69719026e-07 6.52612685e-07\n 6.32741318e-07 6.10386174e-07 5.85856147e-07 5.59480692e-07\n 5.31602514e-07 5.02570228e-07 4.72731236e-07 4.42424991e-07\n 4.11976849e-07 3.81692637e-07 3.51854057e-07 3.22715000e-07\n 2.94498815e-07 2.67396526e-07 2.41565985e-07 2.17131888e-07\n 1.94186595e-07 1.72791628e-07 1.52979752e-07 1.34757523e-07\n 1.18108158e-07 1.02994630e-07 8.93628642e-08 7.71449294e-08\n 6.62621467e-08 5.66280265e-08 4.81509835e-08 4.07367808e-08\n 3.42906771e-08 2.87192586e-08 2.39319561e-08 1.98422529e-08\n 1.63686015e-08 1.34350713e-08 1.09717572e-08 8.91497668e-09\n 7.20729076e-09 5.79737888e-09 4.63979866e-09 3.69465929e-09\n 2.92723417e-09 2.30753526e-09 1.80986856e-09 1.41238643e-09\n 1.09664950e-09 8.47207833e-10 6.51208245e-10 4.98032436e-10\n 3.78968556e-10 2.86917167e-10 2.16131317e-10 1.61989430e-10\n 1.20799035e-10 8.96288985e-11 6.61668508e-11 4.86004936e-11\n 3.55180165e-11 2.58264396e-11 1.86847884e-11 1.34499180e-11\n 9.63294103e-12 6.86445362e-12 4.86699453e-12 3.43339326e-12\n 2.40987243e-12 1.68295446e-12 1.16938761e-12 8.08448608e-13]\nHoliday_Flag\n[-0.05       -0.03888889 -0.02777778 -0.01666667 -0.00555556  0.00555556\n  0.01666667  0.02777778  0.03888889  0.05        0.06111111  0.07222222\n  0.08333333  0.09444444  0.10555556  0.11666667  0.12777778  0.13888889\n  0.15        0.16111111  0.17222222  0.18333333  0.19444444  0.20555556\n  0.21666667  0.22777778  0.23888889  0.25        0.26111111  0.27222222\n  0.28333333  0.29444444  0.30555556  0.31666667  0.32777778  0.33888889\n  0.35        0.36111111  0.37222222  0.38333333  0.39444444  0.40555556\n  0.41666667  0.42777778  0.43888889  0.45        0.46111111  0.47222222\n  0.48333333  0.49444444  0.50555556  0.51666667  0.52777778  0.53888889\n  0.55        0.56111111  0.57222222  0.58333333  0.59444444  0.60555556\n  0.61666667  0.62777778  0.63888889  0.65        0.66111111  0.67222222\n  0.68333333  0.69444444  0.70555556  0.71666667  0.72777778  0.73888889\n  0.75        0.76111111  0.77222222  0.78333333  0.79444444  0.80555556\n  0.81666667  0.82777778  0.83888889  0.85        0.86111111  0.87222222\n  0.88333333  0.89444444  0.90555556  0.91666667  0.92777778  0.93888889\n  0.95        0.96111111  0.97222222  0.98333333  0.99444444  1.00555556\n  1.01666667  1.02777778  1.03888889  1.05      ] [1.40046748e+00 1.42809627e+00 1.45350893e+00 1.47656879e+00\n 1.49715040e+00 1.51514060e+00 1.53043964e+00 1.54296203e+00\n 1.55263737e+00 1.55941100e+00 1.56324451e+00 1.56411614e+00\n 1.56202091e+00 1.55697075e+00 1.54899432e+00 1.53813679e+00\n 1.52445938e+00 1.50803880e+00 1.48896655e+00 1.46734800e+00\n 1.44330154e+00 1.41695737e+00 1.38845644e+00 1.35794911e+00\n 1.32559390e+00 1.29155605e+00 1.25600620e+00 1.21911892e+00\n 1.18107131e+00 1.14204163e+00 1.10220788e+00 1.06174653e+00\n 1.02083124e+00 9.79631669e-01 9.38312373e-01 8.97031788e-01\n 8.55941308e-01 8.15184478e-01 7.74896287e-01 7.35202577e-01\n 6.96219566e-01 6.58053481e-01 6.20800303e-01 5.84545622e-01\n 5.49364595e-01 5.15322001e-01 4.82472389e-01 4.50860313e-01\n 4.20520637e-01 3.91478921e-01 3.63751853e-01 3.37347742e-01\n 3.12267053e-01 2.88502967e-01 2.66041977e-01 2.44864492e-01\n 2.24945454e-01 2.06254955e-01 1.88758851e-01 1.72419357e-01\n 1.57195634e-01 1.43044348e-01 1.29920202e-01 1.17776443e-01\n 1.06565332e-01 9.62385821e-02 8.67477564e-02 7.80446342e-02\n 7.00815358e-02 6.28116113e-02 5.61890929e-02 5.01695113e-02\n 4.47098778e-02 3.97688340e-02 3.53067705e-02 3.12859175e-02\n 2.76704077e-02 2.44263168e-02 2.15216802e-02 1.89264920e-02\n 1.66126853e-02 1.45540987e-02 1.27264289e-02 1.11071736e-02\n 9.67556475e-03 8.41249563e-03 7.30044179e-03 6.32337876e-03\n 5.46669711e-03 4.71711638e-03 4.06259877e-03 3.49226376e-03\n 2.99630421e-03 2.56590478e-03 2.19316309e-03 1.87101420e-03\n 1.59315856e-03 1.35399382e-03 1.14855058e-03 9.72432206e-04]\nTemperature\n[ -7.17        -6.03444444  -4.89888889  -3.76333333  -2.62777778\n  -1.49222222  -0.35666667   0.77888889   1.91444444   3.05\n   4.18555556   5.32111111   6.45666667   7.59222222   8.72777778\n   9.86333333  10.99888889  12.13444444  13.27        14.40555556\n  15.54111111  16.67666667  17.81222222  18.94777778  20.08333333\n  21.21888889  22.35444444  23.49        24.62555556  25.76111111\n  26.89666667  28.03222222  29.16777778  30.30333333  31.43888889\n  32.57444444  33.71        34.84555556  35.98111111  37.11666667\n  38.25222222  39.38777778  40.52333333  41.65888889  42.79444444\n  43.93        45.06555556  46.20111111  47.33666667  48.47222222\n  49.60777778  50.74333333  51.87888889  53.01444444  54.15\n  55.28555556  56.42111111  57.55666667  58.69222222  59.82777778\n  60.96333333  62.09888889  63.23444444  64.37        65.50555556\n  66.64111111  67.77666667  68.91222222  70.04777778  71.18333333\n  72.31888889  73.45444444  74.59        75.72555556  76.86111111\n  77.99666667  79.13222222  80.26777778  81.40333333  82.53888889\n  83.67444444  84.81        85.94555556  87.08111111  88.21666667\n  89.35222222  90.48777778  91.62333333  92.75888889  93.89444444\n  95.03        96.16555556  97.30111111  98.43666667  99.57222222\n 100.70777778 101.84333333 102.97888889 104.11444444 105.25      ] [2.50099169e-05 3.13053732e-05 3.90372716e-05 4.84946649e-05\n 6.00153584e-05 7.39920024e-05 9.08784879e-05 1.11196557e-04\n 1.35542509e-04 1.64593863e-04 1.99115786e-04 2.39967087e-04\n 2.88105535e-04 3.44592214e-04 4.10594620e-04 4.87388169e-04\n 5.76355772e-04 6.78985103e-04 7.96863217e-04 9.31668152e-04\n 1.08515720e-03 1.25915152e-03 1.45551696e-03 1.67614073e-03\n 1.92290410e-03 2.19765091e-03 2.50215220e-03 2.83806719e-03\n 3.20690105e-03 3.60995993e-03 4.04830419e-03 4.52270052e-03\n 5.03357400e-03 5.58096142e-03 6.16446680e-03 6.78322075e-03\n 7.43584484e-03 8.12042243e-03 8.83447733e-03 9.57496145e-03\n 1.03382527e-02 1.11201639e-02 1.19159636e-02 1.27204092e-02\n 1.35277924e-02 1.43319972e-02 1.51265691e-02 1.59047956e-02\n 1.66597961e-02 1.73846202e-02 1.80723517e-02 1.87162167e-02\n 1.93096942e-02 1.98466250e-02 2.03213177e-02 2.07286492e-02\n 2.10641566e-02 2.13241182e-02 2.15056228e-02 2.16066234e-02\n 2.16259759e-02 2.15634608e-02 2.14197869e-02 2.11965786e-02\n 2.08963445e-02 2.05224312e-02 2.00789608e-02 1.95707555e-02\n 1.90032502e-02 1.83823960e-02 1.77145564e-02 1.70063997e-02\n 1.62647884e-02 1.54966701e-02 1.47089711e-02 1.39084949e-02\n 1.31018286e-02 1.22952574e-02 1.14946902e-02 1.07055961e-02\n 9.93295275e-03 9.18120791e-03 8.45425241e-03 7.75540590e-03\n 7.08741381e-03 6.45245499e-03 5.85215894e-03 5.28763144e-03\n 4.75948733e-03 4.26788893e-03 3.81258889e-03 3.39297596e-03\n 3.00812253e-03 2.65683264e-03 2.33768940e-03 2.04910094e-03\n 1.78934397e-03 1.55660438e-03 1.34901445e-03 1.16468608e-03]\nFuel_Price\n[2.3722     2.39437778 2.41655556 2.43873333 2.46091111 2.48308889\n 2.50526667 2.52744444 2.54962222 2.5718     2.59397778 2.61615556\n 2.63833333 2.66051111 2.68268889 2.70486667 2.72704444 2.74922222\n 2.7714     2.79357778 2.81575556 2.83793333 2.86011111 2.88228889\n 2.90446667 2.92664444 2.94882222 2.971      2.99317778 3.01535556\n 3.03753333 3.05971111 3.08188889 3.10406667 3.12624444 3.14842222\n 3.1706     3.19277778 3.21495556 3.23713333 3.25931111 3.28148889\n 3.30366667 3.32584444 3.34802222 3.3702     3.39237778 3.41455556\n 3.43673333 3.45891111 3.48108889 3.50326667 3.52544444 3.54762222\n 3.5698     3.59197778 3.61415556 3.63633333 3.65851111 3.68068889\n 3.70286667 3.72504444 3.74722222 3.7694     3.79157778 3.81375556\n 3.83593333 3.85811111 3.88028889 3.90246667 3.92464444 3.94682222\n 3.969      3.99117778 4.01335556 4.03553333 4.05771111 4.07988889\n 4.10206667 4.12424444 4.14642222 4.1686     4.19077778 4.21295556\n 4.23513333 4.25731111 4.27948889 4.30166667 4.32384444 4.34602222\n 4.3682     4.39037778 4.41255556 4.43473333 4.45691111 4.47908889\n 4.50126667 4.52344444 4.54562222 4.5678    ] [0.08635809 0.09569467 0.10579341 0.11668518 0.12839821 0.14095758\n 0.15438463 0.16869643 0.18390516 0.20001756 0.21703437 0.23494981\n 0.25375105 0.27341781 0.2939219  0.3152269  0.33728792 0.36005139\n 0.38345498 0.40742761 0.43188958 0.45675275 0.48192096 0.5072904\n 0.53275024 0.55818332 0.58346693 0.60847371 0.63307269 0.65713036\n 0.68051182 0.70308203 0.72470709 0.74525553 0.76459965 0.78261681\n 0.79919072 0.81421272 0.82758292 0.83921133 0.84901889 0.8569383\n 0.86291484 0.866907   0.86888694 0.86884083 0.86676897 0.86268586\n 0.85661996 0.8486134  0.83872149 0.82701206 0.81356471 0.7984699\n 0.78182794 0.76374786 0.74434627 0.72374606 0.70207514 0.67946512\n 0.65604998 0.63196477 0.60734436 0.58232217 0.55702903 0.53159211\n 0.50613388 0.48077124 0.45561471 0.43076775 0.40632619 0.38237777\n 0.35900181 0.336269   0.31424127 0.29297178 0.27250504 0.25287709\n 0.23411574 0.21624095 0.19926519 0.18319395 0.1680262  0.15375494\n 0.14036775 0.12784736 0.11617225 0.10531718 0.09525378 0.0859511\n 0.07737609 0.06949417 0.06226961 0.05566601 0.04964668 0.04417499\n 0.03921471 0.03473023 0.03068687 0.02705102]\nCPI\n[121.00555966 122.12965751 123.25375537 124.37785322 125.50195107\n 126.62604893 127.75014678 128.87424463 129.99834249 131.12244034\n 132.24653819 133.37063605 134.4947339  135.61883175 136.74292961\n 137.86702746 138.99112531 140.11522317 141.23932102 142.36341887\n 143.48751673 144.61161458 145.73571243 146.85981029 147.98390814\n 149.10800599 150.23210385 151.3562017  152.48029955 153.60439741\n 154.72849526 155.85259311 156.97669097 158.10078882 159.22488667\n 160.34898453 161.47308238 162.59718023 163.72127809 164.84537594\n 165.96947379 167.09357165 168.2176695  169.34176735 170.46586521\n 171.58996306 172.71406091 173.83815877 174.96225662 176.08635447\n 177.21045233 178.33455018 179.45864803 180.58274589 181.70684374\n 182.83094159 183.95503945 185.0791373  186.20323515 187.32733301\n 188.45143086 189.57552871 190.69962657 191.82372442 192.94782227\n 194.07192013 195.19601798 196.32011583 197.44421369 198.56831154\n 199.69240939 200.81650725 201.9406051  203.06470295 204.18880081\n 205.31289866 206.43699651 207.56109437 208.68519222 209.80929007\n 210.93338793 212.05748578 213.18158363 214.30568149 215.42977934\n 216.55387719 217.67797505 218.8020729  219.92617075 221.05026861\n 222.17436646 223.29846431 224.42256217 225.54666002 226.67075787\n 227.79485573 228.91895358 230.04305143 231.16714929 232.29124714] [0.00443956 0.00460365 0.00476991 0.00493814 0.00510814 0.00527968\n 0.00545253 0.00562645 0.00580119 0.00597647 0.00615203 0.00632758\n 0.00650283 0.00667749 0.00685124 0.00702379 0.00719481 0.00736399\n 0.00753099 0.0076955  0.0078572  0.00801575 0.00817083 0.00832212\n 0.0084693  0.00861206 0.00875007 0.00888306 0.00901071 0.00913274\n 0.00924887 0.00935884 0.0094624  0.00955931 0.00964933 0.00973225\n 0.00980789 0.00987605 0.00993658 0.00998932 0.01003416 0.01007098\n 0.01009969 0.01012022 0.01013253 0.01013657 0.01013236 0.01011988\n 0.01009918 0.0100703  0.01003331 0.00998831 0.00993541 0.00987472\n 0.0098064  0.00973062 0.00964754 0.00955738 0.00946034 0.00935664\n 0.00924654 0.00913028 0.00900813 0.00888037 0.00874728 0.00860916\n 0.00846632 0.00831905 0.00816768 0.00801252 0.0078539  0.00769215\n 0.00752758 0.00736052 0.00719131 0.00702025 0.00684768 0.0066739\n 0.00649923 0.00632397 0.00614841 0.00597286 0.00579758 0.00562286\n 0.00544896 0.00527614 0.00510462 0.00493466 0.00476646 0.00460025\n 0.00443621 0.00427453 0.00411538 0.00395893 0.00380532 0.00365469\n 0.00350716 0.00336284 0.00322183 0.00308421]\nUnemployment\n[ 3.3573      3.47323333  3.58916667  3.7051      3.82103333  3.93696667\n  4.0529      4.16883333  4.28476667  4.4007      4.51663333  4.63256667\n  4.7485      4.86443333  4.98036667  5.0963      5.21223333  5.32816667\n  5.4441      5.56003333  5.67596667  5.7919      5.90783333  6.02376667\n  6.1397      6.25563333  6.37156667  6.4875      6.60343333  6.71936667\n  6.8353      6.95123333  7.06716667  7.1831      7.29903333  7.41496667\n  7.5309      7.64683333  7.76276667  7.8787      7.99463333  8.11056667\n  8.2265      8.34243333  8.45836667  8.5743      8.69023333  8.80616667\n  8.9221      9.03803333  9.15396667  9.2699      9.38583333  9.50176667\n  9.6177      9.73363333  9.84956667  9.9655     10.08143333 10.19736667\n 10.3133     10.42923333 10.54516667 10.6611     10.77703333 10.89296667\n 11.0089     11.12483333 11.24076667 11.3567     11.47263333 11.58856667\n 11.7045     11.82043333 11.93636667 12.0523     12.16823333 12.28416667\n 12.4001     12.51603333 12.63196667 12.7479     12.86383333 12.97976667\n 13.0957     13.21163333 13.32756667 13.4435     13.55943333 13.67536667\n 13.7913     13.90723333 14.02316667 14.1391     14.25503333 14.37096667\n 14.4869     14.60283333 14.71876667 14.8347    ] [0.0099562  0.01157924 0.01341552 0.01548376 0.01780273 0.02039097\n 0.02326646 0.02644625 0.02994602 0.03377966 0.03795882 0.04249241\n 0.04738613 0.05264199 0.05825787 0.06422707 0.07053795 0.0771736\n 0.08411161 0.09132388 0.09877658 0.10643018 0.11423965 0.12215469\n 0.13012018 0.13807669 0.14596116 0.15370765 0.16124819 0.16851379\n 0.17543542 0.18194508 0.18797695 0.19346841 0.19836122 0.20260246\n 0.2061455  0.20895089 0.21098705 0.2122309  0.21266824 0.21229409\n 0.2111127  0.20913757 0.2063911  0.20290423 0.19871583 0.19387197\n 0.18842513 0.18243318 0.17595843 0.16906649 0.16182522 0.15430362\n 0.14657072 0.1386946  0.1307414  0.12277442 0.11485341 0.10703384\n 0.0993664  0.09189655 0.08466426 0.07770379 0.07104369 0.06470682\n 0.05871051 0.05306679 0.04778274 0.04286082 0.03829932 0.03409282\n 0.03023264 0.02670732 0.02350314 0.02060453 0.01799453 0.01565524\n 0.01356814 0.01171445 0.01007546 0.00863274 0.00736841 0.00626528\n 0.00530699 0.00447814 0.00376433 0.00315224 0.00262961 0.00218527\n 0.00180909 0.00149196 0.00122573 0.00100317 0.00081789 0.00066428\n 0.00053747 0.00043321 0.00034784 0.00027823]\n\n\n\n\n\n\nfrom scipy.stats import rv_histogram\ncolumn_of_interest = 'Weekly_Sales'\nhist, bin_edges = np.histogram(df[column_of_interest], bins=30, density=True)\npmf = rv_histogram((hist, bin_edges))\nx_values = np.linspace(df[column_of_interest].min(), df[column_of_interest].max(), 100).round(2)\npmf_values = pmf.pdf(x_values)\nprint(\"X values\\n\",x_values)\nprint(\"pmf values \\n\",pmf_values)\nplt.bar(bin_edges[:-1], hist, width=np.diff(bin_edges), alpha=0.7, label='Histogram')\nplt.plot(x_values, pmf_values, color='red', label='PMF', linewidth=2)\nplt.title(f'Probability Mass Function (PMF) for {column_of_interest}')\nplt.xlabel(column_of_interest)\nplt.ylabel('Probability')\nplt.legend()\nplt.show()\n\nX values\n [ 209986.25  246437.77  282889.28  319340.8   355792.32  392243.84\n  428695.35  465146.87  501598.39  538049.9   574501.42  610952.94\n  647404.46  683855.97  720307.49  756759.01  793210.52  829662.04\n  866113.56  902565.08  939016.59  975468.11 1011919.63 1048371.14\n 1084822.66 1121274.18 1157725.7  1194177.21 1230628.73 1267080.25\n 1303531.77 1339983.28 1376434.8  1412886.32 1449337.83 1485789.35\n 1522240.87 1558692.39 1595143.9  1631595.42 1668046.94 1704498.45\n 1740949.97 1777401.49 1813853.01 1850304.52 1886756.04 1923207.56\n 1959659.07 1996110.59 2032562.11 2069013.63 2105465.14 2141916.66\n 2178368.18 2214819.69 2251271.21 2287722.73 2324174.25 2360625.76\n 2397077.28 2433528.8  2469980.31 2506431.83 2542883.35 2579334.87\n 2615786.38 2652237.9  2688689.42 2725140.93 2761592.45 2798043.97\n 2834495.49 2870947.   2907398.52 2943850.04 2980301.56 3016753.07\n 3053204.59 3089656.11 3126107.62 3162559.14 3199010.66 3235462.18\n 3271913.69 3308365.21 3344816.73 3381268.24 3417719.76 3454171.28\n 3490622.8  3527074.31 3563525.83 3599977.35 3636428.86 3672880.38\n 3709331.9  3745783.42 3782234.93 3818686.45]\npmf values \n [5.42589256e-07 5.42589256e-07 5.42589256e-07 5.42589256e-07\n 6.96322879e-07 6.96322879e-07 6.96322879e-07 9.53406836e-07\n 9.53406836e-07 9.53406836e-07 6.75652812e-07 6.75652812e-07\n 6.75652812e-07 6.75652812e-07 4.80579056e-07 4.80579056e-07\n 4.80579056e-07 6.09766974e-07 6.09766974e-07 6.09766974e-07\n 6.71777175e-07 6.71777175e-07 6.71777175e-07 6.71777175e-07\n 4.75411539e-07 4.75411539e-07 4.75411539e-07 4.93497847e-07\n 4.93497847e-07 4.93497847e-07 5.92972544e-07 5.92972544e-07\n 5.92972544e-07 4.43114559e-07 4.43114559e-07 4.43114559e-07\n 4.43114559e-07 3.19094158e-07 3.19094158e-07 3.19094158e-07\n 2.31246374e-07 2.31246374e-07 2.31246374e-07 3.11342883e-07\n 3.11342883e-07 3.11342883e-07 3.11342883e-07 3.20386037e-07\n 3.20386037e-07 3.20386037e-07 2.41581407e-07 2.41581407e-07\n 2.41581407e-07 1.27896039e-07 1.27896039e-07 1.27896039e-07\n 1.27896039e-07 3.61726171e-08 3.61726171e-08 3.61726171e-08\n 2.32538253e-08 2.32538253e-08 2.32538253e-08 1.55025502e-08\n 1.55025502e-08 1.55025502e-08 1.55025502e-08 1.03350335e-08\n 1.03350335e-08 1.03350335e-08 1.67944294e-08 1.67944294e-08\n 1.67944294e-08 6.45939591e-09 6.45939591e-09 6.45939591e-09\n 2.58375836e-09 2.58375836e-09 2.58375836e-09 2.58375836e-09\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.29187918e-09\n 1.29187918e-09 1.29187918e-09 2.58375836e-09 2.58375836e-09\n 2.58375836e-09 2.58375836e-09 5.16751673e-09 5.16751673e-09\n 5.16751673e-09 2.58375836e-09 2.58375836e-09 2.58375836e-09\n 3.87563755e-09 3.87563755e-09 3.87563755e-09 0.00000000e+00]\n\n\n\n\n\n\nfrom scipy.stats import geom\nselected_column = 'Temperature'\ntemperature_threshold = 60\nsuccess_data = df[df[selected_column] &gt; temperature_threshold]\np = len(success_data) / len(df)\nprint('probability of success ',p)\nk_values = np.arange(1, 11)\npmf_values = geom.pmf(k_values, p)\nplt.bar(k_values, pmf_values, align='center', alpha=0.7)\nplt.title(f'Geometric Distribution PMF for {selected_column}')\nplt.xlabel('Number of Trials Until First Success (k)')\nplt.ylabel('Probability')\nplt.show()\n\nprobability of success  0.5435897435897435\n\n\n\n\n\n\nimport statsmodels.api as sm\ncolumns = df.columns\nfig, axes = plt.subplots(nrows=len(columns), ncols=1, figsize=(8, 4 * len(columns)))\nfor i, column in enumerate(columns):\n    ecdf = sm.distributions.ECDF(df[column])\n    x = sorted(df[column].unique())\n    y = ecdf(x)\n    print(column)\n    print(x,y)\n    axes[i].step(x, y, label=column)\n    axes[i].set_title(f'Cumulative Distribution Function of {column}')\n    axes[i].set_xlabel(column)\n    axes[i].set_ylabel('Cumulative Probability')\n    axes[i].legend()\nplt.tight_layout()\nplt.show()\n\nStore\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45] [0.02222222 0.04444444 0.06666667 0.08888889 0.11111111 0.13333333\n 0.15555556 0.17777778 0.2        0.22222222 0.24444444 0.26666667\n 0.28888889 0.31111111 0.33333333 0.35555556 0.37777778 0.4\n 0.42222222 0.44444444 0.46666667 0.48888889 0.51111111 0.53333333\n 0.55555556 0.57777778 0.6        0.62222222 0.64444444 0.66666667\n 0.68888889 0.71111111 0.73333333 0.75555556 0.77777778 0.8\n 0.82222222 0.84444444 0.86666667 0.88888889 0.91111111 0.93333333\n 0.95555556 0.97777778 1.        ]\nWeekly_Sales\n[209986.25, 213538.32, 215359.21, 219804.85, 220060.35, 224031.19, 224294.39, 224639.76, 224806.96, 226702.36, 229731.98, 230519.49, 231155.9, 231319.96, 231976.84, 232769.09, 233543.08, 234175.92, 234218.03, 234793.12, 236157.12, 236920.49, 237095.82, 237129.81, 237405.82, 238084.08, 238172.66, 238433.53, 238875.26, 238915.05, 239198.36, 239206.26, 239431.85, 240044.57, 241937.11, 242047.03, 242456.39, 242526.7, 242771.37, 242813.51, 242901.21, 243477.03, 243948.82, 244338.31, 244856.44, 244899.2, 245435.8, 246124.61, 246277.18, 246970.97, 247234.47, 247672.56, 248051.53, 248561.86, 248603.3, 249134.32, 249246.8, 249798.75, 251294.5, 251327.67, 251732.92, 252349.6, 252709.58, 252734.31, 253050.1, 253205.89, 253731.13, 254412.34, 255996.47, 256091.32, 256235.19, 257031.19, 257361.3, 258427.39, 258533.12, 259419.91, 259527.75, 259638.35, 260636.71, 261131.09, 261666.29, 261837.2, 261851.74, 262159.13, 262407.57, 262717.71, 262789.95, 262893.76, 263263.02, 263917.85, 264214.12, 265003.47, 265367.51, 265444.9, 266300.98, 266484.19, 267058.08, 267065.35, 267495.76, 267956.3, 268708.43, 268929.03, 269624.2, 270097.76, 270110.3, 270281.63, 270373.05, 270497.51, 270516.84, 270677.98, 270921.44, 271924.73, 271961.04, 272190.83, 272399.08, 272487.33, 272489.41, 272803.94, 272834.88, 272997.65, 273079.07, 273282.97, 273690.37, 274593.43, 274634.52, 274721.85, 274742.63, 275020.96, 275142.17, 275313.34, 275749.56, 275883.23, 275911.97, 276011.4, 276157.8, 276198.74, 276279.49, 276899.95, 277137.86, 277417.53, 278031.81, 278067.73, 278253.28, 278287.04, 278529.71, 278646.35, 279088.39, 279246.33, 279364.13, 279427.93, 279447.22, 279466.87, 279524.44, 279643.43, 279677.0, 280048.74, 280357.3, 280472.78, 280681.2, 280701.7, 280785.76, 280937.84, 281090.95, 281247.97, 281313.78, 281514.26, 281699.68, 281706.41, 281842.28, 281909.79, 282235.73, 282351.82, 282545.55, 282552.58, 282558.65, 282647.48, 283178.12, 283248.62, 283455.13, 284052.77, 284309.34, 284322.52, 284496.14, 284617.27, 284740.5, 284988.27, 285031.81, 285100.0, 285379.86, 286082.76, 286117.72, 286197.5, 286347.26, 286428.78, 286477.35, 286497.49, 286515.92, 286857.13, 287033.64, 287205.38, 287346.29, 287360.05, 287425.22, 287523.98, 288247.24, 288320.38, 288519.75, 288839.73, 288855.71, 289201.21, 289667.55, 289886.16, 290399.66, 290444.31, 290494.85, 290909.69, 290930.01, 291024.98, 291028.09, 291454.52, 291484.89, 291530.43, 291781.15, 291808.87, 291891.01, 292315.38, 292498.61, 292539.73, 292680.16, 292859.36, 293031.78, 293098.1, 293131.58, 293155.08, 293350.51, 293481.36, 293728.57, 293804.45, 293953.08, 293984.54, 294264.2, 294659.5, 294732.5, 294882.83, 295257.3, 295339.01, 295811.25, 295841.84, 295880.12, 295974.22, 296641.91, 296673.77, 296765.59, 296804.49, 296818.2, 296850.83, 296947.06, 297099.95, 297149.69, 297293.59, 297384.81, 297753.49, 298080.45, 298180.18, 298337.41, 298697.84, 298947.51, 299354.67, 299614.33, 299757.75, 299800.67, 300152.45, 300236.85, 300255.87, 300628.19, 301444.94, 301615.49, 301827.36, 301893.63, 302423.93, 302881.64, 303043.02, 303108.81, 303289.55, 303438.24, 303447.57, 303559.23, 303643.84, 303908.81, 303974.28, 304144.9, 304300.91, 304811.82, 304984.14, 304989.97, 305969.81, 305993.27, 306005.53, 306069.18, 306098.17, 306193.81, 306336.07, 306411.01, 306533.08, 306578.89, 306759.7, 306827.36, 306858.69, 307035.11, 307095.31, 307126.34, 307291.56, 307306.76, 307333.62, 307409.13, 307486.73, 307646.5, 307779.64, 307913.58, 307944.37, 308295.38, 308389.82, 308442.49, 308770.42, 308950.04, 309111.47, 309129.01, 309543.52, 310013.11, 310027.29, 310141.68, 310338.17, 310531.04, 310800.79, 310804.93, 310982.87, 311144.16, 311390.22, 311590.54, 311825.7, 311906.7, 312078.71, 312161.0, 312177.67, 312220.47, 312233.56, 312298.37, 312361.88, 312467.52, 312577.36, 312698.67, 313270.45, 313358.15, 313387.11, 313795.6, 314014.18, 314316.55, 314607.22, 314910.37, 315273.08, 315396.72, 315641.8, 315645.53, 316203.64, 316687.22, 316948.39, 317173.1, 317738.56, 317872.51, 318674.93, 319550.77, 319855.26, 320021.1, 320691.21, 320831.36, 321110.22, 321205.12, 321299.99, 322046.76, 322405.13, 322868.56, 322904.68, 323233.8, 323410.94, 323766.77, 323798.0, 323915.32, 324174.79, 324195.17, 324801.13, 324839.74, 325041.68, 325201.05, 325262.46, 325310.3, 325327.93, 325345.41, 325377.97, 325976.34, 325986.05, 326053.28, 326316.73, 326469.43, 326870.13, 327093.89, 327237.92, 327383.64, 327389.51, 328020.49, 328336.85, 328415.44, 328498.92, 328633.34, 329033.66, 329139.73, 329183.92, 329467.82, 329613.2, 329658.1, 330063.06, 330338.36, 330518.34, 330604.9, 330927.21, 331026.11, 331173.51, 331318.73, 331406.0, 331965.95, 332811.55, 332901.94, 333025.47, 333146.88, 333522.6, 333594.81, 333870.52, 333948.0, 334042.43, 334222.73, 334441.15, 335121.82, 335345.82, 335691.85, 335741.9, 335858.11, 336017.6, 336189.66, 336227.69, 336241.0, 336253.19, 336378.38, 336479.49, 337390.44, 337723.49, 337743.26, 337796.13, 337819.16, 337825.89, 337979.65, 337985.74, 338273.38, 338277.71, 338386.08, 338400.82, 338737.33, 339042.18, 339392.54, 339407.94, 339490.69, 339597.38, 339976.65, 340147.2, 340238.38, 340497.08, 340708.78, 341098.08, 341214.43, 341219.63, 341381.08, 341400.72, 341503.92, 341630.46, 341704.59, 341994.48, 342023.92, 342076.99, 342214.9, 342385.38, 342667.35, 343048.29, 343108.12, 343268.29, 343763.17, 343850.34, 344225.99, 344490.88, 344642.01, 344964.2, 345381.29, 345584.39, 346137.87, 346174.3, 347295.6, 347726.67, 347955.05, 348591.74, 348593.99, 348655.2, 348895.98, 349214.18, 349239.88, 349459.95, 349518.1, 349624.88, 350089.23, 350276.29, 350648.91, 350876.7, 351271.36, 351541.62, 351728.21, 351832.03, 351925.36, 352260.97, 352728.78, 352811.53, 352864.49, 352960.64, 353652.23, 354078.95, 354232.34, 354270.77, 354361.08, 354828.65, 354993.26, 355017.09, 355131.33, 355307.94, 355626.87, 355965.23, 356138.79, 356427.98, 356622.61, 356797.0, 356886.1, 357346.48, 357557.16, 357897.18, 358461.58, 358496.14, 358784.1, 359206.21, 359310.65, 359867.8, 359949.27, 360009.94, 360256.58, 360266.09, 360617.37, 360857.98, 360932.69, 361067.07, 361181.48, 361248.39, 361311.41, 362134.09, 362224.7, 362758.94, 362952.34, 363064.64, 364076.85, 364603.13, 364606.7, 364866.24, 365098.24, 365248.94, 365562.67, 365672.55, 365730.76, 365818.61, 366250.69, 366367.55, 366473.97, 366819.84, 366977.79, 367181.71, 367405.4, 367433.77, 367438.62, 367801.19, 368282.57, 368477.93, 368600.0, 368929.55, 368962.72, 369106.72, 369350.6, 369722.32, 370230.94, 370897.82, 372174.12, 372239.89, 372545.32, 372673.61, 373267.58, 373454.33, 373655.61, 373703.95, 374182.04, 374556.08, 374574.72, 375328.59, 375629.51, 375988.69, 376183.44, 376184.88, 376225.61, 376233.89, 376777.45, 377096.55, 377347.49, 377464.62, 377672.46, 378100.31, 378241.34, 378539.17, 378574.44, 379530.3, 379716.91, 380188.69, 380279.44, 380376.85, 380683.67, 380870.09, 381017.75, 381061.1, 381151.72, 381763.02, 382098.13, 382677.76, 382914.66, 383550.93, 383657.44, 384075.31, 384200.69, 384357.94, 384659.85, 385520.71, 385631.48, 385672.11, 386312.68, 386344.54, 386635.03, 387001.13, 387334.04, 387844.05, 387944.83, 389427.9, 389540.62, 389813.02, 390732.02, 391638.75, 391811.6, 391837.56, 391860.04, 392003.13, 392109.51, 392152.3, 392654.26, 393401.4, 393715.71, 394507.84, 394616.11, 394645.25, 394665.28, 394918.83, 394976.36, 395107.35, 395146.24, 395195.65, 395316.65, 395453.83, 395987.24, 396826.06, 396968.8, 397211.19, 397428.22, 397631.02, 397771.68, 398178.21, 398445.15, 398468.08, 398838.97, 399191.05, 399323.86, 399699.17, 400430.78, 401501.2, 401615.8, 401770.9, 402341.76, 402579.84, 402635.76, 402709.17, 402985.7, 403217.22, 403342.4, 403423.34, 404283.84, 404438.51, 404545.03, 404634.36, 404751.25, 404753.3, 405215.91, 405429.43, 405432.7, 405860.37, 405938.35, 406228.19, 407012.47, 407112.22, 407186.47, 407204.86, 407488.84, 407506.78, 407589.16, 407764.25, 408188.88, 408229.73, 408679.36, 408838.73, 408891.49, 409411.61, 409600.98, 409686.63, 409981.25, 410214.7, 410406.95, 410426.97, 410429.73, 410497.73, 410553.88, 410711.99, 410804.39, 411116.95, 411206.5, 411252.02, 411615.71, 411775.8, 411866.46, 412050.73, 412224.67, 412314.71, 412385.75, 412449.67, 412456.48, 412882.31, 413042.12, 413617.45, 413701.29, 413907.25, 414094.05, 414245.96, 414392.09, 414450.61, 414986.54, 415202.04, 415513.97, 415870.28, 416036.75, 416881.66, 416913.1, 416953.51, 417070.51, 417290.38, 417521.7, 417528.26, 417640.34, 418112.76, 418925.47, 419348.59, 419497.95, 419717.41, 419764.37, 419964.77, 419990.29, 420515.63, 420728.96, 420789.74, 421642.19, 422093.59, 422169.47, 422186.65, 422715.74, 422810.12, 422965.33, 423175.56, 423192.4, 423260.82, 423294.4, 423351.15, 423380.14, 423441.76, 423687.2, 423707.69, 423805.22, 424083.99, 424513.08, 424581.17, 424614.59, 424670.98, 424697.05, 424904.95, 424956.3, 424960.66, 425136.55, 425215.71, 425296.65, 425410.04, 425470.84, 425804.8, 425989.04, 426151.88, 426666.92, 426905.26, 426959.62, 427021.18, 427175.03, 427491.04, 427855.24, 428465.11, 428554.63, 428631.91, 428727.61, 428782.38, 428784.7, 428806.46, 428851.99, 428953.6, 428958.53, 429305.82, 429914.6, 430222.07, 430222.97, 430526.21, 430878.28, 430944.39, 431266.64, 431294.45, 431412.22, 431754.01, 431798.64, 431985.36, 432268.53, 432359.04, 432424.85, 432451.91, 432687.97, 432782.1, 432808.48, 433037.66, 433565.77, 433744.83, 434080.74, 434116.8, 434252.15, 434471.38, 434593.26, 434822.13, 434879.87, 435102.2, 435109.11, 435300.51, 435397.19, 435401.64, 435481.03, 435579.7, 435790.74, 435972.82, 436070.45, 436221.26, 436293.4, 436312.41, 436462.63, 436690.13, 436741.02, 436970.1, 437084.51, 437222.94, 437320.66, 437537.29, 437631.44, 437773.31, 437893.76, 437926.79, 437949.9, 438068.71, 438334.39, 438516.53, 438523.24, 438529.81, 438640.61, 438760.62, 438789.52, 439132.5, 439132.62, 439276.5, 439424.5, 439913.57, 440491.33, 440553.42, 441265.08, 441407.06, 441434.2, 441683.74, 442074.79, 442457.35, 442734.55, 442894.2, 443242.17, 443334.71, 443557.65, 443810.78, 443889.5, 443891.64, 444160.07, 444181.85, 444351.61, 444718.68, 444756.37, 445134.15, 445162.05, 445332.28, 445358.73, 445393.74, 445475.3, 445530.16, 445736.36, 446336.8, 446516.26, 446617.89, 446751.45, 446863.31, 446905.02, 447023.91, 447050.42, 447139.8, 447519.44, 447727.52, 448110.25, 448391.99, 448392.17, 448998.73, 449355.91, 449516.29, 449603.91, 450075.31, 450337.47, 450756.71, 451077.21, 451327.61, 451365.99, 452021.2, 452163.93, 452169.28, 452792.53, 452905.22, 453016.91, 453210.24, 453308.15, 453632.76, 453943.89, 453979.19, 454021.69, 454183.42, 454412.28, 454477.54, 454694.21, 454769.68, 454800.96, 455162.92, 455751.84, 455952.18, 456140.34, 457030.86, 457216.87, 457340.06, 457504.35, 457711.68, 457883.94, 457884.06, 457899.64, 458086.69, 458343.7, 458479.01, 458562.24, 458634.68, 459443.22, 459631.74, 459756.11, 459770.85, 460020.74, 460331.7, 460397.41, 460945.14, 461511.92, 461548.98, 461622.22, 461868.09, 462058.19, 462080.47, 462454.39, 462474.16, 462676.5, 462732.36, 462888.13, 462909.41, 463370.48, 463448.59, 463513.26, 463561.48, 463752.89, 463977.54, 464189.09, 465108.52, 465198.89, 465279.68, 465338.41, 465408.72, 465493.15, 465992.02, 465993.51, 466010.25, 466045.63, 466322.76, 466594.89, 466806.89, 466962.04, 467442.94, 467546.74, 467642.03, 467711.18, 467827.75, 468189.93, 468428.3, 468675.19, 468772.8, 469311.17, 469563.7, 469598.57, 469607.73, 469752.56, 469773.85, 469787.38, 469868.7, 470281.03, 470365.59, 470436.8, 470921.24, 471054.16, 471086.22, 471088.88, 471115.38, 471281.68, 471311.5, 471449.98, 471713.59, 472044.28, 472330.71, 472450.81, 472511.32, 472591.07, 473292.47, 473766.97, 473773.27, 474030.51, 474129.35, 474389.75, 474653.06, 474698.01, 474917.98, 474964.6, 475127.18, 475158.24, 475201.64, 475591.08, 475615.26, 475696.37, 475770.14, 475776.45, 475905.1, 476420.77, 476733.74, 476967.65, 477409.3, 477615.87, 478021.68, 478483.2, 478503.06, 478773.05, 479195.02, 479256.8, 479263.15, 479424.2, 479430.0, 479855.0, 480130.04, 480203.43, 480239.88, 480353.64, 480452.1, 480512.44, 481119.6, 481144.09, 481523.93, 481897.98, 481915.11, 482451.21, 482528.36, 482926.93, 483011.69, 483177.2, 483660.15, 483699.56, 484032.75, 484097.03, 484263.25, 484327.56, 484588.34, 484661.87, 484829.07, 484835.2, 484946.56, 485095.41, 485150.01, 485389.15, 485419.39, 485694.72, 485744.61, 485764.32, 486930.72, 487311.03, 487912.95, 488008.83, 488362.61, 488417.61, 488683.57, 488782.63, 488880.26, 489059.93, 489079.23, 489293.72, 489372.02, 489408.53, 489674.23, 490274.82, 490503.69, 490970.95, 490981.78, 491115.86, 491179.79, 491290.37, 491419.55, 491449.94, 491510.58, 491723.42, 491755.69, 491815.83, 491817.19, 491998.12, 492022.68, 492262.96, 492364.77, 492721.85, 492792.8, 492932.51, 493078.64, 493159.35, 493430.45, 493503.89, 493506.47, 493653.43, 494069.49, 494145.8, 494265.48, 495022.51, 495269.0, 495543.28, 495692.19, 495720.3, 495951.0, 495951.66, 496010.17, 496083.24, 496725.44, 496851.6, 497085.91, 497250.22, 497373.49, 497374.57, 497488.4, 498056.0, 498159.39, 498241.06, 498292.53, 498580.87, 498749.62, 498925.86, 499081.79, 499267.66, 499325.38, 500250.8, 500274.03, 500381.23, 500552.16, 500801.72, 500945.63, 500964.59, 501013.47, 501251.4, 501268.78, 501780.66, 501959.19, 502021.82, 502456.04, 502504.39, 502662.07, 502918.18, 503232.13, 503295.29, 503463.93, 503486.37, 503720.98, 503744.56, 504126.89, 504307.35, 504403.16, 504566.28, 504601.29, 504750.35, 504754.74, 504760.57, 504856.97, 504963.84, 504974.95, 505068.22, 505069.21, 505196.08, 505221.17, 505246.15, 505304.33, 505329.66, 505405.85, 505406.72, 505543.53, 505610.4, 505761.62, 505830.56, 505907.41, 505918.21, 505978.46, 506005.47, 506273.74, 506274.94, 506343.83, 506502.09, 506705.36, 506743.78, 506760.54, 506789.66, 506897.98, 506973.17, 507086.75, 507168.8, 507297.88, 507335.75, 507403.77, 507584.29, 507900.07, 508174.55, 508213.14, 508309.81, 508432.17, 508520.09, 508567.04, 508573.16, 508576.62, 508794.87, 508801.61, 509100.84, 509183.22, 509263.28, 509276.22, 509633.71, 509640.77, 509647.25, 509872.77, 509942.56, 510243.79, 510296.07, 510382.5, 510425.4, 510427.53, 510494.7, 510747.62, 510787.46, 511049.06, 511059.95, 511207.52, 511316.29, 511327.9, 511330.32, 511871.63, 511883.36, 512157.25, 512260.59, 512292.01, 512834.04, 513015.35, 513073.87, 513107.2, 513181.31, 513327.55, 513341.94, 513372.17, 513409.67, 513615.82, 513636.01, 513672.36, 513737.0, 513991.57, 514116.58, 514485.9, 514489.17, 514651.74, 514709.76, 514731.6, 514756.08, 514758.19, 514993.0, 515119.64, 515226.11, 515599.7, 515777.97, 516087.65, 516295.81, 516352.21, 516361.06, 516402.1, 516424.83, 516556.94, 516909.24, 517021.3, 517355.44, 517408.48, 517420.23, 517546.69, 517783.5, 517850.83, 517869.97, 518045.09, 518124.16, 518220.72, 518245.97, 518266.9, 518628.42, 518696.89, 518940.88, 519042.49, 519255.68, 519354.88, 519498.32, 519585.67, 519787.93, 519823.3, 519914.1, 520284.79, 520493.83, 520632.8, 520846.68, 520850.71, 520887.23, 520962.14, 521002.97, 521297.31, 521320.98, 521516.96, 521539.46, 521810.75, 521896.6, 521953.78, 521959.28, 522049.52, 522105.93, 522296.71, 522467.51, 522514.32, 522554.04, 522665.04, 522673.62, 522715.68, 522784.33, 522815.45, 522816.85, 523427.35, 523483.19, 523831.64, 524104.92, 524316.21, 524450.7, 524483.65, 524544.83, 524559.95, 524658.06, 525132.36, 525200.59, 525545.76, 525559.17, 525616.9, 525866.36, 526128.61, 526434.37, 526486.82, 526525.16, 526625.49, 526641.23, 526838.14, 526904.08, 527019.78, 527041.46, 527058.59, 527117.81, 527165.7, 527339.52, 527389.28, 527402.62, 527495.09, 527509.76, 527572.25, 527947.21, 527953.14, 527983.04, 528420.28, 528486.45, 528784.86, 528807.45, 528832.54, 528940.78, 529384.31, 529418.64, 529515.66, 529672.95, 529707.87, 529852.7, 529877.93, 530059.06, 530318.39, 530367.83, 530492.84, 530842.25, 531080.31, 531600.62, 531640.19, 531811.85, 532226.2, 532241.22, 532311.93, 532739.77, 532765.05, 533161.64, 533414.62, 533564.42, 533734.94, 533756.88, 533887.54, 533905.67, 533917.52, 534285.21, 534372.53, 534578.78, 534597.69, 534738.43, 534740.3, 534780.57, 534847.96, 534970.68, 535087.91, 535153.47, 535311.64, 535537.03, 535587.31, 535764.58, 535769.32, 535937.25, 535983.13, 536006.73, 536031.67, 536144.81, 536537.64, 536840.69, 536871.58, 536914.17, 537035.28, 537064.03, 537124.76, 537138.58, 537224.52, 537276.41, 537300.94, 537455.65, 537518.57, 538344.1, 538634.46, 538713.47, 538745.93, 538978.67, 539081.09, 539126.0, 539337.87, 539683.42, 539826.56, 540031.29, 540149.85, 540189.7, 540625.79, 540716.58, 540811.85, 540819.44, 540922.94, 541037.98, 541071.29, 541120.2, 541216.92, 541292.64, 541406.98, 542009.46, 542016.18, 542087.89, 542295.37, 542399.07, 542414.27, 542464.02, 542529.21, 542556.05, 542663.53, 542819.03, 543234.77, 543384.01, 543703.16, 543706.04, 543754.17, 543757.97, 543775.87, 543894.07, 544408.14, 544612.28, 544643.33, 544770.7, 545052.34, 545109.3, 545120.67, 545206.32, 545251.1, 545368.17, 545570.86, 545766.13, 545840.05, 545844.91, 546122.37, 546221.4, 546554.96, 546675.65, 546690.84, 547226.0, 547384.9, 547513.3, 547564.09, 547586.07, 548527.49, 548542.47, 549502.11, 549505.55, 549528.16, 549551.02, 549665.67, 549731.49, 549788.36, 549967.89, 550076.32, 550373.57, 550387.78, 550414.99, 550735.64, 550791.32, 551058.13, 551152.15, 551221.21, 551378.39, 551553.99, 551743.05, 551799.63, 551837.31, 551969.1, 552338.76, 552529.23, 552677.48, 552811.62, 552985.34, 553714.87, 553834.04, 553836.98, 553901.97, 554036.84, 554093.15, 554309.24, 554630.42, 554879.67, 554972.42, 555075.27, 555183.72, 555279.02, 555424.24, 555652.77, 555925.6, 555954.13, 556015.59, 556046.12, 556485.1, 556550.85, 556925.19, 557075.21, 557166.35, 557312.01, 557543.62, 557547.25, 558027.77, 558143.53, 558239.32, 558343.57, 558431.44, 558464.8, 558473.6, 558585.13, 558671.14, 558691.43, 558731.74, 558794.63, 558837.27, 558963.83, 558981.45, 559061.58, 559285.35, 559606.91, 559903.13, 560104.16, 560764.41, 561137.06, 561145.14, 561226.38, 561573.08, 561625.92, 562173.12, 562558.27, 562633.67, 563449.43, 563460.77, 563578.79, 563884.47, 564304.15, 564345.55, 564536.01, 564538.07, 564606.1, 564848.78, 564883.2, 565297.54, 565390.4, 565481.88, 565567.84, 565812.29, 566945.95, 567114.6, 567673.87, 568093.57, 568497.35, 569005.06, 569105.03, 569937.23, 570045.79, 570069.48, 570103.64, 570162.28, 570231.21, 570611.23, 570791.11, 570816.34, 570879.04, 571190.83, 571463.93, 572360.83, 572387.47, 572447.52, 572516.57, 572603.33, 572887.78, 573084.71, 573374.49, 573498.64, 573545.96, 574450.23, 574622.56, 574798.86, 574955.95, 574985.37, 575317.38, 575570.77, 575676.13, 575709.96, 575997.78, 576252.35, 576332.05, 576620.31, 576809.92, 576837.09, 576879.15, 577011.26, 577198.97, 577511.02, 577627.66, 577698.37, 577792.32, 577868.38, 578002.85, 578164.82, 578209.63, 578539.86, 578790.36, 578832.41, 579068.88, 579166.5, 579272.38, 579499.93, 579539.95, 579544.21, 579738.2, 579874.22, 580453.32, 580805.33, 581010.52, 581473.55, 581546.23, 581745.72, 581758.22, 581854.5, 582353.17, 582381.95, 582525.42, 582552.26, 582846.22, 582864.35, 583079.97, 583210.87, 583322.2, 583364.02, 583648.59, 583835.18, 584000.71, 585028.26, 585175.24, 585548.79, 585895.34, 585989.1, 586061.46, 586108.13, 586289.08, 586467.16, 586583.69, 586737.66, 586781.78, 586841.77, 586886.16, 586936.45, 587004.29, 587259.82, 587370.81, 587603.55, 587685.38, 588017.66, 588344.18, 588363.62, 588448.21, 588592.61, 588722.99, 589091.04, 589252.69, 589467.35, 589554.29, 589842.69, 590453.63, 590636.38, 590739.62, 590836.37, 590984.56, 591335.5, 591603.79, 591703.82, 591907.88, 592111.49, 592340.01, 592355.24, 592369.22, 592572.3, 592947.75, 592981.33, 593128.13, 593138.59, 593141.29, 593162.53, 593234.27, 593462.34, 593710.67, 593875.46, 593941.9, 594224.9, 594385.2, 594574.12, 594625.96, 594744.89, 595421.23, 595626.56, 596218.24, 596554.05, 597354.39, 597406.39, 597644.02, 597667.21, 597855.07, 597856.51, 597876.55, 598185.4, 598234.64, 598251.57, 598301.5, 598437.98, 598495.02, 598502.83, 598553.43, 598679.02, 599488.98, 599629.25, 599730.07, 599759.45, 599828.39, 600050.98, 600395.73, 600448.69, 600952.06, 601004.79, 601990.02, 603024.75, 603041.14, 603147.26, 603318.89, 603393.64, 603460.79, 603547.16, 603618.89, 604173.59, 604925.08, 605078.62, 605325.43, 605413.17, 605791.46, 605956.59, 605960.2, 605990.41, 606210.77, 606309.13, 606671.5, 606755.3, 607218.6, 607294.56, 607475.44, 607593.51, 607691.36, 607819.33, 607961.21, 608200.81, 608294.98, 608390.94, 608737.56, 609099.37, 609274.89, 609736.12, 610076.32, 610185.32, 610641.42, 610940.94, 610985.56, 610991.35, 611258.71, 611390.67, 611464.21, 611585.54, 611796.61, 612337.35, 612379.9, 612987.64, 613042.97, 613115.21, 613135.23, 613270.79, 613501.05, 613531.11, 613899.15, 614253.33, 614378.94, 614764.31, 614940.07, 615026.15, 615121.78, 615666.78, 615761.77, 615975.5, 615997.29, 616094.72, 616324.24, 616345.25, 616598.1, 616701.99, 617207.58, 617405.35, 617898.07, 618121.82, 618377.79, 618702.09, 618702.79, 618767.26, 618877.13, 618949.82, 619133.48, 619224.06, 619225.65, 619337.29, 619369.72, 619498.28, 619639.74, 620087.35, 620735.72, 620885.93, 620908.18, 621099.95, 621425.98, 622112.23, 622437.08, 623092.54, 623097.93, 623258.4, 623919.23, 624081.64, 624099.48, 624114.56, 625135.11, 625196.14, 625196.94, 626627.77, 627634.04, 628063.88, 628099.08, 628115.61, 628218.22, 628494.63, 628516.57, 628720.46, 628989.88, 629026.75, 629152.06, 629176.71, 629994.47, 630315.76, 630327.28, 630482.91, 630522.67, 630740.11, 630972.15, 631181.25, 631827.38, 633203.69, 633240.53, 633289.78, 633520.34, 633826.55, 634605.77, 634634.66, 634637.03, 634815.1, 635014.69, 635118.48, 635171.05, 635650.98, 635758.03, 635862.55, 636193.24, 636372.37, 636419.12, 636677.67, 636737.65, 637090.44, 637964.2, 638144.98, 638204.27, 638280.67, 638647.21, 638957.35, 639123.45, 639160.24, 639651.24, 639830.45, 639928.85, 640043.97, 640159.04, 640181.86, 640210.85, 640912.18, 641208.65, 641368.14, 641763.53, 641905.37, 641965.2, 642450.4, 642614.89, 642671.48, 642678.53, 642679.81, 642748.21, 642776.4, 642827.29, 642828.62, 642963.75, 643032.51, 643041.71, 643097.6, 643125.29, 643155.89, 643558.78, 643603.69, 643711.53, 643854.17, 644285.33, 645156.21, 645386.94, 645618.59, 646915.47, 647029.28, 647097.65, 648330.18, 648541.81, 648606.13, 648652.01, 649035.55, 649111.23, 649128.23, 649148.74, 649159.68, 649245.0, 649289.75, 649791.15, 649878.29, 649945.54, 649993.5, 650102.8, 650263.95, 650418.75, 650789.13, 651147.83, 651178.2, 651521.77, 651768.91, 651837.77, 651970.48, 652122.44, 652312.11, 652329.53, 653043.44, 653382.62, 653468.75, 653525.84, 653845.45, 653989.65, 654018.95, 654088.02, 655157.32, 655284.69, 655318.26, 655811.95, 656594.95, 656637.63, 656988.64, 657108.77, 657241.63, 658468.27, 658600.05, 658640.14, 658691.56, 658965.05, 658984.38, 658997.55, 659109.53, 659446.55, 659795.84, 659816.15, 659902.07, 659950.36, 660619.99, 660632.05, 660838.75, 661163.94, 661348.88, 661566.48, 661644.19, 661707.02, 662198.65, 663396.32, 663452.46, 663529.64, 663814.18, 663941.73, 663971.26, 664348.2, 664745.2, 665007.08, 665290.93, 665687.92, 665750.06, 665781.74, 665861.06, 666942.02, 667130.48, 667151.46, 667353.79, 668098.49, 668132.36, 668390.82, 669205.73, 669850.04, 669965.22, 670670.46, 670854.96, 670993.01, 671379.44, 671482.9, 671522.87, 671688.06, 671708.09, 672062.08, 672194.03, 672831.78, 672903.23, 673248.48, 674055.81, 674283.86, 674458.03, 674562.45, 674669.16, 674829.58, 674919.45, 675202.87, 675282.2, 675926.3, 676260.67, 676290.46, 676615.53, 677158.39, 677231.63, 677789.14, 677885.99, 677971.33, 678024.75, 678228.58, 679156.2, 679481.9, 679706.01, 680254.35, 680510.23, 680532.69, 680579.49, 680725.43, 680943.03, 680954.81, 681913.29, 682012.53, 682124.34, 682447.1, 682918.99, 683300.84, 683665.37, 684023.95, 684340.86, 684348.92, 684655.91, 684783.15, 685243.2, 685531.85, 685676.58, 685700.08, 686072.39, 686345.69, 686365.4, 686497.53, 687085.6, 687344.68, 687670.78, 688043.96, 688940.94, 688958.75, 689326.91, 690007.76, 690675.5, 690851.59, 691200.33, 691497.62, 691498.6, 693013.59, 693058.34, 693192.5, 693249.98, 693412.05, 693780.42, 693785.85, 694150.89, 694531.72, 694765.95, 695392.84, 695396.19, 695439.83, 696314.53, 696421.72, 696687.6, 696844.36, 697317.41, 697645.32, 698073.95, 698529.64, 698536.06, 698986.34, 699028.66, 699270.1, 699464.43, 699536.73, 699779.0, 699852.68, 700009.77, 700272.01, 700392.21, 700554.16, 702238.27, 704113.22, 704335.41, 704344.21, 704680.97, 705557.8, 705557.97, 705802.45, 706206.86, 706924.02, 707895.72, 708568.29, 709337.11, 709724.6, 710496.97, 710668.45, 711367.56, 711461.95, 711470.8, 711571.88, 711671.58, 712312.89, 712362.72, 712422.4, 712425.76, 712647.97, 712869.09, 713117.66, 713173.95, 713250.08, 713332.54, 713479.91, 713834.74, 714014.73, 714081.05, 714093.95, 714106.42, 714601.11, 714677.47, 714828.73, 715263.3, 715311.6, 715876.27, 716026.51, 716341.39, 716388.81, 716859.27, 716987.58, 717207.19, 717373.43, 718056.73, 718125.53, 718232.26, 718373.94, 718393.61, 718470.71, 718748.33, 718780.59, 718890.81, 718898.33, 719235.07, 719591.13, 719737.25, 719888.76, 720242.19, 720946.99, 721212.45, 721460.22, 721601.9, 721744.33, 722120.04, 722262.21, 722496.93, 722891.24, 723086.2, 723559.88, 723708.99, 723987.85, 724180.89, 724443.97, 724468.97, 724499.81, 724798.76, 725043.04, 725729.51, 726422.55, 726482.39, 726997.84, 727049.04, 727070.0, 727163.67, 727772.8, 728311.15, 728467.72, 728525.6, 729036.06, 729572.08, 729759.97, 730254.19, 730899.37, 731756.65, 732056.37, 732859.76, 733037.32, 733455.07, 733564.77, 733716.78, 734099.4, 734297.87, 734464.36, 734858.91, 735796.38, 735870.0, 736335.69, 736805.66, 737014.09, 737163.2, 737265.57, 737551.74, 737569.14, 737613.65, 738792.11, 738812.0, 739279.19, 739866.16, 740259.63, 741625.25, 741891.65, 744389.81, 744525.69, 744772.88, 744782.89, 744836.56, 744969.42, 745782.1, 746129.56, 746517.32, 746901.03, 747099.07, 747444.32, 747577.05, 747888.25, 748435.2, 749549.55, 749597.24, 749676.95, 749779.1, 749817.08, 750182.71, 751167.12, 751181.4, 751963.81, 752034.52, 753060.78, 753361.72, 753385.55, 753447.05, 753664.12, 753711.32, 753860.89, 754134.95, 754236.7, 755084.4, 755098.41, 755214.26, 755804.37, 756288.89, 756527.64, 756588.42, 757330.95, 757369.87, 757738.76, 758069.78, 758182.2, 758510.36, 759407.87, 759442.33, 759995.18, 760281.43, 760671.1, 761793.94, 761880.36, 761956.58, 762184.1, 762620.94, 762861.78, 763479.9, 763867.59, 763961.82, 764014.06, 764014.75, 764155.44, 764385.4, 764565.55, 765270.02, 765648.93, 765687.42, 765823.48, 765996.92, 766456.0, 766512.66, 766571.1, 767256.53, 767338.32, 767358.37, 768070.53, 768313.85, 768390.05, 768718.11, 769319.04, 769522.33, 769848.75, 770157.29, 770228.02, 770487.37, 770652.79, 770820.27, 771065.21, 771298.98, 771315.62, 771686.4, 771908.51, 772036.6, 772302.94, 772539.12, 772859.25, 773367.71, 773586.49, 773603.77, 773725.08, 773819.49, 773878.58, 774262.28, 775910.43, 776661.74, 776838.56, 776933.37, 776968.87, 777175.28, 777207.3, 777254.06, 777951.22, 778178.53, 778436.81, 778672.64, 779602.36, 780444.94, 780607.52, 781267.76, 781694.57, 781970.6, 782221.96, 782252.57, 782256.66, 782300.68, 782563.38, 782796.01, 783250.75, 783300.05, 783371.02, 783614.89, 783929.7, 784305.55, 784490.67, 784639.12, 785104.29, 785515.88, 786459.23, 786561.61, 787062.0, 787295.09, 788340.23, 788633.42, 789036.02, 790144.7, 791356.9, 791495.25, 791637.53, 791835.37, 792299.15, 792442.54, 793045.82, 793097.64, 793184.25, 793569.0, 793589.18, 793889.1, 794397.89, 794660.24, 794698.77, 795133.0, 795157.2, 795301.17, 795859.23, 796277.72, 796351.35, 797523.04, 798376.99, 798593.88, 799369.15, 800147.84, 800662.82, 800714.0, 800819.79, 800842.28, 801098.43, 801302.01, 802003.61, 802105.5, 802253.41, 802383.63, 802583.89, 803657.12, 804105.49, 804362.36, 805028.74, 805642.61, 806012.48, 806444.29, 806979.15, 807082.19, 807535.52, 807798.73, 808030.15, 808624.82, 808821.5, 809049.37, 809272.65, 809321.44, 809646.66, 809833.21, 810150.64, 811153.67, 811318.3, 811328.4, 811365.42, 811824.06, 811855.72, 812011.8, 812190.76, 812323.29, 813211.46, 813352.41, 813486.55, 813630.44, 813756.09, 813845.5, 813954.82, 814099.86, 814395.17, 814753.5, 814919.11, 815093.76, 815130.5, 815915.52, 816138.33, 816603.05, 816838.31, 817485.14, 817653.25, 817661.76, 817741.17, 818333.21, 818434.49, 819196.68, 819911.89, 820059.89, 820188.42, 820288.35, 820964.1, 821127.53, 821498.18, 821568.64, 822167.17, 822486.37, 822569.16, 822668.23, 823220.43, 824568.39, 825283.43, 825584.22, 825763.48, 826155.95, 826546.96, 826626.5, 826820.71, 827705.82, 827717.85, 827738.06, 827968.36, 828594.86, 829207.27, 829210.73, 829284.67, 829902.42, 830601.39, 830756.76, 831425.2, 831443.61, 833429.22, 833517.19, 833782.7, 833979.01, 834373.73, 834621.39, 834663.52, 835181.18, 835189.26, 836049.89, 836305.65, 836419.14, 836707.85, 836717.75, 836915.92, 837144.63, 837390.79, 837548.62, 838227.52, 838297.32, 838751.5, 839911.0, 841129.26, 841224.74, 841264.04, 841778.34, 841889.08, 841951.91, 843361.1, 843755.12, 843848.65, 843864.43, 844373.31, 844443.35, 844490.86, 844928.17, 844958.49, 845252.21, 845289.77, 845715.37, 846850.35, 847246.5, 847294.04, 847348.08, 847380.07, 847592.11, 848289.41, 848358.09, 848521.17, 848630.57, 848873.28, 849074.04, 849231.61, 849397.57, 849540.85, 849779.14, 849925.37, 850440.26, 850448.54, 850538.25, 850708.6, 850936.26, 851461.9, 851762.28, 851919.34, 852333.75, 852452.93, 852876.29, 852882.61, 853073.17, 853136.46, 855001.02, 855014.77, 855046.95, 855130.21, 855385.01, 855421.39, 855437.57, 855459.96, 855546.5, 855882.57, 855922.64, 856178.47, 856796.1, 857796.45, 857797.33, 857811.17, 857883.46, 857939.41, 858572.22, 858853.75, 859258.17, 859922.19, 860224.98, 860255.58, 860293.46, 860336.16, 860962.91, 861894.77, 861941.25, 861965.12, 862419.84, 863188.26, 863266.12, 863917.41, 864852.85, 864881.24, 865137.6, 865305.88, 865467.86, 865709.11, 865924.2, 866064.4, 866184.92, 866216.36, 866401.45, 866566.54, 867283.25, 867539.07, 867919.21, 868041.56, 868191.05, 868636.3, 869403.63, 869922.56, 870349.86, 870415.49, 870625.49, 870676.44, 870914.69, 870971.82, 871024.26, 871264.25, 871404.6, 871692.74, 871847.85, 871945.64, 872113.23, 872288.46, 872450.37, 872469.03, 872817.62, 873065.23, 873119.06, 873337.84, 873347.55, 873354.58, 873415.01, 873450.29, 873576.96, 873643.14, 873954.7, 874223.25, 874446.32, 874453.88, 874615.32, 874790.68, 875038.84, 875372.91, 875699.81, 875976.83, 876583.98, 876591.41, 876712.31, 876902.87, 877055.32, 877235.96, 877268.29, 877380.22, 877423.45, 877724.31, 877813.33, 877996.27, 878298.22, 878314.57, 878762.3, 879244.9, 879448.25, 880165.7, 880415.67, 880576.33, 880742.35, 881190.46, 881503.95, 881930.87, 882132.28, 882180.91, 882636.96, 882917.12, 883569.38, 883683.35, 884042.63, 884233.67, 884701.92, 884724.41, 885445.47, 885572.96, 885608.04, 885613.91, 885892.37, 887426.12, 887907.01, 887979.47, 888203.69, 888368.8, 888703.62, 888816.78, 888834.07, 888869.27, 889290.23, 889670.29, 890130.25, 890488.01, 890547.07, 890661.79, 890689.51, 891025.39, 891148.55, 891152.33, 891154.18, 891387.14, 891671.44, 891736.91, 892056.64, 892070.82, 892133.41, 892393.77, 893399.77, 893504.87, 893613.0, 893995.44, 894280.19, 894493.7, 894686.67, 894865.3, 895020.48, 895066.5, 895069.88, 895157.44, 895274.72, 895358.2, 895535.31, 895763.41, 895800.07, 895973.02, 896295.41, 896613.19, 896979.93, 897027.44, 897032.19, 897037.25, 897076.73, 897309.41, 897747.13, 898289.14, 898610.33, 899036.47, 899044.34, 899352.4, 899449.65, 899479.43, 899761.48, 899768.4, 899834.75, 900309.75, 900387.29, 900646.94, 901709.82, 901972.7, 902050.95, 902109.69, 902727.76, 902779.25, 902852.73, 903119.03, 903366.55, 903606.03, 903864.02, 903882.96, 904261.65, 904503.85, 904650.55, 905324.68, 905399.99, 905548.38, 905756.13, 905935.29, 905984.49, 905987.17, 907110.83, 907262.47, 907493.24, 908278.74, 908853.15, 909989.45, 910110.24, 910240.68, 910298.44, 910899.05, 911106.22, 911210.81, 911245.43, 911538.98, 911696.0, 911788.79, 911807.02, 911969.0, 912403.67, 912542.82, 912762.76, 912857.1, 912958.95, 913165.19, 913236.62, 913548.25, 913616.32, 913755.12, 913922.01, 914500.91, 914759.2, 914835.86, 915064.22, 916033.92, 916289.2, 916402.76, 916446.02, 916522.66, 916820.96, 916918.7, 916967.92, 917088.48, 917317.15, 917693.06, 917883.17, 917883.79, 917924.47, 918006.9, 918049.28, 918170.5, 918285.97, 918295.79, 918335.68, 919229.36, 919301.81, 919503.4, 919595.44, 919839.19, 919878.34, 920128.89, 920719.98, 921161.2, 921178.39, 921247.88, 921264.52, 921612.53, 921700.61, 922018.43, 922048.41, 922231.92, 922328.02, 922341.82, 922440.64, 922539.94, 922735.37, 922850.57, 922898.38, 923221.52, 923344.54, 923473.7, 923600.02, 923644.6, 923795.04, 924011.76, 924134.99, 924174.4, 924299.78, 924506.26, 925731.21, 925916.65, 926133.8, 926250.21, 926294.02, 926427.12, 926455.64, 926573.81, 926934.57, 927084.65, 927249.61, 927266.34, 927511.99, 927600.01, 927610.69, 927657.63, 927732.02, 928264.4, 928470.83, 928537.54, 928629.31, 928820.0, 929096.9, 929222.16, 929690.71, 929976.55, 930121.14, 930269.79, 930506.14, 930745.69, 931278.97, 931710.67, 931939.52, 932160.37, 932195.52, 932240.96, 932397.0, 932679.79, 933487.71, 933528.39, 933924.44, 933960.3, 934917.47, 935266.43, 935481.32, 936001.98, 936205.5, 936293.6, 936373.65, 936508.43, 936751.68, 937232.09, 937420.65, 937473.13, 937522.77, 937956.89, 938083.17, 938149.21, 938303.28, 938334.62, 938604.58, 938861.77, 938914.28, 939118.24, 939158.25, 939367.14, 940299.87, 940405.03, 940554.34, 941008.85, 941311.83, 941457.34, 941550.34, 941612.04, 941675.95, 941829.0, 942236.45, 942319.65, 942475.24, 942868.38, 942970.63, 943047.78, 943124.74, 943237.12, 943465.29, 943506.28, 943717.38, 943891.64, 943912.77, 943951.67, 944100.3, 944337.32, 944523.3, 944587.23, 944594.78, 944698.7, 944958.69, 945018.83, 945143.33, 945267.68, 945318.47, 945643.17, 945823.65, 945889.59, 946060.98, 946091.79, 946573.29, 946614.55, 947229.24, 947552.44, 947753.32, 947815.05, 948447.34, 948613.39, 948660.79, 948964.99, 948977.5, 949075.87, 949625.52, 949825.83, 950154.24, 950684.2, 950691.96, 950862.92, 950929.59, 951208.65, 951244.66, 951549.61, 951569.84, 951588.37, 951957.31, 952264.91, 952609.17, 952766.93, 953252.14, 953314.16, 953331.45, 953393.02, 953495.48, 953533.95, 953693.23, 953844.85, 954069.45, 954107.32, 954148.64, 954220.22, 954233.87, 954396.85, 954401.46, 954576.86, 954677.75, 954681.56, 955146.04, 955211.7, 955294.7, 955338.29, 955451.16, 955463.84, 955466.84, 955506.95, 955641.74, 955766.33, 955913.68, 956211.2, 956228.96, 956251.18, 956987.81, 957155.31, 957298.26, 957356.84, 957997.52, 958007.69, 958063.87, 958225.41, 958374.56, 958487.75, 958619.8, 958667.23, 958875.37, 959229.09, 959339.51, 960115.56, 960312.75, 960418.54, 960476.1, 960746.04, 960945.43, 960998.52, 961084.08, 961186.23, 961685.98, 961953.57, 961993.34, 962475.55, 963382.09, 963516.28, 963910.81, 963960.37, 964169.67, 964332.51, 964356.74, 964683.8, 964726.37, 964729.18, 965056.4, 965512.36, 965769.93, 965788.76, 965853.58, 966054.97, 966145.09, 966187.51, 966232.69, 966780.01, 966817.24, 967187.37, 967304.07, 967310.82, 967576.95, 967729.35, 968258.09, 968270.66, 968502.38, 968694.45, 968816.33, 968896.68, 969046.69, 969387.48, 969594.47, 969611.3, 970224.51, 970307.83, 970328.68, 970641.34, 970773.64, 971193.01, 971361.38, 971386.65, 971422.67, 971557.62, 971615.62, 971932.87, 972088.34, 972256.98, 972292.31, 972373.81, 972405.38, 972663.59, 972716.24, 972834.42, 973004.91, 973105.3, 973250.41, 973585.33, 973812.79, 974114.39, 974123.2, 974697.6, 974866.65, 974907.28, 974916.13, 975268.91, 975479.83, 975500.87, 975578.02, 975964.86, 976137.73, 976242.09, 976393.43, 976415.56, 976436.02, 976453.34, 976479.51, 976522.93, 977033.5, 977062.44, 977070.62, 977103.64, 977286.07, 977322.52, 977628.78, 977683.06, 977790.83, 977950.28, 978027.95, 978082.84, 979428.66, 979552.34, 979730.78, 979825.92, 979848.71, 979977.89, 980642.1, 981210.57, 981273.26, 981345.2, 981386.25, 981567.22, 981615.81, 981646.46, 981978.02, 982322.24, 982345.51, 982523.26, 982598.88, 982661.14, 983232.96, 983825.15, 983963.07, 984336.04, 984689.9, 984833.35, 984881.59, 985152.94, 985229.81, 985479.64, 985594.23, 985896.44, 986131.94, 986504.93, 986601.46, 986612.02, 986765.01, 986922.62, 987264.67, 987353.65, 987435.35, 987886.08, 987990.24, 988157.72, 988392.99, 988467.61, 988712.52, 988742.08, 988764.84, 988950.75, 990152.28, 990263.7, 990926.38, 990951.77, 991054.49, 991104.4, 991127.01, 991262.46, 991514.21, 991570.02, 991779.2, 991941.73, 991969.37, 992621.93, 992774.4, 993097.43, 993172.98, 993311.59, 993436.67, 993833.44, 994610.43, 994610.99, 994801.4, 994966.1, 996147.39, 996628.8, 996723.58, 996937.95, 997282.75, 997353.15, 997474.93, 997502.47, 997672.62, 997868.63, 997998.21, 998362.05, 998443.5, 998672.85, 999298.43, 999348.55, 999511.29, 999785.48, 1000285.1, 1000342.87, 1000582.06, 1000968.67, 1001069.52, 1001286.67, 1001512.21, 1001558.74, 1001790.16, 1001943.8, 1002364.34, 1002714.25, 1002806.39, 1002856.2, 1003202.66, 1004039.84, 1004137.09, 1004252.38, 1004434.54, 1004516.46, 1004523.59, 1004730.69, 1004749.41, 1005003.12, 1005083.31, 1005324.28, 1005360.5, 1005448.76, 1005463.49, 1005669.58, 1005983.31, 1006486.96, 1006597.69, 1006814.85, 1006888.16, 1007257.83, 1007385.36, 1007574.67, 1007579.44, 1007867.68, 1007906.43, 1008483.07, 1008557.04, 1009121.2, 1009201.24, 1009206.33, 1009502.01, 1009797.06, 1009887.36, 1010326.14, 1010562.49, 1010711.08, 1011201.12, 1011321.18, 1011822.3, 1011938.29, 1012075.12, 1012498.49, 1012584.2, 1013820.86, 1014218.8, 1014898.78, 1015051.62, 1015196.46, 1015654.6, 1015684.09, 1015737.61, 1015853.03, 1016019.47, 1016039.71, 1016143.64, 1016637.39, 1016752.55, 1016756.1, 1017045.44, 1017050.65, 1017593.47, 1017867.8, 1018541.3, 1019555.51, 1019741.1, 1020486.05, 1020651.74, 1021154.48, 1021391.99, 1021400.42, 1021534.7, 1021568.34, 1021766.75, 1022018.43, 1022270.86, 1022293.81, 1022571.25, 1022704.2, 1023997.71, 1024778.23, 1024784.92, 1025382.22, 1025766.27, 1025813.8, 1026439.93, 1027584.51, 1028151.72, 1028206.51, 1028569.01, 1028635.39, 1029248.22, 1029618.1, 1029849.2, 1031139.3, 1031451.35, 1031745.14, 1032076.06, 1032908.23, 1033017.37, 1033171.07, 1033543.56, 1033552.18, 1033719.5, 1034119.21, 1034448.07, 1037464.27, 1037476.38, 1037549.71, 1037687.07, 1037861.11, 1038576.54, 1040143.14, 1041202.13, 1041238.87, 1041995.22, 1042043.55, 1042226.3, 1042454.61, 1043240.27, 1043388.79, 1043698.64, 1043962.36, 1044079.2, 1044639.69, 1045124.88, 1045419.87, 1045722.37, 1045859.39, 1046068.17, 1046203.72, 1046416.17, 1046782.52, 1046816.59, 1047178.91, 1047297.38, 1047444.59, 1047658.09, 1047707.59, 1048101.39, 1048134.24, 1048212.62, 1048617.17, 1048706.75, 1048802.62, 1048866.3, 1049357.36, 1049372.38, 1049625.9, 1049772.04, 1050027.89, 1051116.95, 1051121.02, 1051190.44, 1051518.45, 1051740.29, 1051864.6, 1051922.95, 1051944.79, 1052034.74, 1052051.45, 1052066.58, 1052120.43, 1052429.03, 1052609.16, 1052895.25, 1052973.28, 1053247.1, 1053495.51, 1053881.78, 1054454.4, 1054754.67, 1055841.24, 1056282.91, 1056478.65, 1056992.18, 1057290.41, 1057295.87, 1057425.83, 1058250.91, 1058767.95, 1059676.62, 1059715.27, 1059781.78, 1060433.1, 1060446.16, 1060770.11, 1060868.49, 1060906.75, 1061089.56, 1061134.37, 1061196.47, 1061219.19, 1061943.49, 1062548.73, 1062629.3, 1063056.21, 1063149.78, 1063310.62, 1063557.49, 1063818.2, 1063960.11, 1064617.62, 1065108.64, 1065214.14, 1065350.56, 1065427.37, 1066478.1, 1066566.74, 1066792.63, 1067310.74, 1067340.74, 1067432.1, 1067754.06, 1068157.45, 1068292.56, 1068346.76, 1068719.22, 1069061.63, 1069112.0, 1069533.17, 1069710.97, 1069851.59, 1070119.09, 1070389.98, 1070457.8, 1071040.22, 1071383.1, 1071598.36, 1072474.8, 1073433.69, 1073862.59, 1074079.0, 1074479.73, 1074535.88, 1075656.34, 1075687.74, 1075758.55, 1076021.58, 1077018.27, 1077253.67, 1077491.68, 1077640.13, 1078182.18, 1078348.91, 1078455.48, 1078557.62, 1078900.44, 1078905.68, 1079314.52, 1079386.88, 1079398.81, 1079669.11, 1079931.63, 1080012.04, 1080357.89, 1081005.64, 1081322.12, 1081420.96, 1081874.03, 1082158.21, 1082559.06, 1082763.27, 1083071.14, 1083521.24, 1083657.61, 1083811.19, 1084243.91, 1084487.55, 1084722.78, 1084894.47, 1085248.21, 1085483.44, 1086231.47, 1086421.57, 1086533.18, 1086661.02, 1087051.26, 1087507.47, 1087578.78, 1087616.19, 1087644.5, 1088025.8, 1088248.4, 1088446.58, 1088498.52, 1088943.98, 1089530.94, 1090558.57, 1090587.5, 1090915.09, 1091020.37, 1091822.72, 1092204.79, 1092616.49, 1092654.26, 1092704.09, 1093319.37, 1094058.68, 1094422.69, 1095020.33, 1095058.57, 1095091.53, 1095421.65, 1095504.26, 1095539.13, 1095889.22, 1095932.51, 1096232.89, 1096692.88, 1096930.65, 1097006.3, 1097786.14, 1098286.61, 1099055.65, 1099351.68, 1099714.93, 1099937.25, 1100046.37, 1100418.69, 1100625.06, 1100765.5, 1101458.21, 1101621.36, 1102185.0, 1102367.65, 1102857.37, 1102975.59, 1103740.4, 1104277.57, 1105860.06, 1106176.83, 1106575.59, 1106642.63, 1106847.62, 1107170.39, 1107366.06, 1107432.71, 1107494.55, 1107552.43, 1108580.19, 1108686.87, 1109105.92, 1109216.35, 1109574.11, 1110244.52, 1110479.94, 1110706.06, 1110827.48, 1110941.78, 1111170.91, 1111215.72, 1111638.07, 1111797.21, 1112034.72, 1112449.3, 1112871.23, 1113208.57, 1114530.29, 1115138.51, 1115240.61, 1115255.65, 1115504.26, 1115514.61, 1115985.81, 1116140.29, 1116211.39, 1116295.24, 1116439.02, 1116829.23, 1117097.23, 1117536.09, 1117863.33, 1118163.94, 1118313.7, 1119809.71, 1119979.98, 1120018.92, 1120172.24, 1120259.71, 1120508.14, 1120619.32, 1120731.76, 1121405.91, 1121476.51, 1121756.43, 1121934.15, 1122034.48, 1122053.58, 1122356.53, 1123282.85, 1123446.51, 1123566.12, 1124357.2, 1124414.87, 1124537.97, 1124660.77, 1124763.74, 1125169.92, 1125329.77, 1126685.95, 1126921.34, 1126962.44, 1127516.25, 1127859.69, 1128237.3, 1128485.1, 1128765.71, 1129031.98, 1129422.86, 1129508.61, 1129540.48, 1129909.44, 1130022.99, 1130926.79, 1131732.94, 1132064.23, 1132433.55, 1132948.48, 1133657.58, 1133807.03, 1133913.33, 1134767.28, 1135035.09, 1135340.19, 1135577.62, 1138101.18, 1138162.76, 1138800.32, 1139131.78, 1140501.03, 1140578.16, 1141019.11, 1141184.66, 1141860.67, 1142499.25, 1143724.48, 1143819.35, 1144254.26, 1144552.09, 1144854.56, 1144901.52, 1145084.76, 1145840.91, 1146632.46, 1146992.13, 1147503.92, 1147556.83, 1147636.96, 1147906.46, 1148624.83, 1148987.46, 1149234.96, 1149427.48, 1149448.02, 1149612.04, 1150003.36, 1150191.72, 1150204.71, 1150344.39, 1150662.55, 1150663.42, 1150729.89, 1151052.86, 1151214.41, 1151258.74, 1151282.31, 1151993.85, 1152117.5, 1152781.9, 1153332.89, 1153596.53, 1155594.2, 1156003.7, 1156377.47, 1156826.31, 1157111.15, 1157557.79, 1157975.68, 1158062.99, 1158247.31, 1158693.12, 1158698.44, 1158708.98, 1158722.74, 1159089.6, 1159119.6, 1159132.58, 1159212.1, 1159438.53, 1159812.35, 1160043.98, 1160412.71, 1160619.61, 1161190.29, 1161615.51, 1161900.18, 1162042.24, 1162610.27, 1162675.85, 1163234.33, 1163803.3, 1163869.52, 1163878.49, 1165870.54, 1166117.85, 1166142.85, 1166479.51, 1167621.14, 1167740.2, 1167757.0, 1167829.33, 1168397.26, 1168582.02, 1168815.31, 1168826.39, 1169413.27, 1169773.85, 1169831.38, 1169988.62, 1170103.25, 1170456.16, 1170672.94, 1171391.41, 1171834.47, 1172003.1, 1172155.28, 1172672.27, 1173059.79, 1173131.63, 1173307.4, 1173521.82, 1174209.52, 1174447.55, 1175003.67, 1175326.23, 1175420.26, 1175447.49, 1175738.22, 1176079.59, 1176588.25, 1176681.31, 1177340.99, 1177539.71, 1178039.0, 1178211.81, 1178641.71, 1178841.05, 1178905.44, 1179036.3, 1179125.48, 1179315.72, 1179420.5, 1179738.5, 1179773.88, 1179788.38, 1179851.68, 1179915.04, 1180183.39, 1180470.8, 1180671.55, 1180797.2, 1181204.53, 1181651.55, 1181793.55, 1181815.31, 1182099.88, 1182198.7, 1182490.46, 1182500.16, 1182691.87, 1182694.95, 1182733.0, 1182901.56, 1183225.92, 1183528.58, 1183571.35, 1183740.91, 1183979.27, 1184198.41, 1185391.96, 1185674.48, 1186971.02, 1187051.07, 1187359.77, 1187384.53, 1187776.19, 1187880.7, 1187988.64, 1188047.61, 1189556.47, 1189646.45, 1189887.86, 1190515.83, 1191585.92, 1192031.38, 1192074.09, 1192213.87, 1192982.07, 1193199.39, 1194334.65, 1194449.78, 1195036.0, 1195897.6, 1196105.44, 1196813.33, 1196880.11, 1197019.39, 1197373.13, 1197489.66, 1197761.17, 1198014.97, 1198025.76, 1198071.6, 1198104.22, 1198670.19, 1198709.65, 1199292.06, 1199309.59, 1199330.85, 1199449.54, 1199845.29, 1200019.74, 1200729.45, 1200815.3, 1200888.28, 1200892.56, 1201059.72, 1201511.62, 1201694.14, 1202775.24, 1202963.06, 1203080.41, 1203119.96, 1203172.05, 1203399.64, 1203682.62, 1204628.28, 1204807.83, 1205307.5, 1205536.71, 1205662.85, 1205884.98, 1206252.12, 1206795.74, 1206917.2, 1207303.29, 1208191.61, 1208600.05, 1208654.4, 1208809.34, 1208825.6, 1209524.11, 1210602.91, 1211026.13, 1211136.63, 1212938.67, 1212967.84, 1213310.45, 1213486.95, 1213860.61, 1213981.64, 1213994.39, 1214183.97, 1214302.76, 1214944.29, 1215273.2, 1215354.38, 1215676.31, 1216059.41, 1216876.52, 1217199.39, 1217923.71, 1218688.09, 1218764.94, 1219263.4, 1219583.91, 1219979.29, 1220115.75, 1220579.55, 1220815.33, 1220983.17, 1220984.94, 1221318.17, 1221723.94, 1222255.47, 1222367.9, 1222511.29, 1223355.5, 1223777.48, 1224175.99, 1224475.12, 1224915.66, 1225182.04, 1225336.41, 1225700.28, 1226997.7, 1227118.75, 1227148.13, 1227430.73, 1227469.2, 1227535.97, 1227893.89, 1229008.32, 1229257.7, 1229635.7, 1229760.97, 1229777.24, 1230011.95, 1230012.16, 1230106.13, 1230118.02, 1230245.74, 1230250.25, 1230514.58, 1230591.97, 1230596.8, 1230613.5, 1231025.07, 1231428.46, 1231688.48, 1231752.54, 1231993.14, 1232073.18, 1232376.49, 1232784.22, 1234281.7, 1234759.54, 1234875.33, 1235094.66, 1235121.33, 1235163.86, 1235775.15, 1236238.29, 1237104.73, 1238742.0, 1238844.56, 1239299.12, 1239423.19, 1239466.97, 1239741.34, 1239766.89, 1239813.26, 1240048.85, 1240126.07, 1240921.19, 1242746.06, 1242874.98, 1242909.53, 1243200.24, 1243370.74, 1243812.59, 1243814.77, 1244177.21, 1244381.98, 1244390.03, 1244391.83, 1244542.33, 1244925.94, 1244956.91, 1245268.77, 1245480.95, 1245576.65, 1245624.27, 1245628.61, 1245772.7, 1245827.08, 1245898.73, 1246062.17, 1246242.61, 1246322.44, 1246654.24, 1247130.22, 1248330.1, 1248901.98, 1248915.43, 1248950.65, 1249439.95, 1249696.97, 1249786.4, 1250178.89, 1251581.89, 1252915.43, 1253218.7, 1253316.3, 1253329.17, 1254107.84, 1254587.84, 1254914.87, 1254955.68, 1255081.22, 1255087.26, 1255414.84, 1255633.29, 1256282.79, 1257778.34, 1257921.28, 1257928.35, 1257972.37, 1258311.56, 1258364.31, 1258674.12, 1259278.36, 1259414.52, 1259877.19, 1259941.48, 1261109.01, 1261158.47, 1261253.18, 1261306.37, 1261693.16, 1261872.67, 1261964.96, 1262025.08, 1262289.29, 1263534.86, 1263680.51, 1263836.59, 1264014.16, 1264117.01, 1264272.52, 1264434.7, 1264575.18, 1264736.59, 1265852.43, 1266229.07, 1266254.21, 1266460.45, 1266546.73, 1266564.94, 1266570.4, 1266796.13, 1267619.06, 1267675.05, 1268240.66, 1268266.72, 1268503.49, 1268766.76, 1269113.41, 1270025.74, 1270036.53, 1270577.01, 1270658.64, 1270816.01, 1271311.76, 1271646.62, 1272395.02, 1272809.11, 1272842.85, 1272948.27, 1273279.79, 1273295.46, 1273670.32, 1274463.02, 1275146.94, 1275591.84, 1275597.85, 1276031.84, 1276609.36, 1277150.6, 1277758.76, 1277882.77, 1277959.42, 1278304.33, 1279041.64, 1279080.58, 1279623.26, 1279666.47, 1279819.43, 1280156.47, 1280231.85, 1280414.8, 1280465.8, 1280958.97, 1281675.6, 1282320.05, 1282378.71, 1283482.85, 1283563.43, 1283716.81, 1283766.55, 1283849.38, 1283885.55, 1284185.49, 1284334.79, 1285358.01, 1285534.74, 1285783.87, 1285897.24, 1285976.53, 1286388.96, 1286413.71, 1286598.59, 1286833.62, 1287034.7, 1287288.59, 1287899.41, 1288154.1, 1288823.72, 1289082.81, 1289151.84, 1289156.9, 1290532.97, 1290576.44, 1290609.54, 1290684.95, 1291398.71, 1292346.57, 1292436.23, 1292724.9, 1292830.93, 1293404.18, 1293472.8, 1293707.19, 1294105.01, 1294769.08, 1295391.19, 1295605.35, 1296658.47, 1297028.6, 1297237.7, 1297335.87, 1297452.0, 1297472.06, 1297535.69, 1297584.95, 1297792.41, 1298775.8, 1298809.8, 1300104.03, 1300131.68, 1300147.07, 1300375.76, 1300593.61, 1300806.74, 1301185.28, 1301590.13, 1302047.48, 1302499.23, 1302600.14, 1303055.09, 1303233.15, 1303523.73, 1303726.54, 1303732.36, 1303914.27, 1304481.75, 1304584.4, 1304706.75, 1304850.67, 1305068.1, 1305950.22, 1306194.55, 1306551.71, 1306644.25, 1307142.75, 1307182.29, 1307339.14, 1307551.92, 1307928.01, 1308122.15, 1308179.02, 1308222.24, 1308537.75, 1308967.44, 1308977.05, 1309226.79, 1309340.16, 1309437.17, 1309476.68, 1310087.0, 1310684.1, 1310701.8, 1310973.06, 1311153.72, 1311175.93, 1311263.07, 1311352.25, 1311690.11, 1311704.92, 1311775.83, 1311796.91, 1311950.16, 1311965.09, 1311986.87, 1312329.78, 1312849.1, 1312877.01, 1312905.8, 1313729.72, 1314557.31, 1314626.75, 1314651.83, 1314987.4, 1314994.32, 1315023.08, 1315091.63, 1315118.4, 1315356.99, 1315610.66, 1315684.86, 1316385.43, 1316542.59, 1316849.36, 1316899.31, 1317379.68, 1317672.92, 1318343.58, 1318854.22, 1318905.53, 1319035.06, 1319054.57, 1319325.59, 1319588.04, 1319767.55, 1319773.55, 1320239.51, 1320301.61, 1320359.23, 1321102.35, 1321741.35, 1321914.34, 1322117.96, 1322852.2, 1322932.36, 1323004.73, 1323243.35, 1323487.91, 1323999.36, 1324455.72, 1325022.77, 1325107.53, 1325835.7, 1326132.98, 1326197.24, 1326255.7, 1326370.08, 1326621.98, 1326877.11, 1327035.27, 1327139.35, 1327401.06, 1327405.42, 1327424.28, 1327705.44, 1327719.34, 1328468.89, 1328740.71, 1330451.46, 1330473.47, 1330757.22, 1331137.96, 1331453.41, 1331514.44, 1331883.16, 1332261.01, 1332594.07, 1332716.53, 1332759.13, 1332940.35, 1332952.47, 1333315.03, 1333347.78, 1333634.45, 1333740.35, 1334571.87, 1334627.96, 1335233.74, 1335433.72, 1335647.1, 1336044.75, 1336404.65, 1336522.92, 1336838.41, 1337405.6, 1337506.74, 1337617.55, 1337875.49, 1338132.72, 1338299.02, 1338572.29, 1338627.55, 1338657.95, 1338716.37, 1338862.58, 1339570.85, 1339630.35, 1339811.68, 1339972.83, 1340232.55, 1340293.87, 1341240.62, 1342123.78, 1342254.55, 1342273.64, 1343637.0, 1343773.94, 1344167.13, 1344243.17, 1344354.41, 1344483.81, 1344558.92, 1344580.92, 1344723.97, 1344890.73, 1345167.61, 1345311.65, 1345454.0, 1345595.82, 1345631.96, 1346271.06, 1346345.97, 1346783.35, 1346862.72, 1346994.53, 1347175.93, 1347454.59, 1347607.74, 1348031.55, 1348410.05, 1348995.17, 1349202.25, 1350059.35, 1350441.68, 1350646.16, 1350673.98, 1350730.31, 1351407.79, 1351450.43, 1351791.03, 1352039.88, 1352084.21, 1352219.79, 1352401.08, 1352442.31, 1352470.09, 1352547.7, 1352780.76, 1352809.5, 1353285.1, 1353838.39, 1354168.18, 1354188.43, 1355234.3, 1355391.79, 1355405.95, 1355600.01, 1355680.3, 1355704.21, 1356689.88, 1356938.95, 1357036.1, 1357154.71, 1357589.89, 1357600.68, 1357672.24, 1358111.62, 1358444.07, 1358816.46, 1359158.57, 1359770.73, 1359921.13, 1360150.12, 1360317.9, 1360517.52, 1360520.56, 1360969.45, 1361595.33, 1361944.94, 1361945.18, 1362144.0, 1363155.77, 1363167.95, 1363365.05, 1363460.86, 1363973.16, 1364207.0, 1364445.98, 1364721.58, 1365098.46, 1365541.59, 1365546.69, 1365552.28, 1365633.53, 1365678.22, 1365824.97, 1366053.69, 1366193.35, 1366381.6, 1366395.96, 1366937.1, 1367202.84, 1367320.01, 1367448.28, 1368090.08, 1368130.35, 1368312.45, 1368318.17, 1368471.23, 1369125.37, 1369131.46, 1369317.63, 1369634.92, 1369971.57, 1370251.22, 1370562.11, 1370653.41, 1370659.54, 1370920.87, 1371405.33, 1371465.66, 1371889.27, 1371986.6, 1372043.71, 1372484.9, 1372500.63, 1372504.9, 1372872.35, 1373064.87, 1373270.06, 1373651.49, 1373841.91, 1373907.21, 1374301.34, 1374863.1, 1374891.36, 1375101.26, 1375166.86, 1375307.54, 1375458.21, 1375962.46, 1376520.1, 1376571.21, 1376670.27, 1376732.18, 1377092.33, 1377119.45, 1377322.73, 1377485.12, 1377593.1, 1377716.17, 1378340.18, 1378730.45, 1379456.3, 1379473.03, 1379488.05, 1379579.63, 1379651.87, 1379652.65, 1379783.21, 1380020.27, 1380257.12, 1380522.64, 1380836.35, 1380892.08, 1380952.05, 1381339.23, 1381796.8, 1382359.21, 1382783.83, 1384209.22, 1384243.75, 1384339.1, 1384552.17, 1384584.59, 1384721.84, 1384870.51, 1384921.63, 1385065.2, 1385323.7, 1385362.49, 1385769.03, 1385860.38, 1386407.17, 1386472.59, 1386520.99, 1386789.31, 1387365.19, 1387517.63, 1387953.75, 1388118.53, 1388553.11, 1388725.63, 1388755.61, 1388809.43, 1388973.65, 1390122.11, 1390174.63, 1390934.27, 1391013.96, 1391256.12, 1391257.28, 1391479.91, 1391580.41, 1391792.69, 1391813.69, 1392093.04, 1392143.82, 1392395.2, 1392543.37, 1392938.06, 1394065.76, 1394299.0, 1394393.84, 1394561.83, 1394841.03, 1395339.71, 1395710.09, 1396150.15, 1396322.19, 1396612.36, 1396926.82, 1397094.26, 1397301.38, 1397970.54, 1399073.75, 1399322.44, 1399341.07, 1399456.99, 1399662.07, 1399960.15, 1400160.95, 1401113.42, 1401232.52, 1401944.8, 1402233.69, 1402372.09, 1402654.95, 1402818.01, 1402902.47, 1403198.94, 1403460.87, 1403779.25, 1404429.92, 1404516.29, 1404576.48, 1405007.44, 1405065.57, 1405119.23, 1405168.06, 1405475.78, 1405572.93, 1405914.39, 1406124.14, 1406313.13, 1407036.59, 1407191.96, 1407842.91, 1407897.57, 1408016.1, 1408082.96, 1408118.22, 1408464.08, 1408907.89, 1408968.55, 1409310.4, 1409515.73, 1409544.97, 1409705.03, 1409727.59, 1409989.67, 1410181.7, 1410683.94, 1411835.57, 1412065.04, 1412157.02, 1412387.37, 1412721.18, 1412925.25, 1412959.97, 1413124.11, 1413302.7, 1413382.76, 1414107.1, 1414343.53, 1414564.53, 1414713.5, 1415204.88, 1415473.91, 1415746.91, 1416005.59, 1416168.98, 1416301.17, 1416344.68, 1416720.54, 1416790.17, 1416926.31, 1417013.07, 1417267.07, 1417515.93, 1417535.78, 1417616.81, 1417875.42, 1417922.37, 1418027.08, 1418697.05, 1418973.62, 1419236.9, 1419383.19, 1419445.12, 1419911.91, 1420405.41, 1421111.55, 1421307.2, 1422546.05, 1422600.43, 1422711.6, 1422794.26, 1423144.47, 1423289.9, 1424225.44, 1424500.47, 1424720.27, 1425078.59, 1425100.71, 1425299.37, 1425559.02, 1425603.65, 1426405.46, 1426418.53, 1426622.65, 1427023.45, 1427162.26, 1427383.24, 1427624.5, 1427881.22, 1428218.27, 1428436.33, 1428869.9, 1428960.72, 1428993.33, 1429143.06, 1429345.86, 1429829.36, 1429954.66, 1430192.37, 1430348.1, 1430378.67, 1430851.11, 1431003.43, 1431426.34, 1431910.98, 1432069.95, 1432953.21, 1433391.04, 1433569.44, 1434036.18, 1434709.63, 1434908.13, 1435379.25, 1436311.76, 1436383.84, 1436883.99, 1436940.78, 1437059.26, 1437319.45, 1438383.44, 1438465.81, 1438830.15, 1439034.86, 1439123.71, 1439432.06, 1439541.59, 1439607.35, 1439901.05, 1440263.15, 1440374.13, 1440687.69, 1440785.7, 1440963.0, 1441032.59, 1441473.82, 1441559.4, 1441884.28, 1442092.08, 1442819.28, 1442873.22, 1442988.44, 1443206.46, 1443285.79, 1443311.49, 1443884.39, 1444732.28, 1444783.64, 1445174.79, 1445249.09, 1445596.61, 1446210.26, 1447028.06, 1447301.24, 1447614.08, 1447916.29, 1448797.02, 1448938.92, 1449142.92, 1450407.32, 1450628.85, 1450733.29, 1450766.12, 1451392.67, 1451740.57, 1451782.16, 1451953.95, 1453047.02, 1453153.33, 1453185.65, 1453329.5, 1453416.53, 1455090.69, 1455119.97, 1456073.24, 1456221.1, 1456300.89, 1456793.33, 1456800.28, 1456957.38, 1456997.2, 1457270.16, 1457314.39, 1457345.75, 1458059.42, 1458287.38, 1459276.77, 1459396.84, 1459409.1, 1459601.17, 1459655.85, 1460234.31, 1460354.67, 1461129.94, 1461393.91, 1461718.87, 1462254.05, 1462731.93, 1462941.03, 1463501.99, 1463942.62, 1464050.02, 1464295.69, 1464462.85, 1464616.59, 1464693.46, 1465089.85, 1465187.71, 1465283.29, 1465489.75, 1466046.07, 1466046.67, 1466058.28, 1466164.49, 1467024.3, 1467473.63, 1467722.19, 1467889.2, 1468350.36, 1468871.49, 1468928.37, 1469252.05, 1469593.37, 1469693.99, 1470308.32, 1470520.83, 1470764.35, 1470792.41, 1471261.76, 1471816.52, 1472033.38, 1472515.79, 1472663.1, 1472752.01, 1473386.75, 1473868.15, 1474498.59, 1475685.1, 1476144.34, 1477134.75, 1478321.26, 1478537.93, 1479249.9, 1479514.66, 1480289.64, 1481618.74, 1481728.13, 1481739.2, 1483574.38, 1483784.18, 1483991.05, 1484169.74, 1484708.38, 1484995.38, 1485540.28, 1486920.17, 1487542.53, 1487797.54, 1488538.09, 1489613.32, 1490235.86, 1491039.14, 1491300.42, 1492060.89, 1492388.98, 1492399.13, 1492418.14, 1492507.44, 1493525.93, 1493544.04, 1493659.74, 1494122.38, 1494251.5, 1494417.07, 1494479.49, 1494497.39, 1495064.75, 1495143.62, 1495536.46, 1495607.07, 1495844.57, 1496169.81, 1496305.78, 1497054.81, 1497073.82, 1497462.72, 1497954.76, 1498080.16, 1499496.67, 1499727.02, 1500863.54, 1501095.49, 1501503.68, 1501663.26, 1502078.93, 1502562.78, 1502617.99, 1503284.06, 1503298.7, 1504545.94, 1504651.57, 1505442.15, 1506126.06, 1507460.69, 1507637.17, 1507708.93, 1508068.77, 1508237.76, 1508239.93, 1508933.26, 1509323.09, 1510131.45, 1510397.27, 1510443.62, 1510925.32, 1511041.69, 1511068.07, 1511641.09, 1511717.53, 1512207.95, 1512227.34, 1513080.49, 1513229.16, 1513635.64, 1514055.97, 1514259.78, 1514288.82, 1514435.51, 1514828.82, 1514999.17, 1515175.01, 1515890.38, 1515976.11, 1516924.23, 1517029.9, 1517075.67, 1517428.87, 1518177.71, 1518790.89, 1518841.45, 1519013.49, 1519604.5, 1519846.36, 1521577.87, 1521957.99, 1522042.57, 1522421.07, 1522512.2, 1522978.54, 1523101.38, 1523410.71, 1523420.38, 1523870.89, 1523979.11, 1524059.4, 1524390.07, 1524734.29, 1525147.09, 1526506.08, 1526801.24, 1527014.04, 1527455.19, 1527682.99, 1527688.58, 1527845.81, 1528008.64, 1529615.54, 1530761.43, 1531599.44, 1531938.44, 1532114.86, 1532308.62, 1532308.78, 1532316.79, 1532893.22, 1534594.0, 1534849.64, 1535287.4, 1535857.49, 1536176.54, 1536549.95, 1537139.56, 1539063.91, 1539230.32, 1539387.83, 1539483.7, 1539930.5, 1540163.53, 1540421.49, 1540435.99, 1540471.24, 1540687.63, 1541102.38, 1541745.59, 1542131.05, 1542173.33, 1542561.09, 1542719.87, 1543049.52, 1543365.9, 1543461.12, 1543532.83, 1543667.68, 1543678.02, 1543947.23, 1544422.35, 1544653.37, 1545370.16, 1545418.53, 1546074.18, 1547654.98, 1547729.24, 1548033.78, 1548661.45, 1549018.68, 1549113.18, 1550214.02, 1550229.22, 1550369.92, 1550385.65, 1551659.28, 1552886.59, 1552934.64, 1553191.63, 1553250.16, 1553629.59, 1554651.08, 1554747.15, 1554794.22, 1554806.68, 1554837.62, 1555444.55, 1555672.51, 1556017.91, 1556627.62, 1556798.94, 1557120.44, 1557314.58, 1557485.75, 1557776.1, 1557888.16, 1558621.36, 1558968.49, 1559592.79, 1559889.0, 1560120.8, 1560590.05, 1562161.97, 1563140.85, 1563387.94, 1564246.02, 1564502.26, 1564516.43, 1564819.81, 1564897.32, 1565352.46, 1565498.84, 1566219.77, 1566668.91, 1566712.79, 1567138.07, 1567340.07, 1568048.54, 1568159.48, 1569304.4, 1569502.0, 1569607.94, 1570813.52, 1571158.56, 1572117.54, 1572966.15, 1573072.81, 1573898.63, 1573982.47, 1574287.76, 1574361.97, 1574408.67, 1574684.08, 1576654.67, 1576818.06, 1577439.81, 1577468.78, 1577486.33, 1577541.24, 1580732.73, 1582083.4, 1582168.27, 1584083.95, 1584623.36, 1585240.92, 1587257.78, 1587499.82, 1588142.26, 1588380.73, 1588430.71, 1588948.32, 1590274.72, 1591453.39, 1591816.88, 1591835.02, 1591920.42, 1592409.97, 1593012.75, 1593655.96, 1594938.89, 1594968.28, 1595362.27, 1595901.87, 1596036.66, 1596325.01, 1597002.71, 1597868.05, 1598080.52, 1598643.65, 1599626.26, 1601348.82, 1601377.41, 1601584.57, 1601585.7, 1603793.42, 1603955.12, 1604605.69, 1604775.58, 1605491.78, 1606208.68, 1606221.56, 1606283.86, 1606629.58, 1607343.41, 1608077.01, 1608277.74, 1608435.45, 1609811.75, 1609951.02, 1611096.05, 1611196.61, 1611968.17, 1613259.77, 1613342.19, 1613718.38, 1613773.9, 1614259.35, 1615494.14, 1615524.71, 1615987.96, 1616394.45, 1617025.41, 1617612.03, 1618272.25, 1619920.04, 1620374.24, 1620603.92, 1620748.25, 1620839.34, 1621031.7, 1621109.3, 1621841.33, 1622150.33, 1623442.24, 1623519.64, 1623716.46, 1624170.99, 1624383.75, 1624477.58, 1624539.21, 1624994.43, 1625883.71, 1627274.93, 1627707.31, 1627904.68, 1628100.79, 1628868.28, 1629066.9, 1629391.28, 1629978.46, 1630564.48, 1630607.0, 1630989.95, 1631135.79, 1631737.68, 1632406.0, 1632616.09, 1632894.58, 1633241.59, 1633663.12, 1634635.86, 1635078.41, 1635984.07, 1636224.77, 1636263.41, 1636339.65, 1637266.29, 1639358.93, 1639585.61, 1639999.47, 1640168.99, 1640476.77, 1640681.88, 1641867.92, 1641957.44, 1642074.64, 1642247.48, 1642970.27, 1643690.9, 1644470.66, 1645097.75, 1645892.97, 1646655.94, 1648570.03, 1648602.39, 1648829.18, 1648863.46, 1648882.62, 1649604.63, 1649614.93, 1650285.54, 1650394.44, 1650405.21, 1650604.6, 1650894.3, 1651605.35, 1652635.1, 1653759.36, 1655036.75, 1656130.67, 1656886.46, 1659221.99, 1660081.29, 1660228.88, 1660433.3, 1660906.14, 1661767.33, 1663525.77, 1665502.55, 1667181.82, 1669299.78, 1669388.45, 1670579.82, 1670785.97, 1671857.57, 1672339.27, 1672352.29, 1674306.31, 1675292.0, 1675431.16, 1675562.94, 1677067.24, 1677248.24, 1677472.78, 1677556.18, 1680693.06, 1680764.06, 1681121.38, 1682316.31, 1682368.32, 1682614.26, 1682652.51, 1682862.03, 1683401.78, 1684468.66, 1684519.99, 1685375.47, 1685652.35, 1685910.53, 1686010.02, 1686842.78, 1687592.16, 1688281.86, 1688420.76, 1688531.34, 1688935.71, 1688955.49, 1689591.44, 1689844.18, 1690317.99, 1691439.52, 1693058.91, 1693935.29, 1694551.15, 1694862.41, 1695371.68, 1696248.27, 1696619.52, 1697229.58, 1697230.96, 1699095.9, 1699708.38, 1700627.97, 1702220.96, 1703047.74, 1703850.25, 1704218.84, 1704357.62, 1704753.02, 1704785.74, 1705506.29, 1705655.09, 1705810.84, 1707158.82, 1707298.14, 1707481.9, 1707662.87, 1708283.28, 1709365.19, 1709373.62, 1710372.4, 1710803.59, 1710923.94, 1711562.73, 1711769.11, 1711813.13, 1712987.56, 1712995.44, 1713769.06, 1713889.11, 1714309.9, 1714667.0, 1715769.05, 1716755.78, 1720221.91, 1720530.23, 1720537.26, 1720908.01, 1723736.91, 1723739.44, 1724557.22, 1724669.75, 1725268.56, 1727175.61, 1727565.42, 1728388.2, 1728399.07, 1730913.66, 1731935.43, 1733822.4, 1733983.09, 1734834.82, 1735339.59, 1737947.64, 1740063.1, 1740234.06, 1741308.56, 1743000.38, 1743188.87, 1743816.41, 1743882.19, 1744193.58, 1744349.05, 1744544.39, 1744725.48, 1744879.06, 1745362.72, 1745545.28, 1745841.33, 1745928.56, 1746470.56, 1747289.53, 1748000.65, 1748010.29, 1750197.81, 1750434.55, 1750891.47, 1751369.75, 1751384.9, 1754879.45, 1755334.18, 1755889.53, 1757041.96, 1757242.51, 1757923.88, 1758050.79, 1758437.96, 1758587.35, 1758971.38, 1759777.25, 1761016.51, 1761235.67, 1761506.68, 1762155.79, 1762539.3, 1763545.32, 1764133.09, 1764756.31, 1764847.94, 1764984.15, 1765571.91, 1765584.48, 1766162.05, 1766331.45, 1767471.48, 1768172.31, 1768249.89, 1769296.25, 1769793.37, 1769854.16, 1770177.37, 1770333.9, 1771792.97, 1772143.94, 1772192.42, 1773500.56, 1774342.61, 1775068.4, 1777166.53, 1779236.54, 1779276.51, 1780443.36, 1781528.77, 1781717.71, 1781767.22, 1781805.66, 1781866.98, 1781905.24, 1783910.06, 1784029.95, 1785187.29, 1785823.37, 1788227.6, 1789113.32, 1789687.65, 1790279.74, 1790439.16, 1790694.59, 1790925.8, 1792210.89, 1792345.3, 1793903.6, 1794355.49, 1794868.74, 1794962.64, 1795152.73, 1796203.51, 1796637.61, 1796949.59, 1797732.56, 1799070.98, 1799520.14, 1799682.38, 1799737.79, 1800171.36, 1800728.07, 1802450.29, 1802477.43, 1802677.9, 1802755.11, 1804246.16, 1805885.04, 1805999.79, 1806924.74, 1807545.43, 1808056.41, 1808250.71, 1809119.7, 1809989.29, 1810684.68, 1811455.15, 1811562.88, 1811606.21, 1812208.22, 1812768.26, 1814240.85, 1814740.09, 1814806.63, 1815760.42, 1815798.85, 1816489.53, 1817273.28, 1817603.66, 1817850.32, 1817887.23, 1817914.71, 1817934.76, 1818452.72, 1818906.73, 1819660.44, 1819666.46, 1819870.0, 1820723.17, 1821139.91, 1821364.42, 1821990.93, 1824711.21, 1826241.44, 1827440.43, 1827521.71, 1827733.18, 1827797.4, 1828010.25, 1828052.47, 1829415.67, 1829521.83, 1830075.13, 1830939.1, 1831676.03, 1831933.95, 1832211.96, 1832664.03, 1833511.08, 1834458.35, 1834680.25, 1834737.58, 1835662.69, 1836595.58, 1836714.84, 1837457.69, 1837553.43, 1837636.24, 1837743.6, 1837884.79, 1838513.07, 1839128.83, 1840131.19, 1840491.41, 1840686.94, 1840955.23, 1841173.6, 1841369.99, 1842172.46, 1842465.78, 1842555.32, 1842821.02, 1843030.95, 1843971.15, 1844094.59, 1845879.79, 1845893.87, 1846651.95, 1847344.45, 1847430.96, 1847552.61, 1847613.58, 1847990.41, 1848186.58, 1848403.92, 1848426.78, 1848953.48, 1849921.44, 1850205.47, 1851431.06, 1851519.69, 1852179.15, 1852432.58, 1852443.78, 1853161.99, 1853657.6, 1854967.66, 1855703.66, 1856467.84, 1857380.09, 1857480.84, 1857500.96, 1857533.7, 1858440.92, 1858856.06, 1859144.96, 1860923.55, 1861802.7, 1862128.95, 1862476.27, 1863195.68, 1863840.49, 1864238.64, 1864436.12, 1864637.89, 1864746.1, 1865097.27, 1865502.46, 1865752.78, 1865820.81, 1866243.0, 1866369.93, 1866681.57, 1866719.96, 1867345.09, 1867403.01, 1869087.85, 1869110.55, 1869967.03, 1870619.23, 1870684.21, 1870720.73, 1870843.9, 1871021.01, 1871555.64, 1871924.07, 1872365.99, 1872921.31, 1873646.34, 1873812.93, 1874226.52, 1874289.79, 1874957.94, 1875040.16, 1875597.28, 1875686.44, 1875708.88, 1876359.39, 1876704.26, 1876788.15, 1877358.86, 1877410.36, 1877592.55, 1878167.44, 1878862.42, 1879107.31, 1879451.23, 1879794.89, 1879891.13, 1880436.94, 1880691.64, 1880752.36, 1880785.69, 1880902.62, 1881046.12, 1881176.67, 1881337.21, 1881788.19, 1882070.88, 1882095.98, 1882393.4, 1884343.67, 1884345.01, 1884427.84, 1884734.31, 1886299.98, 1886339.6, 1886393.94, 1886503.93, 1887182.27, 1887465.04, 1887733.21, 1887895.07, 1888119.7, 1890273.44, 1890870.75, 1891034.93, 1891816.0, 1892775.94, 1893447.71, 1893532.46, 1893736.9, 1893955.27, 1894324.09, 1894742.95, 1894960.68, 1895583.12, 1895601.05, 1895901.59, 1896873.99, 1896937.1, 1897429.36, 1898193.95, 1898321.33, 1898427.66, 1898777.07, 1899013.34, 1899676.88, 1899959.61, 1900246.47, 1900535.9, 1900638.6, 1900745.13, 1902557.66, 1902762.5, 1903290.58, 1903385.14, 1903752.6, 1904421.74, 1904438.59, 1904512.34, 1904608.09, 1904618.17, 1905033.01, 1905733.68, 1906713.35, 1907351.2, 1907638.58, 1908036.68, 1908110.9, 1908278.27, 1909330.77, 1910092.37, 1910177.38, 1911470.84, 1911510.64, 1911559.1, 1911852.58, 1911967.44, 1912791.09, 1912909.69, 1913494.81, 1914148.89, 1914430.53, 1916812.74, 1917397.63, 1917483.1, 1917520.99, 1919053.21, 1919917.03, 1920725.15, 1921432.16, 1921655.48, 1923223.82, 1923957.09, 1925113.12, 1925393.91, 1925728.84, 1926004.99, 1927610.06, 1927664.11, 1927780.74, 1927993.09, 1928016.01, 1928720.51, 1928773.82, 1929153.16, 1929346.23, 1929486.63, 1929659.07, 1929736.35, 1929738.27, 1929768.03, 1930617.64, 1930690.37, 1930814.66, 1931104.67, 1931406.28, 1931668.64, 1932162.63, 1932231.05, 1932233.17, 1932491.42, 1933333.0, 1933469.15, 1933577.2, 1933719.21, 1933756.21, 1934099.65, 1935299.94, 1935593.87, 1935857.58, 1935869.1, 1936621.09, 1936878.46, 1937033.5, 1937628.26, 1938379.66, 1938608.52, 1939061.41, 1939440.09, 1939458.84, 1939927.09, 1939964.63, 1939980.43, 1941040.5, 1941346.13, 1941676.61, 1944164.32, 1944433.17, 1944438.9, 1945070.33, 1945808.26, 1946070.88, 1946104.64, 1946369.57, 1946875.06, 1947083.3, 1948359.78, 1948733.81, 1948982.7, 1949177.13, 1949183.14, 1949236.09, 1949354.29, 1949983.93, 1950676.39, 1950826.32, 1950904.84, 1950994.04, 1951494.85, 1951848.43, 1952555.66, 1953416.06, 1953539.85, 1953544.76, 1953628.82, 1953771.99, 1954689.21, 1954849.68, 1954952.0, 1955624.11, 1955689.12, 1955814.13, 1955896.59, 1956739.17, 1956813.31, 1957113.89, 1958003.19, 1958823.56, 1959526.96, 1959707.9, 1959967.8, 1960587.76, 1962445.04, 1962468.67, 1962625.01, 1962924.3, 1962996.7, 1964335.23, 1964701.94, 1967220.53, 1967996.71, 1968045.91, 1968462.58, 1969056.91, 1969121.45, 1969360.72, 1969742.76, 1970121.65, 1970170.29, 1970274.64, 1970340.25, 1970341.38, 1971057.44, 1973135.87, 1973544.27, 1974646.78, 1974687.51, 1974960.86, 1975374.56, 1976082.13, 1979009.46, 1979247.12, 1980405.03, 1981607.78, 1983190.56, 1984768.34, 1985784.59, 1986380.4, 1986445.65, 1986597.95, 1987089.36, 1987090.09, 1987531.05, 1988490.21, 1989674.07, 1990017.93, 1990371.02, 1990483.78, 1990932.77, 1991013.13, 1991824.05, 1991909.98, 1992436.96, 1993367.83, 1995891.87, 1995994.51, 1997181.09, 1997397.63, 1997816.98, 1998321.04, 1999079.44, 1999363.49, 1999794.26, 2000055.27, 2000626.14, 2000940.67, 2001636.96, 2002362.37, 2002750.99, 2003435.31, 2003480.59, 2003940.64, 2004330.3, 2004831.14, 2005097.76, 2005341.43, 2005478.46, 2006774.96, 2007050.75, 2007105.86, 2007796.26, 2008344.92, 2008350.58, 2009004.59, 2009163.08, 2010107.68, 2010216.49, 2010974.84, 2013115.79, 2014665.98, 2014954.79, 2015563.48, 2015781.27, 2016323.51, 2018010.15, 2018314.71, 2019031.67, 2020332.07, 2020550.99, 2020839.31, 2021699.38, 2022705.22, 2024554.1, 2025538.76, 2025582.62, 2026026.39, 2026176.14, 2027056.39, 2027507.15, 2027620.23, 2028587.24, 2030869.61, 2030933.46, 2031406.41, 2031650.55, 2031852.16, 2033211.62, 2033320.66, 2034400.78, 2034695.56, 2035189.66, 2035244.54, 2035431.39, 2036231.39, 2036317.54, 2036748.53, 2037663.71, 2037798.88, 2037880.96, 2039222.26, 2039415.74, 2039818.41, 2039875.75, 2041019.92, 2041069.37, 2041215.61, 2041507.4, 2041918.74, 2042581.71, 2043245.0, 2043349.41, 2044148.23, 2044155.39, 2045061.22, 2045396.06, 2045837.55, 2047766.07, 2047949.98, 2048035.74, 2048613.65, 2049046.95, 2049485.49, 2049860.26, 2050396.27, 2050542.56, 2051315.66, 2051533.53, 2052246.4, 2052984.81, 2053089.32, 2053165.41, 2053708.01, 2053952.97, 2054843.28, 2055952.61, 2056846.12, 2057059.53, 2057138.31, 2057406.33, 2057637.86, 2059458.25, 2060389.27, 2060588.69, 2062224.92, 2062481.56, 2063401.06, 2063682.76, 2064065.66, 2064991.71, 2065191.27, 2065377.15, 2065421.52, 2065984.95, 2066020.69, 2066187.72, 2066219.3, 2066541.86, 2067232.56, 2068097.18, 2068942.97, 2069284.57, 2069523.52, 2071022.45, 2072685.05, 2073102.59, 2073855.42, 2073951.38, 2074548.85, 2074668.19, 2074721.74, 2074953.46, 2075577.33, 2076231.8, 2076570.84, 2077256.24, 2078094.69, 2078417.47, 2078420.31, 2078796.76, 2079899.47, 2080529.06, 2080694.24, 2080764.17, 2080884.82, 2081181.35, 2081534.65, 2082083.34, 2082355.12, 2083379.89, 2086433.49, 2089381.77, 2090268.95, 2090340.98, 2090838.44, 2091592.54, 2091663.2, 2091903.63, 2092189.06, 2093139.01, 2094373.0, 2094515.71, 2095591.63, 2095599.93, 2095769.18, 2097266.85, 2097809.4, 2099615.88, 2100252.61, 2100489.79, 2102530.17, 2102539.93, 2103322.68, 2103455.75, 2104241.9, 2105058.91, 2105301.39, 2105668.74, 2106934.55, 2107285.85, 2108187.1, 2109107.9, 2111592.09, 2113432.58, 2114989.0, 2115408.31, 2116475.38, 2117854.6, 2119086.04, 2119163.01, 2119213.72, 2119438.53, 2121561.41, 2121788.61, 2123787.79, 2124316.34, 2124451.54, 2125104.72, 2125241.68, 2127661.17, 2128362.92, 2129035.91, 2129771.13, 2130287.27, 2131900.55, 2132446.0, 2133026.07, 2134680.12, 2135062.04, 2135143.87, 2135982.79, 2136989.46, 2137202.38, 2137809.5, 2138144.91, 2138651.97, 2139265.4, 2139733.68, 2141210.62, 2141765.98, 2142482.14, 2143080.57, 2143126.59, 2143424.61, 2143676.77, 2144245.39, 2144336.89, 2148822.76, 2149355.2, 2149594.46, 2151659.59, 2152229.11, 2154137.67, 2156035.06, 2160057.39, 2161549.76, 2162951.36, 2163384.17, 2163510.89, 2165160.29, 2165796.31, 2166737.65, 2168041.61, 2168097.11, 2168344.23, 2168709.76, 2169933.82, 2173373.91, 2173621.2, 2174056.71, 2174514.13, 2175563.69, 2176028.52, 2179360.94, 2180999.26, 2182246.69, 2182437.9, 2182563.66, 2184316.64, 2184980.35, 2187765.28, 2187847.29, 2188307.39, 2189353.63, 2191767.76, 2193048.75, 2193367.69, 2196688.46, 2196968.33, 2197299.65, 2202450.81, 2202742.9, 2203028.96, 2203523.2, 2203619.35, 2204556.7, 2205919.86, 2206319.9, 2207214.81, 2207742.13, 2209835.43, 2211388.14, 2213518.5, 2214477.06, 2214967.44, 2218595.8, 2219813.5, 2220600.76, 2224499.28, 2225016.73, 2226662.17, 2227152.16, 2231962.13, 2232892.1, 2234190.93, 2236209.13, 2237544.75, 2238573.48, 2243946.59, 2245257.18, 2246179.91, 2246411.89, 2248645.59, 2249570.04, 2249811.55, 2251206.64, 2256461.39, 2258489.63, 2258616.24, 2258781.28, 2263722.68, 2267452.4, 2269975.85, 2270188.99, 2271614.76, 2273470.62, 2281217.31, 2283540.3, 2284106.6, 2290549.32, 2298776.83, 2302504.86, 2306265.36, 2309025.16, 2313861.81, 2316495.56, 2334788.42, 2338832.4, 2350098.36, 2351143.07, 2358055.3, 2363601.47, 2370116.52, 2374660.64, 2376022.26, 2378726.55, 2386015.75, 2387950.2, 2401395.47, 2405395.22, 2411790.21, 2416051.17, 2427640.17, 2429310.9, 2432736.52, 2461468.35, 2462779.06, 2462978.28, 2470206.13, 2470581.29, 2480159.47, 2495489.25, 2495630.51, 2504400.71, 2508955.24, 2546123.78, 2554482.84, 2555031.18, 2565259.92, 2587953.32, 2594363.09, 2600519.26, 2609166.75, 2614202.3, 2623469.95, 2627910.75, 2644633.02, 2658725.29, 2678206.42, 2685351.81, 2727575.18, 2734277.1, 2739019.75, 2740057.14, 2752122.08, 2760346.71, 2762816.65, 2762861.41, 2766400.05, 2771397.17, 2771646.81, 2789469.45, 2811634.04, 2811646.85, 2819193.17, 2864170.61, 2906233.25, 2921709.71, 2939946.38, 2950198.64, 3004702.33, 3078162.08, 3224369.8, 3369068.99, 3436007.68, 3487986.89, 3526713.39, 3555371.03, 3556766.03, 3595903.2, 3676388.98, 3749057.69, 3766687.43, 3818686.45] [1.55400155e-04 3.10800311e-04 4.66200466e-04 ... 9.99689200e-01\n 9.99844600e-01 1.00000000e+00]\nHoliday_Flag\n[0, 1] [0.93006993 1.        ]\nTemperature\n[-2.06, 5.54, 6.23, 7.46, 9.51, 9.55, 10.09, 10.11, 10.24, 10.53, 10.91, 11.17, 11.29, 11.32, 12.19, 12.39, 12.98, 13.29, 13.64, 13.76, 14.02, 14.19, 14.31, 14.44, 14.48, 14.5, 14.56, 14.64, 14.84, 15.02, 15.12, 15.2, 15.22, 15.25, 15.33, 15.47, 15.56, 15.58, 15.64, 16.3, 16.5, 16.57, 16.6, 16.7, 16.81, 16.87, 16.94, 17.0, 17.05, 17.3, 17.46, 17.56, 17.91, 17.94, 17.95, 18.12, 18.14, 18.2, 18.3, 18.36, 18.49, 18.51, 18.55, 18.67, 18.73, 18.75, 18.76, 18.79, 18.8, 18.92, 19.03, 19.05, 19.29, 19.53, 19.55, 19.61, 19.63, 19.64, 19.66, 19.79, 19.83, 20.12, 20.28, 20.38, 20.39, 20.6, 20.61, 20.66, 20.67, 20.69, 20.7, 20.74, 20.79, 20.8, 20.84, 20.87, 20.96, 21.02, 21.07, 21.1, 21.13, 21.18, 21.33, 21.39, 21.52, 21.6, 21.64, 21.68, 21.69, 21.79, 21.81, 21.82, 21.84, 21.88, 22.0, 22.12, 22.16, 22.2, 22.3, 22.41, 22.43, 22.44, 22.47, 22.49, 22.5, 22.52, 22.53, 22.55, 22.62, 22.65, 22.69, 22.8, 22.91, 22.94, 22.96, 22.99, 23.01, 23.04, 23.05, 23.11, 23.21, 23.22, 23.24, 23.29, 23.33, 23.34, 23.35, 23.4, 23.41, 23.44, 23.46, 23.63, 23.64, 23.68, 23.69, 23.76, 23.78, 23.79, 23.82, 23.83, 23.87, 23.89, 23.9, 23.91, 23.92, 23.94, 23.97, 24.05, 24.07, 24.13, 24.16, 24.18, 24.2, 24.21, 24.24, 24.26, 24.27, 24.29, 24.3, 24.36, 24.41, 24.46, 24.48, 24.5, 24.69, 24.76, 24.78, 24.82, 24.83, 24.91, 25.01, 25.06, 25.07, 25.12, 25.13, 25.17, 25.19, 25.24, 25.3, 25.38, 25.4, 25.43, 25.53, 25.55, 25.56, 25.59, 25.61, 25.64, 25.69, 25.7, 25.71, 25.73, 25.8, 25.89, 25.9, 25.91, 25.92, 25.94, 25.97, 26.01, 26.04, 26.05, 26.09, 26.1, 26.15, 26.21, 26.23, 26.26, 26.28, 26.39, 26.41, 26.47, 26.49, 26.51, 26.54, 26.58, 26.6, 26.62, 26.64, 26.7, 26.73, 26.79, 26.8, 26.83, 26.86, 26.9, 27.01, 27.03, 27.11, 27.18, 27.19, 27.25, 27.26, 27.28, 27.31, 27.32, 27.39, 27.4, 27.41, 27.43, 27.49, 27.54, 27.6, 27.61, 27.64, 27.65, 27.69, 27.7, 27.73, 27.74, 27.79, 27.8, 27.81, 27.83, 27.84, 27.85, 27.86, 27.89, 27.92, 27.99, 28.06, 28.09, 28.11, 28.13, 28.14, 28.16, 28.17, 28.22, 28.24, 28.26, 28.36, 28.43, 28.49, 28.5, 28.57, 28.58, 28.6, 28.64, 28.65, 28.66, 28.7, 28.73, 28.82, 28.84, 28.85, 28.89, 28.99, 29.0, 29.03, 29.04, 29.09, 29.11, 29.16, 29.28, 29.3, 29.36, 29.44, 29.45, 29.53, 29.56, 29.59, 29.67, 29.71, 29.76, 29.81, 29.87, 29.88, 29.97, 29.98, 29.99, 30.01, 30.1, 30.13, 30.23, 30.24, 30.26, 30.27, 30.3, 30.33, 30.34, 30.35, 30.44, 30.45, 30.46, 30.51, 30.53, 30.54, 30.55, 30.58, 30.59, 30.64, 30.68, 30.72, 30.75, 30.76, 30.83, 30.87, 30.91, 30.95, 31.05, 31.07, 31.1, 31.11, 31.12, 31.19, 31.27, 31.34, 31.36, 31.39, 31.43, 31.44, 31.51, 31.53, 31.56, 31.58, 31.59, 31.6, 31.62, 31.64, 31.65, 31.7, 31.73, 31.75, 31.76, 31.77, 31.79, 31.82, 31.84, 31.85, 31.91, 31.92, 31.96, 32.02, 32.23, 32.24, 32.3, 32.31, 32.32, 32.36, 32.41, 32.42, 32.43, 32.44, 32.45, 32.46, 32.47, 32.49, 32.56, 32.61, 32.63, 32.65, 32.68, 32.7, 32.71, 32.76, 32.81, 32.83, 32.86, 32.87, 32.92, 32.93, 32.94, 32.99, 33.0, 33.02, 33.05, 33.06, 33.09, 33.11, 33.15, 33.16, 33.17, 33.18, 33.19, 33.2, 33.21, 33.24, 33.26, 33.29, 33.32, 33.33, 33.34, 33.35, 33.4, 33.41, 33.42, 33.43, 33.45, 33.47, 33.52, 33.59, 33.6, 33.73, 33.79, 33.8, 33.81, 33.82, 33.9, 33.91, 33.92, 33.94, 33.96, 33.98, 33.99, 34.04, 34.05, 34.11, 34.12, 34.13, 34.14, 34.19, 34.21, 34.23, 34.24, 34.27, 34.3, 34.32, 34.41, 34.42, 34.43, 34.48, 34.53, 34.54, 34.59, 34.6, 34.61, 34.68, 34.7, 34.74, 34.77, 34.78, 34.8, 34.83, 34.86, 34.89, 34.9, 34.93, 34.95, 34.98, 34.99, 35.04, 35.06, 35.1, 35.18, 35.21, 35.23, 35.25, 35.26, 35.3, 35.34, 35.36, 35.38, 35.4, 35.42, 35.44, 35.47, 35.49, 35.51, 35.52, 35.59, 35.68, 35.7, 35.71, 35.73, 35.75, 35.76, 35.77, 35.78, 35.8, 35.86, 35.88, 35.89, 35.92, 35.98, 36.0, 36.01, 36.04, 36.07, 36.09, 36.13, 36.16, 36.19, 36.22, 36.24, 36.25, 36.26, 36.28, 36.31, 36.32, 36.33, 36.34, 36.35, 36.37, 36.39, 36.4, 36.43, 36.44, 36.45, 36.46, 36.5, 36.51, 36.53, 36.54, 36.56, 36.57, 36.59, 36.6, 36.61, 36.62, 36.64, 36.65, 36.67, 36.7, 36.71, 36.73, 36.74, 36.75, 36.78, 36.82, 36.84, 36.85, 36.86, 36.88, 36.89, 36.9, 36.93, 36.94, 36.96, 36.97, 36.99, 37.0, 37.02, 37.07, 37.08, 37.09, 37.13, 37.16, 37.17, 37.19, 37.21, 37.24, 37.25, 37.27, 37.28, 37.29, 37.3, 37.33, 37.35, 37.43, 37.44, 37.47, 37.49, 37.51, 37.52, 37.62, 37.64, 37.65, 37.72, 37.74, 37.75, 37.77, 37.78, 37.79, 37.85, 37.86, 37.91, 37.93, 37.95, 38.01, 38.02, 38.04, 38.07, 38.09, 38.1, 38.15, 38.16, 38.24, 38.25, 38.26, 38.29, 38.34, 38.36, 38.37, 38.4, 38.42, 38.49, 38.5, 38.51, 38.53, 38.59, 38.61, 38.64, 38.65, 38.69, 38.7, 38.71, 38.77, 38.8, 38.89, 38.97, 38.99, 39.0, 39.05, 39.06, 39.07, 39.08, 39.1, 39.11, 39.16, 39.17, 39.18, 39.26, 39.28, 39.3, 39.32, 39.34, 39.38, 39.39, 39.42, 39.46, 39.51, 39.53, 39.57, 39.62, 39.63, 39.68, 39.69, 39.7, 39.75, 39.79, 39.81, 39.83, 39.87, 39.88, 39.89, 39.9, 39.91, 39.93, 39.94, 39.97, 39.98, 40.01, 40.05, 40.07, 40.08, 40.11, 40.14, 40.18, 40.19, 40.2, 40.22, 40.23, 40.26, 40.29, 40.3, 40.31, 40.32, 40.34, 40.43, 40.47, 40.5, 40.52, 40.54, 40.56, 40.57, 40.58, 40.59, 40.6, 40.65, 40.68, 40.69, 40.7, 40.71, 40.74, 40.75, 40.81, 40.82, 40.85, 40.88, 40.93, 40.94, 40.99, 41.04, 41.1, 41.11, 41.12, 41.13, 41.14, 41.16, 41.18, 41.19, 41.2, 41.25, 41.26, 41.28, 41.3, 41.31, 41.36, 41.37, 41.39, 41.4, 41.41, 41.42, 41.43, 41.44, 41.47, 41.48, 41.5, 41.55, 41.57, 41.59, 41.64, 41.66, 41.67, 41.72, 41.74, 41.76, 41.78, 41.8, 41.81, 41.82, 41.83, 41.85, 41.89, 41.92, 41.93, 41.97, 42.0, 42.05, 42.06, 42.09, 42.11, 42.15, 42.16, 42.17, 42.2, 42.24, 42.26, 42.27, 42.3, 42.31, 42.36, 42.38, 42.39, 42.4, 42.43, 42.44, 42.45, 42.46, 42.47, 42.49, 42.52, 42.53, 42.54, 42.55, 42.57, 42.58, 42.61, 42.62, 42.63, 42.64, 42.65, 42.7, 42.72, 42.74, 42.75, 42.76, 42.78, 42.81, 42.83, 42.85, 42.86, 42.87, 42.91, 42.95, 42.96, 43.01, 43.06, 43.07, 43.15, 43.17, 43.19, 43.21, 43.26, 43.29, 43.3, 43.32, 43.33, 43.38, 43.39, 43.41, 43.43, 43.45, 43.47, 43.49, 43.51, 43.52, 43.57, 43.58, 43.6, 43.61, 43.64, 43.67, 43.68, 43.69, 43.74, 43.76, 43.78, 43.8, 43.81, 43.82, 43.83, 43.85, 43.86, 43.88, 43.9, 43.92, 43.93, 43.95, 43.96, 44.03, 44.04, 44.1, 44.12, 44.13, 44.2, 44.22, 44.24, 44.25, 44.29, 44.3, 44.32, 44.34, 44.42, 44.43, 44.46, 44.47, 44.5, 44.55, 44.56, 44.57, 44.58, 44.61, 44.62, 44.64, 44.69, 44.7, 44.72, 44.73, 44.76, 44.8, 44.81, 44.82, 44.83, 44.86, 44.88, 44.9, 44.96, 44.97, 44.98, 44.99, 45.0, 45.01, 45.03, 45.12, 45.14, 45.16, 45.17, 45.19, 45.22, 45.23, 45.25, 45.26, 45.27, 45.29, 45.31, 45.32, 45.33, 45.34, 45.35, 45.38, 45.4, 45.41, 45.42, 45.45, 45.47, 45.48, 45.5, 45.51, 45.52, 45.54, 45.55, 45.56, 45.61, 45.62, 45.63, 45.64, 45.65, 45.66, 45.67, 45.68, 45.69, 45.71, 45.77, 45.8, 45.83, 45.84, 45.85, 45.86, 45.87, 45.88, 45.9, 45.91, 45.92, 45.95, 45.96, 45.97, 45.98, 45.99, 46.0, 46.03, 46.04, 46.06, 46.09, 46.1, 46.11, 46.12, 46.14, 46.15, 46.2, 46.21, 46.25, 46.28, 46.32, 46.33, 46.35, 46.38, 46.4, 46.41, 46.42, 46.49, 46.5, 46.51, 46.52, 46.53, 46.54, 46.56, 46.57, 46.6, 46.63, 46.65, 46.66, 46.67, 46.68, 46.7, 46.73, 46.74, 46.75, 46.78, 46.79, 46.8, 46.81, 46.84, 46.85, 46.87, 46.9, 46.94, 46.95, 46.97, 46.98, 46.99, 47.07, 47.09, 47.1, 47.11, 47.12, 47.13, 47.17, 47.19, 47.2, 47.21, 47.22, 47.26, 47.27, 47.28, 47.3, 47.31, 47.32, 47.34, 47.35, 47.41, 47.42, 47.43, 47.49, 47.51, 47.54, 47.55, 47.59, 47.6, 47.64, 47.65, 47.66, 47.69, 47.7, 47.74, 47.75, 47.76, 47.78, 47.8, 47.83, 47.87, 47.88, 47.89, 47.92, 47.93, 47.94, 47.95, 47.96, 47.99, 48.0, 48.01, 48.02, 48.04, 48.05, 48.07, 48.08, 48.09, 48.1, 48.16, 48.17, 48.19, 48.2, 48.21, 48.22, 48.25, 48.26, 48.27, 48.28, 48.29, 48.3, 48.33, 48.36, 48.43, 48.45, 48.46, 48.48, 48.5, 48.53, 48.54, 48.56, 48.57, 48.58, 48.61, 48.62, 48.63, 48.65, 48.67, 48.68, 48.69, 48.71, 48.72, 48.73, 48.74, 48.76, 48.77, 48.79, 48.81, 48.85, 48.86, 48.87, 48.88, 48.89, 48.91, 48.92, 48.93, 48.94, 49.0, 49.01, 49.03, 49.04, 49.09, 49.11, 49.12, 49.14, 49.15, 49.2, 49.26, 49.27, 49.3, 49.31, 49.33, 49.36, 49.38, 49.4, 49.41, 49.43, 49.44, 49.45, 49.47, 49.5, 49.56, 49.59, 49.6, 49.61, 49.63, 49.65, 49.66, 49.67, 49.68, 49.69, 49.7, 49.73, 49.76, 49.79, 49.84, 49.86, 49.87, 49.89, 49.9, 49.91, 49.96, 49.97, 49.98, 49.99, 50.01, 50.02, 50.04, 50.06, 50.07, 50.08, 50.11, 50.13, 50.14, 50.15, 50.19, 50.21, 50.22, 50.23, 50.24, 50.25, 50.26, 50.27, 50.29, 50.3, 50.33, 50.38, 50.39, 50.43, 50.44, 50.46, 50.49, 50.5, 50.52, 50.53, 50.55, 50.56, 50.58, 50.59, 50.6, 50.62, 50.63, 50.64, 50.65, 50.66, 50.72, 50.74, 50.75, 50.76, 50.78, 50.79, 50.81, 50.82, 50.87, 50.93, 50.95, 50.96, 50.97, 50.98, 50.99, 51.0, 51.04, 51.07, 51.08, 51.12, 51.13, 51.14, 51.17, 51.18, 51.24, 51.26, 51.29, 51.3, 51.31, 51.32, 51.33, 51.34, 51.4, 51.41, 51.42, 51.43, 51.45, 51.48, 51.49, 51.5, 51.51, 51.52, 51.56, 51.6, 51.61, 51.62, 51.63, 51.64, 51.65, 51.66, 51.67, 51.7, 51.71, 51.72, 51.74, 51.77, 51.78, 51.8, 51.81, 51.83, 51.86, 51.92, 51.93, 51.96, 52.02, 52.05, 52.06, 52.08, 52.1, 52.12, 52.14, 52.16, 52.19, 52.2, 52.21, 52.22, 52.23, 52.24, 52.25, 52.27, 52.3, 52.33, 52.34, 52.36, 52.39, 52.4, 52.42, 52.43, 52.45, 52.47, 52.5, 52.52, 52.55, 52.56, 52.59, 52.65, 52.68, 52.7, 52.72, 52.74, 52.75, 52.76, 52.77, 52.79, 52.8, 52.81, 52.82, 52.83, 52.86, 52.88, 52.89, 52.9, 52.91, 52.96, 52.99, 53.03, 53.04, 53.05, 53.1, 53.11, 53.13, 53.14, 53.15, 53.18, 53.2, 53.25, 53.27, 53.3, 53.33, 53.35, 53.39, 53.4, 53.41, 53.44, 53.47, 53.49, 53.53, 53.55, 53.56, 53.57, 53.59, 53.62, 53.63, 53.66, 53.67, 53.68, 53.69, 53.71, 53.73, 53.74, 53.76, 53.77, 53.79, 53.82, 53.87, 53.9, 53.91, 53.94, 53.95, 53.96, 54.03, 54.07, 54.08, 54.09, 54.11, 54.12, 54.13, 54.16, 54.17, 54.18, 54.2, 54.23, 54.24, 54.26, 54.28, 54.29, 54.31, 54.32, 54.34, 54.36, 54.37, 54.38, 54.4, 54.41, 54.42, 54.43, 54.44, 54.46, 54.47, 54.49, 54.5, 54.51, 54.52, 54.53, 54.54, 54.56, 54.57, 54.58, 54.61, 54.62, 54.63, 54.65, 54.66, 54.68, 54.69, 54.72, 54.73, 54.75, 54.76, 54.81, 54.82, 54.83, 54.89, 54.9, 54.94, 54.95, 54.96, 54.97, 54.98, 55.0, 55.01, 55.03, 55.04, 55.06, 55.07, 55.09, 55.1, 55.12, 55.13, 55.16, 55.19, 55.2, 55.21, 55.23, 55.24, 55.28, 55.3, 55.31, 55.33, 55.34, 55.37, 55.4, 55.41, 55.43, 55.46, 55.47, 55.5, 55.53, 55.58, 55.61, 55.63, 55.66, 55.69, 55.7, 55.72, 55.73, 55.74, 55.75, 55.76, 55.78, 55.82, 55.83, 55.85, 55.89, 55.9, 55.91, 55.92, 55.99, 56.0, 56.01, 56.02, 56.06, 56.08, 56.09, 56.1, 56.11, 56.12, 56.2, 56.22, 56.23, 56.26, 56.28, 56.32, 56.33, 56.36, 56.38, 56.43, 56.44, 56.47, 56.48, 56.49, 56.5, 56.51, 56.53, 56.54, 56.55, 56.56, 56.58, 56.6, 56.61, 56.62, 56.63, 56.65, 56.66, 56.67, 56.68, 56.69, 56.71, 56.72, 56.73, 56.74, 56.75, 56.77, 56.79, 56.8, 56.81, 56.82, 56.85, 56.86, 56.87, 56.89, 56.91, 56.94, 56.96, 56.97, 56.99, 57.06, 57.07, 57.1, 57.11, 57.13, 57.14, 57.15, 57.16, 57.17, 57.18, 57.19, 57.2, 57.21, 57.23, 57.24, 57.25, 57.29, 57.3, 57.35, 57.36, 57.39, 57.44, 57.45, 57.46, 57.48, 57.53, 57.55, 57.56, 57.57, 57.58, 57.59, 57.6, 57.61, 57.62, 57.63, 57.65, 57.66, 57.69, 57.71, 57.72, 57.73, 57.75, 57.77, 57.78, 57.79, 57.8, 57.82, 57.83, 57.84, 57.85, 57.86, 57.88, 57.89, 57.93, 57.94, 57.95, 58.0, 58.02, 58.04, 58.06, 58.09, 58.11, 58.18, 58.19, 58.2, 58.21, 58.22, 58.24, 58.28, 58.29, 58.31, 58.34, 58.35, 58.36, 58.37, 58.38, 58.39, 58.4, 58.41, 58.42, 58.43, 58.46, 58.47, 58.48, 58.49, 58.5, 58.51, 58.53, 58.54, 58.56, 58.58, 58.59, 58.61, 58.62, 58.63, 58.65, 58.66, 58.68, 58.69, 58.7, 58.71, 58.74, 58.75, 58.76, 58.79, 58.8, 58.81, 58.82, 58.83, 58.84, 58.85, 58.86, 58.88, 58.9, 58.92, 58.93, 58.95, 58.97, 58.99, 59.0, 59.01, 59.07, 59.08, 59.09, 59.1, 59.11, 59.12, 59.15, 59.16, 59.17, 59.18, 59.21, 59.25, 59.27, 59.29, 59.3, 59.31, 59.33, 59.35, 59.38, 59.39, 59.41, 59.42, 59.45, 59.46, 59.51, 59.52, 59.53, 59.54, 59.56, 59.57, 59.58, 59.59, 59.6, 59.61, 59.62, 59.64, 59.66, 59.67, 59.68, 59.69, 59.74, 59.77, 59.8, 59.81, 59.84, 59.85, 59.86, 59.87, 59.89, 59.9, 59.91, 59.93, 59.94, 59.97, 59.98, 59.99, 60.04, 60.05, 60.06, 60.07, 60.09, 60.1, 60.11, 60.12, 60.13, 60.14, 60.18, 60.19, 60.21, 60.22, 60.23, 60.24, 60.26, 60.27, 60.28, 60.29, 60.31, 60.32, 60.34, 60.35, 60.38, 60.42, 60.43, 60.44, 60.45, 60.46, 60.48, 60.49, 60.51, 60.52, 60.58, 60.61, 60.62, 60.65, 60.66, 60.67, 60.68, 60.69, 60.7, 60.71, 60.72, 60.74, 60.77, 60.8, 60.82, 60.88, 60.9, 60.91, 60.92, 60.93, 60.94, 60.95, 60.96, 60.97, 60.98, 60.99, 61.01, 61.02, 61.04, 61.05, 61.06, 61.08, 61.09, 61.1, 61.11, 61.12, 61.13, 61.14, 61.24, 61.25, 61.26, 61.27, 61.29, 61.3, 61.31, 61.32, 61.33, 61.35, 61.36, 61.37, 61.39, 61.4, 61.41, 61.44, 61.46, 61.47, 61.48, 61.5, 61.51, 61.53, 61.58, 61.59, 61.62, 61.64, 61.65, 61.7, 61.71, 61.74, 61.76, 61.77, 61.79, 61.81, 61.83, 61.86, 61.87, 61.88, 61.9, 61.92, 61.93, 61.94, 61.95, 61.96, 61.97, 61.99, 62.01, 62.03, 62.05, 62.06, 62.07, 62.08, 62.09, 62.1, 62.11, 62.15, 62.16, 62.17, 62.18, 62.19, 62.2, 62.21, 62.25, 62.26, 62.27, 62.28, 62.29, 62.3, 62.31, 62.32, 62.34, 62.35, 62.36, 62.37, 62.39, 62.4, 62.41, 62.49, 62.52, 62.53, 62.54, 62.57, 62.59, 62.61, 62.62, 62.63, 62.64, 62.66, 62.67, 62.68, 62.7, 62.71, 62.72, 62.73, 62.74, 62.75, 62.76, 62.78, 62.79, 62.8, 62.84, 62.86, 62.87, 62.9, 62.93, 62.94, 62.96, 62.97, 62.98, 62.99, 63.03, 63.04, 63.05, 63.07, 63.08, 63.1, 63.11, 63.13, 63.16, 63.18, 63.19, 63.2, 63.21, 63.23, 63.25, 63.26, 63.27, 63.29, 63.3, 63.31, 63.32, 63.34, 63.35, 63.36, 63.38, 63.39, 63.41, 63.43, 63.44, 63.45, 63.49, 63.5, 63.55, 63.56, 63.58, 63.59, 63.6, 63.61, 63.63, 63.64, 63.66, 63.67, 63.68, 63.69, 63.71, 63.74, 63.75, 63.78, 63.79, 63.8, 63.81, 63.82, 63.83, 63.89, 63.9, 63.91, 63.93, 63.96, 63.97, 63.99, 64.0, 64.01, 64.02, 64.05, 64.08, 64.09, 64.1, 64.11, 64.12, 64.13, 64.16, 64.17, 64.19, 64.2, 64.21, 64.22, 64.23, 64.28, 64.3, 64.31, 64.33, 64.34, 64.36, 64.37, 64.41, 64.42, 64.43, 64.44, 64.45, 64.46, 64.47, 64.5, 64.52, 64.53, 64.54, 64.55, 64.6, 64.61, 64.63, 64.64, 64.67, 64.69, 64.7, 64.73, 64.74, 64.75, 64.76, 64.77, 64.78, 64.79, 64.8, 64.82, 64.83, 64.84, 64.87, 64.88, 64.89, 64.9, 64.91, 64.92, 64.94, 64.95, 64.96, 64.97, 64.99, 65.0, 65.01, 65.02, 65.03, 65.04, 65.06, 65.07, 65.08, 65.1, 65.11, 65.13, 65.14, 65.15, 65.16, 65.17, 65.19, 65.21, 65.23, 65.24, 65.25, 65.28, 65.29, 65.3, 65.32, 65.33, 65.35, 65.36, 65.4, 65.41, 65.42, 65.43, 65.44, 65.45, 65.46, 65.47, 65.48, 65.49, 65.53, 65.54, 65.56, 65.6, 65.62, 65.64, 65.65, 65.66, 65.68, 65.69, 65.7, 65.71, 65.74, 65.76, 65.77, 65.79, 65.8, 65.81, 65.83, 65.86, 65.87, 65.88, 65.93, 65.95, 65.96, 65.97, 65.99, 66.0, 66.01, 66.03, 66.04, 66.07, 66.08, 66.09, 66.1, 66.11, 66.13, 66.14, 66.15, 66.16, 66.17, 66.18, 66.19, 66.22, 66.23, 66.24, 66.25, 66.27, 66.28, 66.3, 66.32, 66.33, 66.34, 66.36, 66.38, 66.4, 66.41, 66.42, 66.43, 66.49, 66.5, 66.51, 66.53, 66.55, 66.56, 66.57, 66.58, 66.59, 66.61, 66.65, 66.66, 66.69, 66.73, 66.74, 66.75, 66.76, 66.77, 66.79, 66.8, 66.82, 66.83, 66.84, 66.88, 66.9, 66.91, 66.93, 66.96, 66.97, 66.98, 66.99, 67.0, 67.01, 67.03, 67.04, 67.05, 67.06, 67.08, 67.09, 67.11, 67.13, 67.14, 67.15, 67.16, 67.17, 67.18, 67.2, 67.21, 67.23, 67.24, 67.27, 67.28, 67.31, 67.32, 67.33, 67.36, 67.38, 67.39, 67.4, 67.41, 67.42, 67.43, 67.45, 67.47, 67.48, 67.51, 67.53, 67.54, 67.55, 67.59, 67.61, 67.63, 67.64, 67.65, 67.66, 67.68, 67.69, 67.7, 67.73, 67.75, 67.76, 67.77, 67.79, 67.81, 67.84, 67.86, 67.87, 67.88, 67.89, 67.91, 67.92, 67.93, 67.96, 67.97, 68.0, 68.01, 68.03, 68.04, 68.07, 68.08, 68.09, 68.1, 68.11, 68.12, 68.13, 68.14, 68.15, 68.18, 68.19, 68.2, 68.22, 68.23, 68.26, 68.27, 68.28, 68.3, 68.32, 68.34, 68.36, 68.37, 68.4, 68.43, 68.44, 68.49, 68.5, 68.52, 68.53, 68.54, 68.55, 68.57, 68.58, 68.59, 68.6, 68.61, 68.62, 68.64, 68.65, 68.66, 68.68, 68.69, 68.7, 68.71, 68.72, 68.74, 68.75, 68.76, 68.78, 68.79, 68.8, 68.83, 68.84, 68.85, 68.88, 68.89, 68.9, 68.91, 68.93, 68.96, 68.97, 68.98, 68.99, 69.0, 69.01, 69.02, 69.03, 69.06, 69.07, 69.08, 69.09, 69.1, 69.11, 69.12, 69.14, 69.16, 69.17, 69.18, 69.19, 69.21, 69.22, 69.23, 69.24, 69.26, 69.27, 69.31, 69.32, 69.34, 69.36, 69.37, 69.39, 69.41, 69.42, 69.44, 69.45, 69.46, 69.47, 69.48, 69.49, 69.51, 69.52, 69.53, 69.54, 69.55, 69.56, 69.57, 69.59, 69.64, 69.65, 69.66, 69.68, 69.7, 69.71, 69.72, 69.74, 69.75, 69.76, 69.78, 69.79, 69.8, 69.82, 69.83, 69.84, 69.86, 69.87, 69.88, 69.9, 69.92, 69.93, 69.94, 69.96, 69.97, 69.99, 70.0, 70.01, 70.02, 70.03, 70.04, 70.05, 70.06, 70.07, 70.09, 70.1, 70.12, 70.13, 70.15, 70.17, 70.18, 70.19, 70.2, 70.23, 70.24, 70.27, 70.28, 70.29, 70.3, 70.31, 70.32, 70.33, 70.34, 70.35, 70.37, 70.38, 70.39, 70.4, 70.41, 70.42, 70.43, 70.44, 70.45, 70.48, 70.49, 70.5, 70.51, 70.53, 70.55, 70.56, 70.58, 70.59, 70.6, 70.61, 70.63, 70.65, 70.66, 70.67, 70.69, 70.71, 70.72, 70.75, 70.77, 70.79, 70.8, 70.81, 70.82, 70.83, 70.84, 70.85, 70.86, 70.87, 70.9, 70.91, 70.92, 70.93, 70.94, 70.96, 70.97, 70.98, 70.99, 71.01, 71.04, 71.05, 71.06, 71.07, 71.08, 71.09, 71.1, 71.12, 71.13, 71.14, 71.17, 71.18, 71.2, 71.22, 71.24, 71.25, 71.27, 71.28, 71.29, 71.3, 71.31, 71.33, 71.34, 71.35, 71.36, 71.37, 71.39, 71.4, 71.41, 71.42, 71.44, 71.45, 71.46, 71.48, 71.5, 71.51, 71.53, 71.54, 71.56, 71.57, 71.59, 71.6, 71.61, 71.64, 71.67, 71.68, 71.7, 71.73, 71.74, 71.76, 71.79, 71.81, 71.82, 71.83, 71.85, 71.89, 71.9, 71.91, 71.93, 71.97, 71.99, 72.01, 72.02, 72.03, 72.04, 72.05, 72.06, 72.08, 72.12, 72.14, 72.15, 72.17, 72.19, 72.2, 72.21, 72.22, 72.24, 72.26, 72.29, 72.3, 72.34, 72.35, 72.36, 72.38, 72.4, 72.42, 72.44, 72.49, 72.52, 72.53, 72.54, 72.55, 72.56, 72.59, 72.6, 72.61, 72.62, 72.63, 72.66, 72.67, 72.7, 72.71, 72.72, 72.73, 72.74, 72.76, 72.78, 72.79, 72.81, 72.83, 72.86, 72.87, 72.88, 72.89, 72.9, 72.91, 72.93, 72.94, 72.95, 72.96, 72.97, 72.98, 72.99, 73.01, 73.02, 73.03, 73.05, 73.07, 73.08, 73.12, 73.13, 73.14, 73.16, 73.17, 73.19, 73.2, 73.21, 73.23, 73.25, 73.26, 73.29, 73.3, 73.31, 73.34, 73.37, 73.38, 73.39, 73.4, 73.41, 73.43, 73.44, 73.45, 73.47, 73.48, 73.49, 73.51, 73.52, 73.54, 73.55, 73.56, 73.57, 73.58, 73.6, 73.62, 73.63, 73.64, 73.65, 73.66, 73.67, 73.68, 73.7, 73.72, 73.74, 73.75, 73.76, 73.77, 73.79, 73.8, 73.84, 73.85, 73.87, 73.88, 73.9, 73.94, 73.95, 73.96, 73.99, 74.0, 74.02, 74.04, 74.06, 74.09, 74.1, 74.11, 74.14, 74.15, 74.16, 74.17, 74.19, 74.2, 74.21, 74.22, 74.23, 74.24, 74.25, 74.26, 74.28, 74.29, 74.32, 74.34, 74.36, 74.37, 74.39, 74.42, 74.43, 74.44, 74.45, 74.47, 74.48, 74.49, 74.54, 74.57, 74.58, 74.6, 74.61, 74.64, 74.66, 74.67, 74.68, 74.69, 74.71, 74.72, 74.73, 74.74, 74.75, 74.78, 74.79, 74.8, 74.83, 74.84, 74.85, 74.86, 74.88, 74.9, 74.92, 74.93, 74.95, 74.97, 74.98, 75.0, 75.01, 75.04, 75.09, 75.11, 75.13, 75.14, 75.16, 75.17, 75.19, 75.2, 75.22, 75.24, 75.27, 75.31, 75.32, 75.33, 75.34, 75.35, 75.36, 75.37, 75.4, 75.42, 75.48, 75.5, 75.52, 75.53, 75.54, 75.55, 75.56, 75.57, 75.58, 75.59, 75.62, 75.64, 75.65, 75.68, 75.69, 75.7, 75.71, 75.76, 75.79, 75.8, 75.81, 75.83, 75.85, 75.87, 75.88, 75.89, 75.91, 75.93, 75.95, 75.98, 76.0, 76.01, 76.02, 76.03, 76.04, 76.07, 76.08, 76.1, 76.11, 76.12, 76.14, 76.17, 76.2, 76.22, 76.25, 76.3, 76.34, 76.35, 76.36, 76.38, 76.41, 76.42, 76.44, 76.45, 76.47, 76.48, 76.49, 76.51, 76.53, 76.54, 76.55, 76.56, 76.57, 76.58, 76.61, 76.64, 76.65, 76.67, 76.68, 76.71, 76.72, 76.73, 76.74, 76.8, 76.86, 76.87, 76.9, 76.91, 76.95, 76.97, 77.0, 77.02, 77.04, 77.08, 77.1, 77.12, 77.14, 77.15, 77.16, 77.17, 77.18, 77.2, 77.22, 77.27, 77.3, 77.31, 77.33, 77.34, 77.36, 77.37, 77.38, 77.39, 77.41, 77.42, 77.43, 77.44, 77.45, 77.46, 77.47, 77.49, 77.53, 77.55, 77.56, 77.62, 77.64, 77.66, 77.67, 77.7, 77.72, 77.75, 77.76, 77.78, 77.79, 77.8, 77.82, 77.83, 77.84, 77.9, 77.91, 77.92, 77.93, 77.94, 77.95, 77.97, 77.99, 78.01, 78.02, 78.04, 78.08, 78.09, 78.1, 78.11, 78.14, 78.15, 78.16, 78.19, 78.22, 78.23, 78.24, 78.26, 78.27, 78.29, 78.3, 78.31, 78.32, 78.33, 78.35, 78.36, 78.37, 78.38, 78.39, 78.44, 78.45, 78.46, 78.47, 78.49, 78.5, 78.51, 78.52, 78.53, 78.55, 78.58, 78.59, 78.6, 78.62, 78.64, 78.65, 78.68, 78.69, 78.72, 78.75, 78.78, 78.79, 78.82, 78.83, 78.85, 78.86, 78.87, 78.89, 78.91, 78.93, 78.94, 78.95, 78.97, 78.98, 79.02, 79.03, 79.04, 79.05, 79.07, 79.09, 79.14, 79.15, 79.17, 79.18, 79.2, 79.22, 79.23, 79.24, 79.29, 79.3, 79.32, 79.35, 79.36, 79.37, 79.4, 79.41, 79.44, 79.45, 79.46, 79.47, 79.49, 79.51, 79.54, 79.58, 79.61, 79.64, 79.65, 79.66, 79.69, 79.72, 79.75, 79.78, 79.79, 79.81, 79.83, 79.84, 79.86, 79.87, 79.93, 79.94, 79.97, 80.05, 80.06, 80.11, 80.14, 80.17, 80.19, 80.21, 80.22, 80.23, 80.26, 80.27, 80.32, 80.33, 80.34, 80.35, 80.37, 80.38, 80.39, 80.4, 80.42, 80.43, 80.44, 80.48, 80.49, 80.5, 80.56, 80.57, 80.58, 80.62, 80.69, 80.7, 80.71, 80.74, 80.75, 80.78, 80.79, 80.83, 80.84, 80.87, 80.88, 80.89, 80.91, 80.93, 80.94, 80.99, 81.02, 81.04, 81.05, 81.06, 81.1, 81.12, 81.13, 81.14, 81.16, 81.19, 81.2, 81.21, 81.22, 81.24, 81.25, 81.26, 81.28, 81.31, 81.32, 81.33, 81.35, 81.37, 81.38, 81.39, 81.41, 81.46, 81.47, 81.5, 81.51, 81.52, 81.53, 81.55, 81.56, 81.57, 81.61, 81.63, 81.64, 81.65, 81.67, 81.69, 81.71, 81.72, 81.74, 81.75, 81.76, 81.77, 81.78, 81.79, 81.81, 81.83, 81.84, 81.85, 81.87, 81.88, 81.91, 81.93, 81.95, 81.96, 81.99, 82.0, 82.01, 82.04, 82.05, 82.06, 82.08, 82.09, 82.1, 82.11, 82.13, 82.14, 82.15, 82.2, 82.21, 82.27, 82.28, 82.29, 82.3, 82.32, 82.33, 82.35, 82.36, 82.39, 82.41, 82.43, 82.45, 82.46, 82.47, 82.5, 82.51, 82.52, 82.57, 82.58, 82.59, 82.6, 82.64, 82.66, 82.68, 82.69, 82.7, 82.74, 82.75, 82.76, 82.77, 82.8, 82.81, 82.82, 82.84, 82.89, 82.92, 82.93, 82.95, 82.96, 82.97, 82.98, 82.99, 83.0, 83.01, 83.03, 83.04, 83.07, 83.09, 83.11, 83.12, 83.13, 83.14, 83.15, 83.17, 83.21, 83.23, 83.24, 83.26, 83.27, 83.31, 83.32, 83.36, 83.39, 83.4, 83.43, 83.44, 83.47, 83.49, 83.51, 83.52, 83.55, 83.57, 83.58, 83.59, 83.61, 83.63, 83.64, 83.66, 83.68, 83.71, 83.72, 83.74, 83.75, 83.79, 83.8, 83.81, 83.82, 83.83, 83.84, 83.86, 83.89, 83.9, 83.91, 83.94, 83.95, 83.96, 83.98, 84.0, 84.01, 84.04, 84.05, 84.06, 84.11, 84.13, 84.14, 84.15, 84.16, 84.17, 84.19, 84.2, 84.23, 84.25, 84.26, 84.29, 84.31, 84.32, 84.33, 84.34, 84.36, 84.37, 84.4, 84.41, 84.43, 84.45, 84.47, 84.49, 84.5, 84.51, 84.54, 84.57, 84.59, 84.6, 84.62, 84.66, 84.67, 84.69, 84.71, 84.72, 84.73, 84.76, 84.77, 84.79, 84.83, 84.84, 84.85, 84.88, 84.9, 84.91, 84.92, 84.93, 84.94, 84.97, 84.99, 85.02, 85.03, 85.05, 85.06, 85.1, 85.11, 85.13, 85.15, 85.17, 85.18, 85.19, 85.2, 85.21, 85.22, 85.26, 85.32, 85.33, 85.38, 85.43, 85.46, 85.49, 85.51, 85.52, 85.53, 85.54, 85.55, 85.56, 85.61, 85.63, 85.66, 85.69, 85.71, 85.72, 85.73, 85.75, 85.77, 85.78, 85.79, 85.8, 85.81, 85.83, 85.85, 85.89, 85.94, 85.96, 86.01, 86.02, 86.03, 86.05, 86.06, 86.09, 86.1, 86.11, 86.13, 86.14, 86.15, 86.18, 86.19, 86.2, 86.21, 86.24, 86.26, 86.3, 86.31, 86.32, 86.33, 86.36, 86.37, 86.41, 86.42, 86.43, 86.46, 86.49, 86.53, 86.54, 86.55, 86.6, 86.61, 86.64, 86.65, 86.68, 86.71, 86.74, 86.75, 86.83, 86.84, 86.85, 86.87, 86.91, 86.94, 86.96, 86.97, 87.0, 87.01, 87.02, 87.04, 87.05, 87.08, 87.09, 87.12, 87.14, 87.16, 87.17, 87.18, 87.24, 87.26, 87.27, 87.35, 87.36, 87.4, 87.41, 87.43, 87.47, 87.5, 87.51, 87.52, 87.53, 87.54, 87.55, 87.57, 87.64, 87.65, 87.66, 87.67, 87.69, 87.7, 87.71, 87.73, 87.75, 87.8, 87.83, 87.84, 87.86, 87.9, 87.93, 87.96, 88.0, 88.02, 88.05, 88.06, 88.07, 88.09, 88.1, 88.12, 88.16, 88.27, 88.3, 88.36, 88.37, 88.4, 88.44, 88.45, 88.48, 88.49, 88.52, 88.54, 88.55, 88.57, 88.58, 88.59, 88.64, 88.65, 88.66, 88.7, 88.83, 88.88, 88.89, 88.93, 88.95, 89.04, 89.05, 89.06, 89.08, 89.09, 89.1, 89.12, 89.13, 89.14, 89.15, 89.16, 89.18, 89.26, 89.33, 89.34, 89.35, 89.42, 89.43, 89.46, 89.51, 89.53, 89.57, 89.58, 89.62, 89.64, 89.65, 89.67, 89.78, 89.8, 89.81, 89.83, 89.85, 89.86, 89.88, 89.9, 89.92, 89.94, 90.04, 90.07, 90.11, 90.16, 90.22, 90.23, 90.27, 90.3, 90.32, 90.37, 90.38, 90.45, 90.47, 90.49, 90.61, 90.69, 90.76, 90.78, 90.82, 90.84, 90.94, 91.03, 91.04, 91.05, 91.07, 91.1, 91.11, 91.17, 91.18, 91.22, 91.36, 91.44, 91.45, 91.46, 91.49, 91.52, 91.56, 91.57, 91.58, 91.59, 91.61, 91.63, 91.65, 91.74, 91.77, 91.8, 91.94, 91.98, 92.02, 92.07, 92.13, 92.32, 92.44, 92.51, 92.71, 92.81, 92.83, 92.89, 92.95, 93.19, 93.21, 93.29, 93.34, 93.47, 93.52, 93.66, 93.95, 94.0, 94.11, 94.22, 94.55, 94.61, 94.87, 95.28, 95.36, 95.57, 95.61, 95.75, 95.88, 95.89, 95.91, 96.0, 96.22, 96.31, 96.44, 96.46, 96.79, 96.93, 97.04, 97.17, 97.18, 97.6, 97.66, 98.15, 98.43, 99.2, 99.22, 99.66, 100.07, 100.14] [1.55400155e-04 3.10800311e-04 4.66200466e-04 ... 9.99689200e-01\n 9.99844600e-01 1.00000000e+00]\nFuel_Price\n[2.472, 2.513, 2.514, 2.52, 2.533, 2.539, 2.54, 2.542, 2.545, 2.548, 2.55, 2.561, 2.562, 2.565, 2.567, 2.572, 2.573, 2.574, 2.577, 2.578, 2.58, 2.582, 2.584, 2.586, 2.59, 2.594, 2.595, 2.596, 2.598, 2.601, 2.602, 2.603, 2.604, 2.606, 2.608, 2.612, 2.615, 2.619, 2.62, 2.621, 2.623, 2.624, 2.625, 2.627, 2.633, 2.635, 2.637, 2.64, 2.642, 2.644, 2.645, 2.65, 2.653, 2.654, 2.655, 2.664, 2.666, 2.667, 2.668, 2.669, 2.671, 2.674, 2.68, 2.681, 2.684, 2.689, 2.69, 2.691, 2.692, 2.694, 2.698, 2.699, 2.7, 2.701, 2.704, 2.705, 2.706, 2.707, 2.708, 2.711, 2.712, 2.713, 2.715, 2.716, 2.717, 2.718, 2.719, 2.72, 2.723, 2.725, 2.727, 2.728, 2.729, 2.731, 2.732, 2.733, 2.735, 2.736, 2.737, 2.74, 2.741, 2.742, 2.743, 2.745, 2.747, 2.748, 2.75, 2.752, 2.753, 2.754, 2.755, 2.756, 2.758, 2.759, 2.762, 2.764, 2.765, 2.766, 2.767, 2.769, 2.77, 2.771, 2.773, 2.776, 2.777, 2.778, 2.779, 2.78, 2.781, 2.782, 2.783, 2.784, 2.786, 2.787, 2.788, 2.791, 2.792, 2.793, 2.795, 2.796, 2.797, 2.8, 2.802, 2.805, 2.806, 2.808, 2.809, 2.81, 2.812, 2.813, 2.814, 2.815, 2.817, 2.818, 2.819, 2.82, 2.825, 2.826, 2.828, 2.829, 2.83, 2.831, 2.834, 2.835, 2.836, 2.837, 2.84, 2.841, 2.842, 2.843, 2.844, 2.845, 2.846, 2.847, 2.849, 2.85, 2.852, 2.853, 2.854, 2.857, 2.86, 2.863, 2.868, 2.869, 2.87, 2.871, 2.872, 2.875, 2.877, 2.878, 2.882, 2.884, 2.885, 2.886, 2.887, 2.891, 2.895, 2.899, 2.902, 2.903, 2.906, 2.908, 2.909, 2.91, 2.911, 2.913, 2.915, 2.917, 2.919, 2.921, 2.923, 2.924, 2.925, 2.931, 2.932, 2.933, 2.934, 2.935, 2.936, 2.938, 2.939, 2.94, 2.941, 2.942, 2.943, 2.946, 2.948, 2.949, 2.95, 2.954, 2.955, 2.957, 2.958, 2.96, 2.961, 2.962, 2.963, 2.966, 2.971, 2.972, 2.973, 2.974, 2.976, 2.978, 2.98, 2.981, 2.982, 2.983, 2.987, 2.989, 2.992, 2.995, 2.996, 2.999, 3.0, 3.001, 3.004, 3.006, 3.008, 3.009, 3.01, 3.011, 3.013, 3.014, 3.016, 3.017, 3.021, 3.022, 3.028, 3.03, 3.031, 3.033, 3.034, 3.037, 3.038, 3.039, 3.041, 3.042, 3.043, 3.044, 3.045, 3.046, 3.047, 3.049, 3.05, 3.051, 3.053, 3.054, 3.055, 3.056, 3.057, 3.058, 3.062, 3.065, 3.07, 3.077, 3.08, 3.083, 3.084, 3.086, 3.087, 3.09, 3.091, 3.094, 3.095, 3.096, 3.098, 3.1, 3.101, 3.103, 3.105, 3.109, 3.112, 3.113, 3.116, 3.119, 3.12, 3.123, 3.125, 3.127, 3.129, 3.13, 3.132, 3.133, 3.138, 3.139, 3.14, 3.141, 3.145, 3.147, 3.148, 3.149, 3.15, 3.153, 3.157, 3.158, 3.159, 3.161, 3.162, 3.164, 3.172, 3.173, 3.176, 3.177, 3.179, 3.181, 3.186, 3.187, 3.191, 3.193, 3.2, 3.203, 3.205, 3.215, 3.223, 3.224, 3.225, 3.227, 3.229, 3.23, 3.231, 3.232, 3.234, 3.236, 3.237, 3.239, 3.24, 3.242, 3.243, 3.245, 3.254, 3.255, 3.256, 3.257, 3.26, 3.261, 3.262, 3.263, 3.266, 3.268, 3.273, 3.274, 3.275, 3.281, 3.282, 3.283, 3.285, 3.286, 3.287, 3.288, 3.29, 3.294, 3.297, 3.299, 3.301, 3.305, 3.306, 3.308, 3.309, 3.311, 3.312, 3.313, 3.322, 3.328, 3.329, 3.331, 3.332, 3.336, 3.341, 3.342, 3.346, 3.348, 3.351, 3.353, 3.354, 3.355, 3.356, 3.358, 3.36, 3.361, 3.362, 3.367, 3.371, 3.372, 3.374, 3.378, 3.38, 3.381, 3.389, 3.391, 3.392, 3.393, 3.398, 3.4, 3.402, 3.404, 3.406, 3.407, 3.409, 3.411, 3.413, 3.414, 3.415, 3.416, 3.417, 3.42, 3.421, 3.422, 3.424, 3.428, 3.43, 3.433, 3.435, 3.437, 3.439, 3.44, 3.441, 3.443, 3.445, 3.448, 3.452, 3.459, 3.461, 3.462, 3.467, 3.469, 3.47, 3.473, 3.475, 3.476, 3.477, 3.479, 3.48, 3.481, 3.483, 3.485, 3.486, 3.487, 3.488, 3.489, 3.49, 3.491, 3.492, 3.493, 3.494, 3.495, 3.498, 3.499, 3.501, 3.502, 3.503, 3.504, 3.505, 3.506, 3.509, 3.51, 3.511, 3.512, 3.513, 3.514, 3.521, 3.523, 3.524, 3.526, 3.527, 3.528, 3.529, 3.53, 3.532, 3.533, 3.534, 3.536, 3.537, 3.538, 3.54, 3.541, 3.542, 3.543, 3.545, 3.546, 3.547, 3.548, 3.55, 3.551, 3.552, 3.553, 3.554, 3.555, 3.556, 3.558, 3.561, 3.563, 3.564, 3.566, 3.567, 3.568, 3.569, 3.57, 3.571, 3.574, 3.575, 3.576, 3.577, 3.578, 3.579, 3.58, 3.581, 3.582, 3.583, 3.585, 3.586, 3.587, 3.589, 3.592, 3.594, 3.595, 3.596, 3.597, 3.599, 3.6, 3.601, 3.603, 3.604, 3.605, 3.606, 3.608, 3.61, 3.611, 3.613, 3.616, 3.617, 3.618, 3.619, 3.62, 3.622, 3.623, 3.624, 3.625, 3.627, 3.629, 3.63, 3.631, 3.633, 3.634, 3.636, 3.637, 3.638, 3.64, 3.641, 3.644, 3.645, 3.646, 3.647, 3.648, 3.651, 3.652, 3.654, 3.655, 3.657, 3.659, 3.66, 3.661, 3.662, 3.663, 3.664, 3.666, 3.667, 3.668, 3.669, 3.671, 3.674, 3.675, 3.677, 3.681, 3.682, 3.683, 3.684, 3.685, 3.686, 3.688, 3.689, 3.69, 3.692, 3.693, 3.694, 3.695, 3.697, 3.698, 3.699, 3.701, 3.702, 3.703, 3.704, 3.705, 3.706, 3.707, 3.709, 3.711, 3.713, 3.716, 3.717, 3.719, 3.72, 3.721, 3.722, 3.723, 3.724, 3.726, 3.727, 3.73, 3.732, 3.734, 3.735, 3.737, 3.738, 3.739, 3.74, 3.741, 3.742, 3.743, 3.744, 3.746, 3.747, 3.748, 3.749, 3.75, 3.751, 3.752, 3.755, 3.756, 3.757, 3.758, 3.759, 3.76, 3.763, 3.764, 3.765, 3.767, 3.769, 3.77, 3.771, 3.772, 3.775, 3.776, 3.778, 3.779, 3.781, 3.784, 3.786, 3.787, 3.788, 3.789, 3.793, 3.794, 3.795, 3.796, 3.797, 3.798, 3.801, 3.802, 3.803, 3.804, 3.805, 3.807, 3.808, 3.809, 3.81, 3.811, 3.812, 3.813, 3.814, 3.815, 3.816, 3.818, 3.819, 3.82, 3.821, 3.823, 3.824, 3.826, 3.827, 3.828, 3.829, 3.831, 3.833, 3.834, 3.835, 3.837, 3.84, 3.842, 3.843, 3.845, 3.848, 3.85, 3.851, 3.854, 3.858, 3.862, 3.863, 3.864, 3.866, 3.867, 3.868, 3.87, 3.871, 3.872, 3.873, 3.874, 3.875, 3.876, 3.877, 3.879, 3.88, 3.881, 3.882, 3.884, 3.886, 3.888, 3.889, 3.891, 3.892, 3.893, 3.895, 3.898, 3.899, 3.9, 3.901, 3.903, 3.906, 3.907, 3.909, 3.911, 3.913, 3.915, 3.916, 3.917, 3.918, 3.919, 3.92, 3.921, 3.922, 3.924, 3.925, 3.927, 3.93, 3.933, 3.934, 3.935, 3.936, 3.937, 3.942, 3.947, 3.948, 3.95, 3.951, 3.953, 3.957, 3.962, 3.963, 3.964, 3.966, 3.969, 3.972, 3.973, 3.979, 3.981, 3.983, 3.985, 3.988, 3.989, 3.99, 3.991, 3.995, 3.996, 3.997, 4.0, 4.002, 4.003, 4.004, 4.014, 4.018, 4.02, 4.021, 4.023, 4.025, 4.026, 4.027, 4.029, 4.031, 4.034, 4.038, 4.044, 4.046, 4.054, 4.055, 4.056, 4.058, 4.061, 4.062, 4.066, 4.069, 4.071, 4.076, 4.078, 4.087, 4.088, 4.089, 4.093, 4.095, 4.098, 4.101, 4.103, 4.11, 4.111, 4.117, 4.121, 4.124, 4.125, 4.127, 4.132, 4.133, 4.134, 4.143, 4.144, 4.151, 4.153, 4.158, 4.163, 4.169, 4.17, 4.171, 4.178, 4.186, 4.187, 4.192, 4.193, 4.202, 4.203, 4.211, 4.222, 4.25, 4.254, 4.273, 4.277, 4.282, 4.288, 4.293, 4.294, 4.301, 4.308, 4.449, 4.468] [1.55400155e-04 3.10800311e-04 2.48640249e-03 2.64180264e-03\n 2.79720280e-03 2.95260295e-03 3.26340326e-03 3.41880342e-03\n 3.57420357e-03 5.74980575e-03 6.52680653e-03 8.85780886e-03\n 9.01320901e-03 1.11888112e-02 1.13442113e-02 1.42968143e-02\n 1.47630148e-02 1.52292152e-02 1.74048174e-02 1.75602176e-02\n 1.81818182e-02 2.03574204e-02 2.06682207e-02 2.12898213e-02\n 2.16006216e-02 2.19114219e-02 2.20668221e-02 2.22222222e-02\n 2.25330225e-02 2.26884227e-02 2.28438228e-02 2.50194250e-02\n 2.51748252e-02 2.53302253e-02 2.75058275e-02 2.78166278e-02\n 2.79720280e-02 3.06138306e-02 3.12354312e-02 3.18570319e-02\n 3.40326340e-02 3.62082362e-02 3.83838384e-02 4.05594406e-02\n 4.27350427e-02 4.30458430e-02 4.55322455e-02 4.80186480e-02\n 5.05050505e-02 5.06604507e-02 5.09712510e-02 5.12820513e-02\n 5.34576535e-02 5.45454545e-02 5.47008547e-02 5.70318570e-02\n 5.74980575e-02 6.01398601e-02 6.29370629e-02 6.51126651e-02\n 6.58896659e-02 6.65112665e-02 6.66666667e-02 6.71328671e-02\n 6.83760684e-02 7.05516706e-02 7.17948718e-02 7.24164724e-02\n 7.52136752e-02 7.53690754e-02 7.58352758e-02 7.75446775e-02\n 7.81662782e-02 7.86324786e-02 7.89432789e-02 8.14296814e-02\n 8.22066822e-02 8.29836830e-02 8.53146853e-02 8.60916861e-02\n 8.67132867e-02 8.74902875e-02 8.88888889e-02 9.21522922e-02\n 9.46386946e-02 9.58818959e-02 9.80574981e-02 1.02408702e-01\n 1.03030303e-01 1.06138306e-01 1.06449106e-01 1.09867910e-01\n 1.10489510e-01 1.11111111e-01 1.14219114e-01 1.14685315e-01\n 1.17793318e-01 1.18104118e-01 1.19347319e-01 1.19658120e-01\n 1.20124320e-01 1.20745921e-01 1.21212121e-01 1.22610723e-01\n 1.23543124e-01 1.24164724e-01 1.24786325e-01 1.25407925e-01\n 1.26340326e-01 1.27117327e-01 1.27894328e-01 1.28205128e-01\n 1.28826729e-01 1.31623932e-01 1.33022533e-01 1.33799534e-01\n 1.34421134e-01 1.35975136e-01 1.36130536e-01 1.36285936e-01\n 1.39393939e-01 1.43900544e-01 1.45609946e-01 1.47785548e-01\n 1.48562549e-01 1.49184149e-01 1.49961150e-01 1.53846154e-01\n 1.55244755e-01 1.55710956e-01 1.56487956e-01 1.58974359e-01\n 1.59129759e-01 1.59440559e-01 1.60994561e-01 1.61460761e-01\n 1.63325563e-01 1.64724165e-01 1.66899767e-01 1.68764569e-01\n 1.69696970e-01 1.70318570e-01 1.71250971e-01 1.74358974e-01\n 1.75291375e-01 1.78243978e-01 1.79020979e-01 1.81196581e-01\n 1.81662782e-01 1.81818182e-01 1.82284382e-01 1.83993784e-01\n 1.84459984e-01 1.85236985e-01 1.86169386e-01 1.86946387e-01\n 1.87878788e-01 1.90986791e-01 1.91452991e-01 1.92074592e-01\n 1.93473193e-01 1.95337995e-01 1.96270396e-01 1.98445998e-01\n 1.98756799e-01 2.00466200e-01 2.01087801e-01 2.02486402e-01\n 2.03885004e-01 2.06060606e-01 2.06837607e-01 2.07614608e-01\n 2.08702409e-01 2.09479409e-01 2.10878011e-01 2.12121212e-01\n 2.12276612e-01 2.12742813e-01 2.16161616e-01 2.17094017e-01\n 2.18026418e-01 2.18181818e-01 2.19891220e-01 2.22843823e-01\n 2.23310023e-01 2.23776224e-01 2.24242424e-01 2.24708625e-01\n 2.25641026e-01 2.26573427e-01 2.27195027e-01 2.27505828e-01\n 2.29059829e-01 2.31235431e-01 2.31546232e-01 2.32012432e-01\n 2.32944833e-01 2.34498834e-01 2.35275835e-01 2.36363636e-01\n 2.37296037e-01 2.37762238e-01 2.38383838e-01 2.39005439e-01\n 2.39627040e-01 2.40248640e-01 2.41647242e-01 2.43201243e-01\n 2.44755245e-01 2.46309246e-01 2.46930847e-01 2.48018648e-01\n 2.48484848e-01 2.49261849e-01 2.49883450e-01 2.50505051e-01\n 2.50971251e-01 2.52369852e-01 2.52836053e-01 2.55322455e-01\n 2.56254856e-01 2.57498057e-01 2.58896659e-01 2.60295260e-01\n 2.62470862e-01 2.62937063e-01 2.63403263e-01 2.64024864e-01\n 2.64491064e-01 2.65112665e-01 2.65423465e-01 2.66045066e-01\n 2.68065268e-01 2.69153069e-01 2.71484071e-01 2.72882673e-01\n 2.73504274e-01 2.74281274e-01 2.74902875e-01 2.75524476e-01\n 2.76146076e-01 2.76612277e-01 2.79254079e-01 2.79720280e-01\n 2.80808081e-01 2.81740482e-01 2.82517483e-01 2.85625486e-01\n 2.86091686e-01 2.88267288e-01 2.89199689e-01 2.89355089e-01\n 2.89665890e-01 2.90287490e-01 2.91064491e-01 2.91996892e-01\n 2.92463092e-01 2.93084693e-01 2.93706294e-01 2.94327894e-01\n 2.97125097e-01 2.97746698e-01 2.98212898e-01 2.99145299e-01\n 3.01787102e-01 3.03185703e-01 3.03807304e-01 3.06759907e-01\n 3.07692308e-01 3.08624709e-01 3.09246309e-01 3.09557110e-01\n 3.10489510e-01 3.11111111e-01 3.11732712e-01 3.12509713e-01\n 3.14374514e-01 3.14996115e-01 3.15462315e-01 3.15928516e-01\n 3.18104118e-01 3.19502720e-01 3.20435120e-01 3.22144522e-01\n 3.23076923e-01 3.23698524e-01 3.23853924e-01 3.24786325e-01\n 3.26029526e-01 3.26495726e-01 3.26961927e-01 3.28671329e-01\n 3.29137529e-01 3.32867133e-01 3.33799534e-01 3.34887335e-01\n 3.35819736e-01 3.36285936e-01 3.37218337e-01 3.38150738e-01\n 3.39393939e-01 3.39860140e-01 3.40326340e-01 3.40792541e-01\n 3.42035742e-01 3.42657343e-01 3.42812743e-01 3.43745144e-01\n 3.44366744e-01 3.45299145e-01 3.46697747e-01 3.48407148e-01\n 3.52136752e-01 3.52758353e-01 3.53224553e-01 3.55089355e-01\n 3.56487956e-01 3.57420357e-01 3.57886558e-01 3.58818959e-01\n 3.62703963e-01 3.64102564e-01 3.65190365e-01 3.65345765e-01\n 3.65967366e-01 3.66899767e-01 3.67676768e-01 3.68453768e-01\n 3.69852370e-01 3.70007770e-01 3.70940171e-01 3.71250971e-01\n 3.72183372e-01 3.72494172e-01 3.74669775e-01 3.77156177e-01\n 3.79797980e-01 3.80264180e-01 3.81196581e-01 3.81351981e-01\n 3.83527584e-01 3.84149184e-01 3.84459984e-01 3.85392385e-01\n 3.86169386e-01 3.86324786e-01 3.87412587e-01 3.87723388e-01\n 3.88344988e-01 3.90054390e-01 3.90675991e-01 3.91142191e-01\n 3.91919192e-01 3.92851593e-01 3.93317793e-01 3.93628594e-01\n 3.93939394e-01 3.96114996e-01 3.96891997e-01 3.97358197e-01\n 3.98601399e-01 4.00310800e-01 4.00466200e-01 4.04040404e-01\n 4.04817405e-01 4.05594406e-01 4.06526807e-01 4.06993007e-01\n 4.07925408e-01 4.08857809e-01 4.09168609e-01 4.10722611e-01\n 4.12898213e-01 4.13209013e-01 4.13364413e-01 4.15540016e-01\n 4.15695416e-01 4.17249417e-01 4.17871018e-01 4.20046620e-01\n 4.20512821e-01 4.23465423e-01 4.23620824e-01 4.24553225e-01\n 4.25174825e-01 4.25485625e-01 4.27661228e-01 4.30769231e-01\n 4.31857032e-01 4.34032634e-01 4.36519037e-01 4.36829837e-01\n 4.39005439e-01 4.39316239e-01 4.40093240e-01 4.40404040e-01\n 4.40559441e-01 4.42890443e-01 4.43512044e-01 4.45687646e-01\n 4.46620047e-01 4.46775447e-01 4.47086247e-01 4.47241647e-01\n 4.47552448e-01 4.48174048e-01 4.50349650e-01 4.51437451e-01\n 4.51903652e-01 4.52369852e-01 4.55011655e-01 4.55944056e-01\n 4.56565657e-01 4.58741259e-01 4.59052059e-01 4.61227661e-01\n 4.61538462e-01 4.61693862e-01 4.63869464e-01 4.64180264e-01\n 4.64491064e-01 4.65112665e-01 4.65423465e-01 4.68531469e-01\n 4.68842269e-01 4.69463869e-01 4.69930070e-01 4.70862471e-01\n 4.72882673e-01 4.73504274e-01 4.73659674e-01 4.75835276e-01\n 4.76301476e-01 4.76923077e-01 4.78477078e-01 4.78632479e-01\n 4.79254079e-01 4.82051282e-01 4.84226884e-01 4.84537685e-01\n 4.86247086e-01 4.86868687e-01 4.87645688e-01 4.88267288e-01\n 4.90442890e-01 4.91064491e-01 4.91219891e-01 4.91996892e-01\n 4.92618493e-01 4.93550894e-01 4.94483294e-01 4.95571096e-01\n 4.96037296e-01 4.96969697e-01 4.97902098e-01 4.98057498e-01\n 4.98212898e-01 4.98679099e-01 5.00077700e-01 5.00233100e-01\n 5.03807304e-01 5.05982906e-01 5.06915307e-01 5.07070707e-01\n 5.09246309e-01 5.09557110e-01 5.10489510e-01 5.13131313e-01\n 5.13908314e-01 5.14219114e-01 5.14685315e-01 5.15306915e-01\n 5.17793318e-01 5.17948718e-01 5.18259518e-01 5.18881119e-01\n 5.19502720e-01 5.20124320e-01 5.22455322e-01 5.23387723e-01\n 5.23543124e-01 5.24164724e-01 5.24941725e-01 5.25252525e-01\n 5.27583528e-01 5.27894328e-01 5.28515929e-01 5.29292929e-01\n 5.32556333e-01 5.32711733e-01 5.32867133e-01 5.33177933e-01\n 5.33799534e-01 5.36752137e-01 5.37839938e-01 5.40015540e-01\n 5.40637141e-01 5.41103341e-01 5.42501943e-01 5.42812743e-01\n 5.43123543e-01 5.47319347e-01 5.51670552e-01 5.53846154e-01\n 5.54467754e-01 5.55089355e-01 5.55710956e-01 5.57731158e-01\n 5.59285159e-01 5.62548563e-01 5.62703963e-01 5.63636364e-01\n 5.64102564e-01 5.66122766e-01 5.66744367e-01 5.68764569e-01\n 5.70318570e-01 5.70784771e-01 5.72494172e-01 5.74669775e-01\n 5.75757576e-01 5.76379176e-01 5.77622378e-01 5.78710179e-01\n 5.79020979e-01 5.79487179e-01 5.82595183e-01 5.84770785e-01\n 5.85858586e-01 5.86480186e-01 5.89588190e-01 5.90831391e-01\n 5.91608392e-01 5.92851593e-01 5.95493395e-01 5.96425796e-01\n 5.97358197e-01 6.00621601e-01 6.03885004e-01 6.04506605e-01\n 6.06682207e-01 6.06837607e-01 6.07770008e-01 6.08236208e-01\n 6.08702409e-01 6.09168609e-01 6.09790210e-01 6.11033411e-01\n 6.12742813e-01 6.13986014e-01 6.14918415e-01 6.15540016e-01\n 6.16006216e-01 6.16472416e-01 6.20823621e-01 6.22222222e-01\n 6.23465423e-01 6.24087024e-01 6.24708625e-01 6.26107226e-01\n 6.28282828e-01 6.28593629e-01 6.29526030e-01 6.30458430e-01\n 6.30924631e-01 6.31235431e-01 6.32789433e-01 6.34188034e-01\n 6.35275835e-01 6.35742036e-01 6.39005439e-01 6.40248640e-01\n 6.41181041e-01 6.44755245e-01 6.47397047e-01 6.47863248e-01\n 6.49261849e-01 6.50038850e-01 6.50971251e-01 6.51903652e-01\n 6.57187257e-01 6.57964258e-01 6.59984460e-01 6.60916861e-01\n 6.61693862e-01 6.64024864e-01 6.70085470e-01 6.71173271e-01\n 6.71639472e-01 6.72571873e-01 6.73504274e-01 6.75058275e-01\n 6.75990676e-01 6.78632479e-01 6.80808081e-01 6.80963481e-01\n 6.81740482e-01 6.83139083e-01 6.83605284e-01 6.84537685e-01\n 6.85780886e-01 6.86402486e-01 6.86713287e-01 6.87801088e-01\n 6.88422688e-01 6.92618493e-01 6.93550894e-01 6.94017094e-01\n 6.96658897e-01 6.97125097e-01 6.98057498e-01 6.98989899e-01\n 6.99456099e-01 6.99922300e-01 7.02097902e-01 7.03807304e-01\n 7.06915307e-01 7.07226107e-01 7.07847708e-01 7.10023310e-01\n 7.12509713e-01 7.12665113e-01 7.13752914e-01 7.14374514e-01\n 7.14840715e-01 7.15617716e-01 7.16860917e-01 7.18259518e-01\n 7.20435120e-01 7.22766123e-01 7.23232323e-01 7.24941725e-01\n 7.26184926e-01 7.27272727e-01 7.27583528e-01 7.27894328e-01\n 7.28360528e-01 7.29914530e-01 7.30069930e-01 7.30536131e-01\n 7.33333333e-01 7.33954934e-01 7.35664336e-01 7.38150738e-01\n 7.40481740e-01 7.42035742e-01 7.42346542e-01 7.42812743e-01\n 7.43278943e-01 7.45454545e-01 7.46542347e-01 7.49184149e-01\n 7.51048951e-01 7.52136752e-01 7.53535354e-01 7.54312354e-01\n 7.54778555e-01 7.55400155e-01 7.56954157e-01 7.60062160e-01\n 7.61149961e-01 7.62082362e-01 7.63170163e-01 7.63947164e-01\n 7.66588967e-01 7.67832168e-01 7.68764569e-01 7.69852370e-01\n 7.70318570e-01 7.70784771e-01 7.72027972e-01 7.72960373e-01\n 7.73271173e-01 7.75602176e-01 7.76379176e-01 7.77156177e-01\n 7.78243978e-01 7.78865579e-01 7.79797980e-01 7.80264180e-01\n 7.81041181e-01 7.82905983e-01 7.83527584e-01 7.83993784e-01\n 7.84459984e-01 7.85703186e-01 7.87723388e-01 7.90054390e-01\n 7.93006993e-01 7.95182595e-01 7.95648796e-01 7.97824398e-01\n 7.98756799e-01 7.99689200e-01 8.00310800e-01 8.02486402e-01\n 8.02952603e-01 8.05594406e-01 8.06060606e-01 8.06526807e-01\n 8.07459207e-01 8.08857809e-01 8.09945610e-01 8.13364413e-01\n 8.13830614e-01 8.16161616e-01 8.18648019e-01 8.20979021e-01\n 8.21756022e-01 8.22222222e-01 8.24397824e-01 8.25796426e-01\n 8.26573427e-01 8.27505828e-01 8.28127428e-01 8.29992230e-01\n 8.31701632e-01 8.32634033e-01 8.33100233e-01 8.33721834e-01\n 8.36519037e-01 8.38073038e-01 8.38850039e-01 8.39316239e-01\n 8.39782440e-01 8.40559441e-01 8.40714841e-01 8.41491841e-01\n 8.42424242e-01 8.46309246e-01 8.46775447e-01 8.49572650e-01\n 8.51126651e-01 8.52059052e-01 8.52991453e-01 8.53613054e-01\n 8.54234654e-01 8.55011655e-01 8.55633256e-01 8.56254856e-01\n 8.56565657e-01 8.58275058e-01 8.58741259e-01 8.59362859e-01\n 8.59984460e-01 8.60295260e-01 8.61227661e-01 8.62781663e-01\n 8.63714064e-01 8.64646465e-01 8.67754468e-01 8.68376068e-01\n 8.69308469e-01 8.70707071e-01 8.72882673e-01 8.73815074e-01\n 8.74902875e-01 8.75058275e-01 8.76767677e-01 8.81274281e-01\n 8.82517483e-01 8.82983683e-01 8.83605284e-01 8.85003885e-01\n 8.88733489e-01 8.89510490e-01 8.90132090e-01 8.91686092e-01\n 8.94483294e-01 8.96658897e-01 8.97125097e-01 8.97902098e-01\n 8.98368298e-01 8.99611500e-01 9.00233100e-01 9.02408702e-01\n 9.03030303e-01 9.04118104e-01 9.04273504e-01 9.06138306e-01\n 9.06915307e-01 9.07847708e-01 9.08003108e-01 9.08624709e-01\n 9.09246309e-01 9.09712510e-01 9.09867910e-01 9.10800311e-01\n 9.11421911e-01 9.12043512e-01 9.12665113e-01 9.13597514e-01\n 9.14374514e-01 9.15306915e-01 9.16083916e-01 9.16860917e-01\n 9.17793318e-01 9.18570319e-01 9.19658120e-01 9.20279720e-01\n 9.22144522e-01 9.22921523e-01 9.23543124e-01 9.24475524e-01\n 9.25097125e-01 9.26651127e-01 9.27272727e-01 9.28826729e-01\n 9.30691531e-01 9.31313131e-01 9.31779332e-01 9.32711733e-01\n 9.33333333e-01 9.34110334e-01 9.35508936e-01 9.37218337e-01\n 9.38150738e-01 9.38616939e-01 9.40015540e-01 9.40947941e-01\n 9.41880342e-01 9.42501943e-01 9.43745144e-01 9.44677545e-01\n 9.46231546e-01 9.46853147e-01 9.48562549e-01 9.50116550e-01\n 9.50582751e-01 9.51515152e-01 9.52758353e-01 9.53535354e-01\n 9.55244755e-01 9.55866356e-01 9.57420357e-01 9.58352758e-01\n 9.58818959e-01 9.59440559e-01 9.60217560e-01 9.60994561e-01\n 9.61616162e-01 9.62237762e-01 9.62859363e-01 9.63791764e-01\n 9.64257964e-01 9.65345765e-01 9.65811966e-01 9.66278166e-01\n 9.67210567e-01 9.67832168e-01 9.68764569e-01 9.69696970e-01\n 9.70163170e-01 9.70629371e-01 9.71250971e-01 9.71717172e-01\n 9.73271173e-01 9.74203574e-01 9.74669775e-01 9.75135975e-01\n 9.75602176e-01 9.76689977e-01 9.77311577e-01 9.78243978e-01\n 9.79797980e-01 9.80419580e-01 9.81041181e-01 9.81662782e-01\n 9.82128982e-01 9.82750583e-01 9.83216783e-01 9.83682984e-01\n 9.85236985e-01 9.85858586e-01 9.86480186e-01 9.87878788e-01\n 9.89432789e-01 9.90054390e-01 9.90675991e-01 9.91142191e-01\n 9.91608392e-01 9.92540793e-01 9.93473193e-01 9.94405594e-01\n 9.94871795e-01 9.95337995e-01 9.95804196e-01 9.96736597e-01\n 9.97668998e-01 9.98135198e-01 9.99067599e-01 1.00000000e+00]\nCPI\n[126.064, 126.0766452, 126.0854516, 126.0892903, 126.1019355, 126.1069032, 126.1119032, 126.114, 126.1145806, 126.1266, 126.1283548, 126.1360645, 126.1392, 126.1454667, 126.1498065, 126.1518, 126.1602258, 126.1843871, 126.1900333, 126.2085484, 126.2346, 126.2791667, 126.2898, 126.3266774, 126.3805667, 126.3815484, 126.4364194, 126.4420645, 126.4713333, 126.4912903, 126.4962581, 126.5262857, 126.5461613, 126.5522857, 126.5621, 126.5782857, 126.6019032, 126.6034839, 126.6042857, 126.6050645, 126.6066452, 126.6072, 126.6692667, 126.7313333, 126.7934, 126.8794839, 126.9835806, 127.0876774, 127.1917742, 127.3009355, 127.4404839, 127.5800323, 127.7195806, 127.859129, 127.99525, 128.13, 128.26475, 128.3995, 128.5121935, 128.6160645, 128.7199355, 128.8238065, 128.9107333, 128.9553, 128.9998667, 129.0357097, 129.0432, 129.0444333, 129.0490323, 129.0623548, 129.0663, 129.0756774, 129.089, 129.0894, 129.1125, 129.1338387, 129.1507742, 129.1677097, 129.1846452, 129.2015806, 129.2405806, 129.2832581, 129.3259355, 129.3686129, 129.4306, 129.5183333, 129.6060667, 129.6938, 129.7706452, 129.7821613, 129.7936774, 129.8051935, 129.8167097, 129.8268333, 129.8364, 129.8459667, 129.8555333, 129.8980645, 129.9845484, 130.0710323, 130.1575161, 130.244, 130.2792258, 130.3144516, 130.3496774, 130.3849032, 130.4546207, 130.5502069, 130.6457931, 130.683, 130.7012903, 130.7195806, 130.7196333, 130.737871, 130.7413793, 130.7561613, 130.7562667, 130.7909677, 130.7929, 130.8261935, 130.8295333, 130.8381613, 130.8853548, 130.8896774, 130.8966452, 130.9325484, 130.9592258, 130.9670968, 130.9776667, 131.0103333, 131.0287742, 131.0375484, 131.043, 131.0756667, 131.0983226, 131.108, 131.1083333, 131.1173333, 131.1266667, 131.136, 131.1453333, 131.1499677, 131.1930968, 131.5279032, 131.5866129, 131.637, 131.686, 131.735, 131.784, 131.8242903, 131.863129, 131.9019677, 131.9408065, 131.9809, 132.0226667, 132.0644333, 132.1062, 132.152129, 132.2230323, 132.2939355, 132.3648387, 132.4357419, 132.4733333, 132.4976, 132.5218667, 132.5461333, 132.5667742, 132.5825806, 132.5983871, 132.6141935, 132.63, 132.6616129, 132.6764, 132.6804516, 132.6932258, 132.7248387, 132.7477419, 132.7516667, 132.7564516, 132.7566667, 132.7568, 132.7619333, 132.7633548, 132.7670667, 132.8150323, 132.8170968, 132.8369333, 132.8708387, 132.8823226, 132.9172, 132.9245806, 132.9510645, 132.9783226, 133.0285161, 133.1059677, 133.1834194, 133.260871, 133.3701429, 133.4921429, 133.6141429, 133.7361429, 133.8492258, 133.9587419, 134.0682581, 134.1777742, 134.2784667, 134.3571, 134.4357333, 134.5143667, 134.593, 134.6803871, 134.7677742, 134.8551613, 134.9425484, 135.0837333, 135.2652667, 135.3524608, 135.4113076, 135.4468, 135.4657781, 135.5195191, 135.5732602, 135.6270013, 135.6283333, 135.6682247, 135.7073618, 135.7464988, 135.7837419, 135.7856359, 135.82725, 135.8721667, 135.8738387, 135.9170833, 135.962, 135.9639355, 136.010394, 136.0540323, 136.0796521, 136.144129, 136.1489101, 136.183129, 136.2136129, 136.2181682, 136.2440968, 136.2745806, 136.2874263, 136.3145, 136.3243393, 136.3483143, 136.367, 136.3722893, 136.3962643, 136.4179827, 136.4195, 136.4366924, 136.4554021, 136.4618065, 136.4666667, 136.472, 136.4741118, 136.475129, 136.4788, 136.4884516, 136.4909333, 136.4928214, 136.5017742, 136.5030667, 136.5150968, 136.5249182, 136.5255714, 136.5292811, 136.5335161, 136.557015, 136.5883871, 136.5891118, 136.597273, 136.6075714, 136.6212085, 136.6277321, 136.6297571, 136.6317821, 136.6338071, 136.6401935, 136.6432581, 136.665265, 136.688871, 136.6895714, 136.698129, 136.7332569, 136.7375484, 136.753, 136.7715714, 136.7862258, 136.803477, 136.8349032, 136.8564194, 136.8870657, 136.9598387, 136.9706544, 137.0542431, 137.0632581, 137.1378318, 137.1666774, 137.2511849, 137.2583103, 137.3411034, 137.3764439, 137.4238966, 137.5017028, 137.5066897, 137.5843871, 137.6269617, 137.6552903, 137.7261935, 137.7398929, 137.7970968, 137.8478929, 137.868, 137.9230667, 137.9558929, 137.9781333, 138.0332, 138.0638929, 138.0882667, 138.1065806, 138.1101935, 138.1138065, 138.1174194, 138.1295333, 138.1437742, 138.1629, 138.1646952, 138.1735806, 138.1857097, 138.1962667, 138.2033871, 138.2296333, 138.2331935, 138.2475036, 138.263, 138.2814516, 138.3303119, 138.3771935, 138.4131202, 138.4729355, 138.4959286, 138.5673, 138.587106, 138.6534, 138.6782834, 138.7281613, 138.7395, 138.7694608, 138.8256, 138.8336129, 138.8606382, 138.9117, 139.0028333, 139.1832917, 139.36375, 139.5442083, 139.7006325, 139.7969712, 139.8933099, 139.9896486, 140.0859873, 140.1289205, 140.1629528, 140.196985, 140.2310173, 140.2735, 140.32725, 140.381, 140.4111613, 140.4127857, 140.4217857, 140.4279758, 140.4307857, 140.43475, 140.4397857, 140.4447903, 140.4616048, 140.4700795, 140.4784194, 140.528765, 140.5874505, 140.6461359, 140.7048214, 140.8086118, 140.9124021, 141.0161924, 141.1199827, 141.2140357, 141.3007857, 141.3875357, 141.4742857, 141.55478, 141.6269332, 141.6990864, 141.7712396, 141.8433929, 141.9015262, 141.9596595, 142.0177929, 142.0759262, 142.0970115, 142.1032776, 142.1095438, 142.1158099, 142.1292548, 142.1606464, 142.1705634, 142.1916279, 142.1920381, 142.2126924, 142.2157385, 142.2234298, 142.2337569, 142.2548214, 142.3105933, 142.4054482, 142.500303, 142.5938833, 142.6798167, 142.7624113, 142.76575, 142.8516833, 142.8633629, 142.9376167, 181.6468154, 181.6612792, 181.6620359, 181.6772564, 181.6924769, 181.7596377, 181.8538486, 181.8657537, 181.8711898, 181.9718697, 181.982317, 182.0347816, 182.0464181, 182.0774691, 182.0779857, 182.1201566, 182.1628441, 182.2389876, 182.2569603, 182.2604411, 182.3187801, 182.3509895, 182.3806, 182.4315571, 182.4415378, 182.4424199, 182.517732, 182.5320862, 182.54459, 182.5519538, 182.5714479, 182.598178, 182.5983058, 182.6042922, 182.6104063, 182.6165205, 182.622509, 182.6226346, 182.6585782, 182.6676154, 182.6696737, 182.7168385, 182.7640032, 182.7832769, 182.8106203, 182.8558685, 182.8989385, 182.9011166, 182.9193368, 182.9463648, 182.9916129, 183.1800955, 183.4408542, 183.7016129, 183.9371353, 184.1625632, 184.3879911, 184.613419, 184.809719, 184.9943679, 185.1790167, 185.3636656, 185.5339821, 185.6486923, 185.6684673, 185.6719333, 185.7545, 185.7919609, 185.8029526, 185.8603077, 185.9119885, 185.9374378, 185.9661154, 186.032016, 186.0719231, 186.1399808, 186.2177885, 186.2955962, 186.3734038, 186.4512115, 186.5093071, 186.5641172, 186.6189274, 186.6737376, 186.8024, 187.0295321, 187.2566641, 187.4837962, 187.6917481, 187.7846197, 187.8774913, 187.9703629, 188.0632345, 188.1983654, 188.3504, 188.5024346, 188.6544692, 188.7979349, 188.9299752, 189.0620155, 189.1940558, 189.3260962, 189.3816974, 189.4000734, 189.4185259, 189.4214733, 189.422658, 189.4452425, 189.4533931, 189.4642725, 189.467827, 189.4882603, 189.4904116, 189.5168505, 189.5231276, 189.5312483, 189.5340998, 189.575127, 189.6018023, 189.6122277, 189.6125456, 189.6190057, 189.6628845, 189.6695049, 189.6901012, 189.7048215, 189.7076048, 189.7195417, 189.734262, 189.7372075, 189.774698, 189.8424834, 189.9368504, 190.0069881, 190.0990028, 190.1714927, 190.2611552, 190.2948237, 190.3284922, 190.3359973, 190.3621607, 190.3958293, 190.4618964, 190.4688287, 190.5363213, 190.5713264, 190.6107463, 190.6738241, 190.6851712, 190.7595962, 190.7763218, 190.8138013, 190.8623087, 190.8680064, 190.9070184, 190.9222115, 190.951728, 190.9741069, 190.9764167, 190.9931437, 190.9964377, 190.9964479, 191.0028096, 191.0091712, 191.0121805, 191.0155329, 191.0299731, 191.0303376, 191.0312172, 191.0411474, 191.0646096, 191.0992462, 191.1338827, 191.1430189, 191.1626135, 191.16409, 191.1655664, 191.1670428, 191.1685192, 191.2284919, 191.2557002, 191.3448865, 191.3683815, 191.461281, 191.4784939, 191.5731924, 191.5776756, 191.667891, 191.69985, 191.7625895, 191.8567038, 191.8572881, 191.9178331, 191.9647167, 192.0116004, 192.0135577, 192.058484, 192.1237981, 192.1704115, 192.1964844, 192.2691707, 192.3088989, 192.3272654, 192.3308542, 192.3418571, 192.4225954, 192.5234638, 192.6243322, 192.7252006, 192.826069, 192.831317, 192.8365651, 192.8418131, 192.8470612, 192.9034759, 192.9982655, 193.0930552, 193.1878448, 193.3125484, 193.5120367, 193.711525, 193.9110133, 194.1105017, 194.2500634, 194.3796374, 194.5092113, 194.6387853, 194.7419707, 194.8099713, 194.8779718, 194.9459724, 195.0261012, 195.1789994, 195.3318977, 195.4847959, 195.6376941, 195.7184713, 195.7704, 195.8223287, 195.8742575, 195.9841685, 196.1713893, 196.3586101, 196.5458309, 196.7330517, 196.7796652, 196.8262786, 196.8728921, 196.9195056, 196.9432711, 196.9499007, 196.9565303, 196.9631599, 197.0457208, 197.2295234, 197.4133259, 197.5481609, 197.5553137, 197.5886046, 197.5971285, 197.6063534, 197.6218954, 197.6551863, 197.664546, 197.692292, 197.7227385, 197.7389345, 197.780931, 197.785577, 197.8322195, 197.8788621, 197.9290378, 197.9792136, 198.0293893, 198.0795651, 198.0950484, 198.0967341, 198.0984199, 198.1001057, 198.1267184, 198.358523, 198.5903276, 198.8221322, 199.0539368, 199.1481963, 199.2195317, 202.3705092, 202.3792571, 202.4312238, 202.4831905, 202.5351571, 202.6210737, 202.6594654, 202.83803, 202.8716382, 203.0165945, 203.0538548, 203.060919, 203.0642742, 203.1222028, 203.1641524, 203.1750161, 203.195159, 203.2010968, 203.2479796, 203.2673857, 203.2798724, 203.2961774, 203.3117653, 203.3436582, 203.370619, 203.3727673, 203.3996521, 203.4086682, 203.4173387, 203.4176843, 203.4214677, 203.4267005, 203.4475786, 203.4499286, 203.4507258, 203.4799839, 203.4840645, 203.5092419, 203.5216786, 203.5385, 203.5714286, 203.5934286, 203.6101784, 203.6133915, 203.6503685, 203.6651786, 203.6905586, 203.6952786, 203.7299032, 203.7307486, 203.7770645, 203.8191286, 203.8242258, 203.8315161, 203.8488134, 203.8713871, 203.8770235, 203.9185484, 204.0252842, 204.1406556, 204.1789677, 204.201755, 204.2471935, 204.3571656, 204.3625658, 204.3782258, 204.3857472, 204.4042877, 204.4321004, 204.4630869, 204.4650559, 204.4812188, 204.4940734, 204.5250598, 204.5264194, 204.5675459, 204.605272, 204.6321194, 204.6376731, 204.6432267, 204.6487803, 204.6679198, 204.670036, 204.6877378, 204.7026042, 204.7266827, 204.7293252, 204.7513279, 204.7583566, 204.7900305, 204.8182126, 204.8217044, 204.8249189, 204.8533784, 204.873871, 204.8850973, 204.951982, 204.9621, 205.0137637, 205.0460497, 205.0627881, 205.0992811, 205.1118126, 205.160837, 205.1683214, 205.2098614, 205.3894952, 205.4415714, 205.7148214, 205.7329407, 205.9880714, 206.0763862, 206.225924, 206.3694701, 206.4496175, 206.6424093, 206.6733111, 206.8560238, 206.8958203, 206.8970046, 206.9153485, 206.9424405, 207.0288571, 207.1015429, 207.1039009, 207.1152738, 207.1882876, 207.1940691, 207.2538111, 207.2581929, 207.3119816, 207.313553, 207.3732949, 207.4148429, 207.4283845, 207.4330369, 207.4953088, 207.5200622, 207.5580023, 207.5714929, 207.6206959, 207.6553444, 207.6833894, 207.7281429, 207.8527143, 207.8823043, 208.1092642, 208.1265604, 208.1535092, 208.1642143, 208.2306018, 208.3172811, 208.3346432, 208.3551116, 208.4386847, 208.4757143, 208.4779405, 208.5309337, 208.556714, 208.5937018, 208.6386, 208.6564699, 208.719238, 208.7583165, 208.7820061, 208.7872143, 208.7992595, 208.8424359, 208.902476, 208.9599189, 208.9625161, 209.0225562, 209.0752166, 209.118536, 209.1215867, 209.170772, 209.1893892, 209.2199574, 209.2222327, 209.2691428, 209.3692488, 209.3922937, 209.400638, 209.4986126, 209.516265, 209.5938656, 209.6632811, 209.6660514, 209.7870932, 209.807836, 209.8529663, 209.8630532, 209.8651071, 209.8708867, 209.9398091, 209.9803208, 209.984205, 209.9958663, 209.9970208, 209.9984585, 210.0011018, 210.0451024, 210.0505833, 210.0771885, 210.0888571, 210.0975233, 210.1000648, 210.1092746, 210.1170595, 210.1286794, 210.1413607, 210.1495463, 210.1787224, 210.1805602, 210.182398, 210.1842358, 210.2108417, 210.2135668, 210.2372494, 210.2379731, 210.2614925, 210.2641156, 210.2768443, 210.2831653, 210.2924504, 210.2966631, 210.3126071, 210.3292106, 210.3374261, 210.3399684, 210.3617581, 210.3664469, 210.376263, 210.3895456, 210.3943056, 210.4027602, 210.4391228, 210.4404433, 210.4798874, 210.4887, 210.5144398, 210.5152765, 210.5363571, 210.5473252, 210.5552301, 210.5805944, 210.6031072, 210.6170934, 210.6228574, 210.6271444, 210.65429, 210.6736944, 210.6766095, 210.6918901, 210.7202444, 210.7365392, 210.7526053, 210.7577954, 210.7657317, 210.7667944, 210.8204499, 210.833616, 210.8364551, 210.8733316, 210.8803726, 210.8872772, 210.8896556, 210.8921319, 210.8967606, 210.8979935, 210.9052972, 210.939388, 210.9451605, 210.9682412, 210.9759573, 210.9810201, 210.9891204, 210.9950134, 211.0067542, 211.0180424, 211.0264684, 211.037551, 211.0388528, 211.0645458, 211.0646599, 211.064774, 211.0648881, 211.0740553, 211.0963582, 211.1003854, 211.108414, 211.1096543, 211.1120018, 211.1176713, 211.1247993, 211.1532104, 211.1608049, 211.169023, 211.1738835, 211.1764278, 211.1806415, 211.1847207, 211.1886931, 211.215635, 211.2235333, 211.2241759, 211.2351443, 211.2421698, 211.2428134, 211.2552578, 211.2596586, 211.265543, 211.2891429, 211.2951413, 211.3196429, 211.3298742, 211.3333753, 211.3386526, 211.3501429, 211.3699032, 211.372888, 211.3806429, 211.4044906, 211.4047419, 211.4049321, 211.4051222, 211.4053124, 211.4062867, 211.4115714, 211.4120757, 211.4507688, 211.4537719, 211.4560951, 211.4574109, 211.4659526, 211.4713286, 211.4864691, 211.4951902, 211.4997811, 211.5046621, 211.5187208, 211.5224596, 211.5312479, 211.5470304, 211.5661131, 211.5673056, 211.5718925, 211.5879908, 211.5972246, 211.6033633, 211.607193, 211.6135053, 211.6394211, 211.6394306, 211.6539716, 211.6561123, 211.6608975, 211.6642907, 211.6719895, 211.6762005, 211.6985093, 211.7325146, 211.7467544, 211.7484333, 211.7644101, 211.770897, 211.7801861, 211.7915565, 211.8004698, 211.8137436, 211.8272343, 211.8421769, 211.8442706, 211.8471283, 211.8552668, 211.8612937, 211.8667856, 211.8771468, 211.8896737, 211.8942725, 211.8960815, 211.9071653, 211.9088438, 211.916835, 211.9270006, 211.9371839, 211.9442745, 211.9560305, 211.9563939, 211.9567142, 211.9573978, 211.9580815, 211.9942765, 212.003944, 212.0063522, 212.008514, 212.0119769, 212.0142605, 212.0193491, 212.0499271, 212.0624447, 212.0685039, 212.0769346, 212.0869176, 212.1174212, 212.123908, 212.1275324, 212.1308239, 212.1519404, 212.1608984, 212.1613951, 212.1970577, 212.1978889, 212.2240646, 212.2269463, 212.2360401, 212.2912786, 212.3019522, 212.303441, 212.3180074, 212.3322805, 212.3691867, 212.3800012, 212.4035763, 212.412888, 212.4169928, 212.445487, 212.464799, 212.5126051, 212.5185936, 212.533737, 212.5604113, 212.5668812, 212.5711125, 212.576205, 212.5928624, 212.6212163, 212.6223518, 212.6296549, 212.6651567, 212.6982436, 212.7351935, 212.7386486, 212.7396889, 212.746898, 212.7514884, 212.7578505, 212.7700425, 212.8142212, 212.8161546, 212.8336399, 212.845337, 212.8611313, 212.8641412, 212.8745193, 212.8887535, 212.8944846, 212.9033115, 212.9037017, 212.9134266, 212.9149674, 212.9165082, 212.918049, 212.9286312, 212.9367046, 212.9632857, 212.9655882, 212.9804059, 212.9813843, 212.9835992, 213.0130524, 213.013312, 213.0236225, 213.0311188, 213.0398643, 213.0519222, 213.062819, 213.0905324, 213.1096787, 213.1125857, 213.1152886, 213.1186138, 213.1229755, 213.123851, 213.1291427, 213.1623524, 213.1677529, 213.1702811, 213.1719747, 213.1736682, 213.1753618, 213.1786952, 213.190421, 213.1907129, 213.2123786, 213.2460619, 213.2478853, 213.2661373, 213.2672961, 213.2732106, 213.2797452, 213.3134286, 213.3219124, 213.3303963, 213.3337977, 213.3388802, 213.3399647, 213.3436744, 213.3473641, 213.3820486, 213.4107412, 213.4226959, 213.4302994, 213.4725116, 213.4775305, 213.4785503, 213.4944627, 213.5268011, 213.535609, 213.5481636, 213.5776701, 213.6196139, 213.6211778, 213.6655355, 213.6670857, 213.6716815, 213.6718127, 213.6736313, 213.7143412, 213.7176024, 213.7221852, 213.7481256, 213.7646401, 213.768119, 213.7726889, 213.7917147, 213.7990991, 213.8068304, 213.8116658, 213.8186357, 213.8233327, 213.8402689, 213.8469819, 213.848478, 213.8711137, 213.8990459, 213.9116886, 213.9120595, 213.9324122, 213.9332167, 213.9496138, 213.9577839, 213.9580793, 214.0001817, 214.0156238, 214.0162805, 214.0167132, 214.0245556, 214.026217, 214.0887176, 214.0907105, 214.0955503, 214.1083654, 214.1110564, 214.1180803, 214.1192333, 214.1399135, 214.1399162, 214.164218, 214.1647079, 214.1713416, 214.1921572, 214.2037634, 214.2343177, 214.2387053, 214.2408462, 214.2500323, 214.2521573, 214.2972939, 214.30525, 214.3127027, 214.3241011, 214.3465181, 214.3580974, 214.3602701, 214.3627114, 214.3675045, 214.3703567, 214.3842702, 214.4176476, 214.4223063, 214.4239935, 214.4248812, 214.4265704, 214.4328505, 214.4630941, 214.4640599, 214.465412, 214.4714512, 214.4729952, 214.4771081, 214.4878416, 214.4886908, 214.4912667, 214.4958382, 214.5240376, 214.5301219, 214.5319099, 214.5422806, 214.5463222, 214.5485571, 214.5499425, 214.5513278, 214.5516896, 214.5527132, 214.5531227, 214.5564968, 214.5653243, 214.5747916, 214.5764954, 214.5928119, 214.5999389, 214.6029664, 214.606, 214.6155376, 214.6198868, 214.6214189, 214.6466757, 214.6474453, 214.6475127, 214.6513538, 214.6554591, 214.6564301, 214.6660741, 214.6664878, 214.6729901, 214.6751386, 214.6772833, 214.6873514, 214.6899778, 214.6940735, 214.695346, 214.6955104, 214.6964908, 214.6986466, 214.7017828, 214.7027646, 214.704919, 214.7126286, 214.7212488, 214.7216592, 214.7252242, 214.7257848, 214.728027, 214.7331351, 214.7382432, 214.7415392, 214.7415521, 214.7418728, 214.7433514, 214.7441108, 214.7447295, 214.7470729, 214.7479069, 214.7484595, 214.7492449, 214.7510843, 214.7597274, 214.7693037, 214.7765028, 214.7775231, 214.7858259, 214.7865779, 214.787913, 214.7930991, 214.7934111, 214.8056534, 214.8065431, 214.8155214, 214.825578, 214.8322484, 214.8324452, 214.8341952, 214.834529, 214.8368678, 214.8371664, 214.8481685, 214.8506185, 214.8528728, 214.8780453, 214.8785562, 214.8807793, 214.8897938, 214.894576, 214.8965756, 214.9054721, 214.9084516, 214.9153531, 214.9234729, 214.9254865, 214.9257105, 214.9257339, 214.9268131, 214.9296249, 214.9301534, 214.9314191, 214.932899, 214.9334937, 214.9362793, 214.9420631, 214.9547795, 214.9567044, 214.9616381, 214.9730444, 214.9749587, 214.9779825, 214.9846548, 214.9980596, 214.9981378, 215.0134426, 215.0166484, 215.0187191, 215.0196857, 215.0310029, 215.0359315, 215.0435229, 215.060858, 215.0614025, 215.0615285, 215.064843, 215.0679731, 215.0743939, 215.0749122, 215.0779426, 215.0878309, 215.0910982, 215.1072262, 215.1077548, 215.1096657, 215.1199536, 215.1233185, 215.1268275, 215.1293114, 215.1363819, 215.1378313, 215.1445203, 215.1544822, 215.1619646, 215.1640872, 215.1729926, 215.1757, 215.17839, 215.1841368, 215.1978515, 215.2039756, 215.2074519, 215.2141341, 215.2189573, 215.2248, 215.2274686, 215.2538714, 215.2593211, 215.2736553, 215.2739, 215.2765472, 215.2771754, 215.2772683, 215.290437, 215.2909028, 215.2918561, 215.3229307, 215.323, 215.3256258, 215.3554013, 215.3583757, 215.3584231, 215.3589917, 215.3611087, 215.3721, 215.3834778, 215.386897, 215.4024406, 215.4081762, 215.4222784, 215.4372854, 215.4448709, 215.4508632, 215.4532259, 215.4573607, 215.4599053, 215.4834482, 215.5037878, 215.5065452, 215.5065821, 215.5148295, 215.544618, 215.5475459, 215.5528862, 215.5557297, 215.560463, 215.6057878, 215.6064555, 215.6124735, 215.6279544, 215.6407939, 215.6488731, 215.6539583, 215.6693107, 215.6944378, 215.7332258, 215.7339202, 215.7358438, 215.7367162, 215.7474537, 215.7831333, 215.7960035, 215.7971409, 215.8384319, 215.8409491, 215.8592673, 215.861056, 215.8863367, 215.9035078, 215.9250696, 215.9258865, 215.9327797, 215.9640526, 215.985753, 216.0280407, 216.0282356, 216.0410526, 216.0464364, 216.0704083, 216.0885258, 216.1071198, 216.1150568, 216.1162864, 216.1438163, 216.1464699, 216.1515902, 216.1678032, 216.1968142, 216.2311855, 216.2468287, 216.2660913, 216.2950176, 216.3023847, 216.3126733, 216.3588498, 216.3620333, 216.3655877, 216.3758246, 216.3841249, 216.4051315, 216.4226819, 216.496729, 216.5343611, 216.5371616, 216.5634344, 216.5840732, 216.5843571, 216.6033083, 216.6244334, 216.6310383, 216.6314502, 216.6958311, 216.7105965, 216.7217373, 216.7257388, 216.8154856, 216.819252, 216.8200275, 216.8446627, 216.8780275, 216.9044732, 216.9247918, 216.9395861, 216.9396605, 216.964312, 217.0048261, 217.0146941, 217.0241507, 217.0453684, 217.0544307, 217.0839894, 217.1095679, 217.1438281, 217.1650042, 217.1716979, 217.1812533, 217.1847255, 217.2069662, 217.2185454, 217.2706543, 217.2760127, 217.2896095, 217.3251824, 217.3343423, 217.3547569, 217.3552733, 217.3980304, 217.4036503, 217.4229206, 217.4653683, 217.4853605, 217.512299, 217.5159762, 217.5247882, 217.5797506, 217.6123648, 217.6455387, 217.6645878, 217.6766791, 217.6977326, 217.7235226, 217.7705442, 217.837382, 217.8670218, 217.8781339, 217.9188471, 217.9237458, 217.9563371, 217.9674705, 217.9980849, 218.0145862, 218.0541851, 218.0852999, 218.1130269, 218.2007506, 218.2114184, 218.2205088, 218.2302364, 218.2468539, 218.2579435, 218.2586281, 218.3551748, 218.3590319, 218.4021448, 218.4037974, 218.4062876, 218.408408, 218.445164, 218.4508115, 218.45094, 218.4553663, 218.4676211, 218.4979481, 218.50267, 218.5467052, 218.5699621, 218.5823389, 218.5877333, 218.5937514, 218.59704, 218.605037, 218.6365747, 218.6424704, 218.6449369, 218.6755292, 218.6788642, 218.6836874, 218.6895548, 218.6895775, 218.6921051, 218.7147333, 218.7262524, 218.7275216, 218.7687195, 218.7746217, 218.7796415, 218.7857881, 218.793912, 218.8217928, 218.8328475, 218.8440545, 218.851237, 218.8619099, 218.8755955, 218.8784768, 218.8860765, 218.8910733, 218.8986857, 218.9023209, 218.9109844, 218.9134935, 218.9333986, 218.9492991, 218.9551002, 218.9605873, 218.9607242, 218.9618456, 218.9995495, 219.0075249, 219.0187895, 219.023271, 219.0236099, 219.0428204, 219.0701968, 219.071119, 219.0740167, 219.0768548, 219.0861659, 219.0866908, 219.1148297, 219.1203788, 219.127216, 219.1336097, 219.1349201, 219.1501106, 219.1746922, 219.1773063, 219.1794533, 219.1929854, 219.1959827, 219.1970226, 219.2135305, 219.237049, 219.2435524, 219.2556109, 219.2588382, 219.2604355, 219.2917287, 219.3189965, 219.3244636, 219.355063, 219.3577216, 219.3622809, 219.3683556, 219.382382, 219.3972867, 219.4000812, 219.4159857, 219.4315106, 219.4442443, 219.4457675, 219.5328198, 219.5340975, 219.5359898, 219.540637, 219.5631135, 219.6041829, 219.6258417, 219.6297841, 219.7142581, 219.7188636, 219.7414914, 219.7437314, 219.7596266, 219.788581, 219.7897137, 219.8118854, 219.8925263, 219.8956335, 219.9049073, 219.9387246, 219.9705599, 219.9746423, 219.9856893, 220.0417412, 220.0454862, 220.0651993, 220.0788523, 220.085696, 220.1178226, 220.1204125, 220.1329176, 220.1720153, 220.1953389, 220.2483937, 220.2651783, 220.275944, 220.2937686, 220.2969205, 220.3014485, 220.3195004, 220.3545033, 220.4075581, 220.4252149, 220.4257586, 220.4287124, 220.4299007, 220.4322099, 220.4357073, 220.4494148, 220.4760185, 220.4772543, 220.4866886, 220.4886472, 220.5278796, 220.5457961, 220.567112, 220.5694104, 220.6063444, 220.6148749, 220.6234054, 220.6319358, 220.636902, 220.6404663, 220.6628023, 220.6643585, 220.6974332, 220.7199609, 220.7486167, 220.7561941, 220.7671856, 220.7960935, 220.8480454, 220.849586, 220.8498468, 220.8526787, 220.9144005, 220.9244858, 220.9477245, 220.9619484, 220.9836849, 220.9853964, 220.9991248, 221.0106341, 221.0591887, 221.0737638, 221.0801842, 221.1181142, 221.1278032, 221.1282634, 221.1484028, 221.1498206, 221.2021074, 221.2118132, 221.2224243, 221.245968, 221.255812, 221.2601207, 221.2864126, 221.2936581, 221.3088023, 221.3095166, 221.3159563, 221.3610119, 221.3632212, 221.380331, 221.3828029, 221.3852748, 221.3877467, 221.4009901, 221.4117517, 221.4342146, 221.4356112, 221.4411622, 221.4578604, 221.4595129, 221.4813343, 221.4820921, 221.4893412, 221.5102105, 221.5215064, 221.5616784, 221.5640737, 221.5701123, 221.5785461, 221.5831306, 221.5869799, 221.5954138, 221.6179368, 221.6432852, 221.6460048, 221.6482278, 221.6556, 221.6718, 221.6751459, 221.6769199, 221.6911738, 221.7256632, 221.742674, 221.744944, 221.7472139, 221.7494839, 221.7626421, 221.7707093, 221.7989713, 221.8030211, 221.8083518, 221.8434, 221.8533396, 221.8644987, 221.8735063, 221.8803923, 221.8837789, 221.9011185, 221.9241579, 221.9327267, 221.9412954, 221.9415576, 221.9480412, 221.9491571, 221.9498642, 221.9584329, 222.0225762, 222.026359, 222.0384109, 222.0510793, 222.0747635, 222.0951719, 222.0971111, 222.1136566, 222.1389683, 222.1512315, 222.1589519, 222.1719457, 222.2053519, 222.2071405, 222.2174395, 222.2392726, 222.2594722, 222.3054805, 222.3135926, 222.330443, 222.3323853, 222.3343277, 222.33627, 222.3492901, 222.3900046, 222.3975443, 222.4167852, 222.4169362, 222.4307191, 222.4390153, 222.4521862, 222.4651238, 222.4714336, 222.5121481, 222.5209358, 222.5297234, 222.5385111, 222.5472987, 222.5820193, 222.6164329, 222.6276753, 222.6426418, 222.6561203, 222.6650998, 222.6910959, 222.7617437, 222.7818386, 222.8159296, 222.8186603, 222.8258628, 222.8743862, 222.8825484, 222.8930835, 222.8958121, 222.9675066, 222.9816579, 223.0090839, 223.0154263, 223.0298805, 223.0419298, 223.0576765, 223.0598077, 223.0661125, 223.0783366, 223.1163529, 223.17092, 223.1734167, 223.1814772, 223.1923049, 223.2254871, 223.2409668, 223.2496766, 223.2800541, 223.3346212, 223.3511928, 223.3525662, 223.3539397, 223.3553131, 223.3680933, 223.3737593, 223.3755259, 223.3812965, 223.4093906, 223.4242571, 223.4257233, 223.4332408, 223.4442513, 223.4506878, 223.4700552, 223.4919851, 223.5188055, 223.5332824, 223.5424501, 223.5516178, 223.5607856, 223.5645845, 223.5699533, 223.5741019, 223.6133539, 223.6168049, 223.6510224, 223.6591137, 223.7079023, 223.7114288, 223.753643, 223.7744444, 223.7860175, 223.8024507, 223.8060527, 223.9006766, 223.9170153, 223.9210125, 223.9658621, 223.974787, 223.9953006, 224.0192873, 224.0378139, 224.0560076, 224.1320199, 224.1588663, 224.1809207, 224.2004678, 224.3470245, 224.3741384, 224.3959793, 224.4017192, 224.5620291, 224.5894104, 224.6029706, 224.6110379, 224.7166953, 224.7657327, 224.7909104, 224.804222, 224.8046825, 224.8399424, 224.8651254, 224.9141521, 224.9393405, 224.9594902, 224.9883618, 225.0054733, 225.0135556, 225.0336786, 225.0501013, 225.0625714, 225.0686254, 225.0689541, 225.107867, 225.1179914, 225.1243526, 225.1734114, 225.1797511, 225.1820555, 225.2288314, 225.2351496, 225.2512024, 225.2515168, 225.2518313, 225.2521458, 225.2562439, 225.2644795, 225.2842514, 225.3002908, 225.3005779, 225.300865, 225.3011521, 225.3068615, 225.3117488, 225.3134743, 225.3492435, 225.3558843, 225.3672537, 225.3916254, 225.3982943, 225.4227585, 225.4340074, 225.4407043, 225.4438827, 225.4537579, 225.4636332, 225.4735085, 225.4782634, 225.4831143, 225.4930078, 225.4942498, 225.4944288, 225.4946078, 225.4947868, 225.5029014, 225.5070634, 225.5127949, 225.5226885, 225.5495841, 225.5558664, 225.5921049, 225.6050797, 225.6346256, 225.6771463, 225.6871121, 225.6925864, 225.6970779, 225.7070437, 225.7170094, 225.7418442, 225.7995323, 225.8293063, 225.8786088, 225.9364729, 225.9660263, 226.0153733, 226.0734135, 226.1122067, 226.1615981, 226.2103541, 226.3151496, 226.3567545, 226.3645848, 226.5180926, 226.5599138, 226.5675714, 226.7210356, 226.7630732, 226.7705581, 226.9239785, 226.9662325, 226.9688442, 226.9735448, 226.9873637, 227.0184166, 227.0369359, 227.1693919, 227.214288, 227.2328068] [0.0017094  0.0034188  0.00512821 ... 0.9996892  0.9998446  1.        ]\nUnemployment\n[3.879, 4.077, 4.125, 4.145, 4.156, 4.261, 4.308, 4.42, 4.584, 4.607, 4.781, 4.954, 5.114, 5.124, 5.143, 5.217, 5.277, 5.287, 5.326, 5.329, 5.401, 5.407, 5.422, 5.435, 5.527, 5.539, 5.603, 5.621, 5.644, 5.667, 5.668, 5.679, 5.765, 5.774, 5.801, 5.825, 5.847, 5.892, 5.936, 5.943, 5.946, 5.964, 5.965, 6.034, 6.054, 6.061, 6.078, 6.104, 6.123, 6.132, 6.162, 6.169, 6.17, 6.195, 6.228, 6.232, 6.235, 6.262, 6.29, 6.297, 6.299, 6.3, 6.315, 6.334, 6.338, 6.339, 6.38, 6.384, 6.392, 6.403, 6.404, 6.415, 6.416, 6.425, 6.432, 6.433, 6.442, 6.465, 6.489, 6.496, 6.51, 6.529, 6.547, 6.548, 6.551, 6.56, 6.565, 6.566, 6.573, 6.589, 6.614, 6.617, 6.623, 6.634, 6.635, 6.664, 6.697, 6.745, 6.759, 6.768, 6.774, 6.833, 6.842, 6.855, 6.858, 6.866, 6.868, 6.877, 6.885, 6.891, 6.895, 6.901, 6.906, 6.908, 6.925, 6.934, 6.943, 6.961, 6.973, 6.986, 6.989, 7.007, 7.039, 7.057, 7.082, 7.092, 7.127, 7.138, 7.139, 7.143, 7.147, 7.17, 7.193, 7.197, 7.224, 7.241, 7.244, 7.259, 7.274, 7.28, 7.287, 7.293, 7.335, 7.343, 7.346, 7.348, 7.363, 7.368, 7.372, 7.382, 7.396, 7.405, 7.441, 7.467, 7.47, 7.484, 7.489, 7.503, 7.508, 7.527, 7.541, 7.543, 7.545, 7.551, 7.557, 7.564, 7.567, 7.574, 7.598, 7.603, 7.61, 7.658, 7.671, 7.682, 7.706, 7.716, 7.725, 7.742, 7.753, 7.767, 7.771, 7.787, 7.795, 7.804, 7.806, 7.808, 7.818, 7.827, 7.838, 7.85, 7.852, 7.856, 7.866, 7.872, 7.874, 7.896, 7.906, 7.907, 7.931, 7.943, 7.951, 7.962, 7.972, 7.982, 7.992, 8.0, 8.009, 8.01, 8.021, 8.023, 8.028, 8.058, 8.067, 8.075, 8.09, 8.099, 8.106, 8.107, 8.117, 8.119, 8.149, 8.15, 8.163, 8.177, 8.185, 8.187, 8.193, 8.2, 8.211, 8.212, 8.237, 8.239, 8.243, 8.252, 8.253, 8.256, 8.257, 8.275, 8.283, 8.3, 8.304, 8.316, 8.324, 8.326, 8.348, 8.35, 8.358, 8.36, 8.395, 8.424, 8.433, 8.442, 8.445, 8.454, 8.458, 8.464, 8.471, 8.476, 8.488, 8.494, 8.512, 8.513, 8.521, 8.523, 8.535, 8.549, 8.554, 8.567, 8.572, 8.595, 8.622, 8.623, 8.625, 8.659, 8.665, 8.667, 8.684, 8.687, 8.693, 8.724, 8.743, 8.744, 8.745, 8.763, 8.818, 8.839, 8.861, 8.876, 8.89, 8.899, 8.951, 8.953, 8.963, 8.975, 8.983, 8.988, 8.992, 9.003, 9.014, 9.017, 9.051, 9.131, 9.137, 9.14, 9.151, 9.199, 9.202, 9.262, 9.265, 9.269, 9.285, 9.331, 9.342, 9.357, 9.419, 9.495, 9.521, 9.524, 9.575, 9.593, 9.653, 9.765, 9.816, 9.849, 9.863, 9.966, 10.064, 10.115, 10.148, 10.16, 10.199, 10.21, 10.256, 10.398, 10.409, 10.524, 10.581, 10.641, 10.926, 11.627, 12.187, 12.89, 13.503, 13.736, 13.975, 14.021, 14.099, 14.18, 14.313] [6.21600622e-04 2.64180264e-03 6.68220668e-03 7.92540793e-03\n 1.19658120e-02 1.60062160e-02 1.80264180e-02 2.20668221e-02\n 2.64180264e-02 2.84382284e-02 3.24786325e-02 3.31002331e-02\n 3.68298368e-02 3.74514375e-02 3.94716395e-02 4.00932401e-02\n 4.21134421e-02 4.64646465e-02 5.05050505e-02 5.11266511e-02\n 5.31468531e-02 5.51670552e-02 5.57886558e-02 5.98290598e-02\n 6.04506605e-02 6.24708625e-02 6.44910645e-02 6.71328671e-02\n 6.93084693e-02 7.13286713e-02 7.33488733e-02 7.53690754e-02\n 7.73892774e-02 7.94094794e-02 8.14296814e-02 8.34498834e-02\n 8.40714841e-02 8.65578866e-02 8.85780886e-02 9.05982906e-02\n 9.26184926e-02 9.46386946e-02 9.66588967e-02 9.79020979e-02\n 9.99222999e-02 1.01942502e-01 1.03962704e-01 1.05982906e-01\n 1.08003108e-01 1.10023310e-01 1.12043512e-01 1.14063714e-01\n 1.16550117e-01 1.17171717e-01 1.19036519e-01 1.21056721e-01\n 1.23076923e-01 1.24941725e-01 1.26961927e-01 1.28982129e-01\n 1.30225330e-01 1.32245532e-01 1.34265734e-01 1.38306138e-01\n 1.40481740e-01 1.42501943e-01 1.44522145e-01 1.46542347e-01\n 1.48562549e-01 1.50582751e-01 1.52758353e-01 1.54001554e-01\n 1.55866356e-01 1.58041958e-01 1.60062160e-01 1.62237762e-01\n 1.64257964e-01 1.66278166e-01 1.68298368e-01 1.70318570e-01\n 1.72183372e-01 1.74358974e-01 1.76379176e-01 1.77622378e-01\n 1.79642580e-01 1.83993784e-01 1.92074592e-01 1.93317793e-01\n 1.93939394e-01 1.95959596e-01 1.97824398e-01 1.99844600e-01\n 2.05905206e-01 2.07770008e-01 2.09790210e-01 2.13830614e-01\n 2.15850816e-01 2.18026418e-01 2.20046620e-01 2.22222222e-01\n 2.24242424e-01 2.28282828e-01 2.30303030e-01 2.32323232e-01\n 2.34188034e-01 2.36052836e-01 2.38073038e-01 2.40248640e-01\n 2.42424242e-01 2.50505051e-01 2.51126651e-01 2.53302253e-01\n 2.55322455e-01 2.57342657e-01 2.59518260e-01 2.61538462e-01\n 2.62781663e-01 2.66822067e-01 2.68842269e-01 2.71017871e-01\n 2.77078477e-01 2.79254079e-01 2.80497280e-01 2.88578089e-01\n 2.92618493e-01 2.94638695e-01 2.96814297e-01 2.97435897e-01\n 3.01476301e-01 3.03496503e-01 3.05516706e-01 3.09557110e-01\n 3.11577312e-01 3.15617716e-01 3.17482517e-01 3.19347319e-01\n 3.25407925e-01 3.26651127e-01 3.31002331e-01 3.35042735e-01\n 3.39083139e-01 3.40326340e-01 3.42346542e-01 3.50116550e-01\n 3.54156954e-01 3.56177156e-01 3.58197358e-01 3.60683761e-01\n 3.62703963e-01 3.66744367e-01 3.68764569e-01 3.70784771e-01\n 3.78865579e-01 3.80885781e-01 3.82750583e-01 3.87101787e-01\n 3.89121989e-01 3.91142191e-01 3.93317793e-01 3.97358197e-01\n 3.98601399e-01 3.99222999e-01 4.03263403e-01 4.06993007e-01\n 4.08236208e-01 4.12587413e-01 4.16938617e-01 4.20979021e-01\n 4.22999223e-01 4.25019425e-01 4.27195027e-01 4.31235431e-01\n 4.33255633e-01 4.35275835e-01 4.37296037e-01 4.43356643e-01\n 4.45376845e-01 4.47241647e-01 4.49261849e-01 4.51437451e-01\n 4.55167055e-01 4.57187257e-01 4.59362859e-01 4.61383061e-01\n 4.65734266e-01 4.67754468e-01 4.69774670e-01 4.71639472e-01\n 4.73815074e-01 4.75990676e-01 4.84693085e-01 4.88733489e-01\n 4.94794095e-01 4.98834499e-01 5.02874903e-01 5.04895105e-01\n 5.06915307e-01 5.08780109e-01 5.16860917e-01 5.20901321e-01\n 5.22921523e-01 5.25097125e-01 5.27117327e-01 5.29137529e-01\n 5.30380730e-01 5.31002331e-01 5.33022533e-01 5.35042735e-01\n 5.37218337e-01 5.39393939e-01 5.46853147e-01 5.48873349e-01\n 5.53224553e-01 5.55244755e-01 5.59285159e-01 5.71406371e-01\n 5.72649573e-01 5.74669775e-01 5.76689977e-01 5.77933178e-01\n 5.80108780e-01 5.84149184e-01 5.92851593e-01 5.99378399e-01\n 6.03418803e-01 6.05905206e-01 6.09945610e-01 6.18026418e-01\n 6.20046620e-01 6.22066822e-01 6.23310023e-01 6.25330225e-01\n 6.25951826e-01 6.29836830e-01 6.31857032e-01 6.35897436e-01\n 6.40248640e-01 6.42424242e-01 6.43667444e-01 6.49728050e-01\n 6.51748252e-01 6.52991453e-01 6.57964258e-01 6.59207459e-01\n 6.61227661e-01 6.63714064e-01 6.65889666e-01 6.71950272e-01\n 6.77544678e-01 6.81585082e-01 6.83605284e-01 6.85780886e-01\n 6.87801088e-01 6.89821290e-01 6.91686092e-01 6.97746698e-01\n 6.99766900e-01 7.06293706e-01 7.07536908e-01 7.11577312e-01\n 7.15617716e-01 7.19658120e-01 7.23698524e-01 7.27738928e-01\n 7.29759130e-01 7.35353535e-01 7.39083139e-01 7.43123543e-01\n 7.45299145e-01 7.49339549e-01 7.53690754e-01 7.54933955e-01\n 7.59285159e-01 7.61305361e-01 7.61926962e-01 7.63170163e-01\n 7.69386169e-01 7.71406371e-01 7.72027972e-01 7.76379176e-01\n 7.80419580e-01 7.86169386e-01 7.88189588e-01 7.90365190e-01\n 7.94094794e-01 7.97358197e-01 7.99378399e-01 8.01398601e-01\n 8.03574204e-01 8.07614608e-01 8.09479409e-01 8.11499611e-01\n 8.15540016e-01 8.17560218e-01 8.19580420e-01 8.21600622e-01\n 8.24087024e-01 8.28438228e-01 8.30924631e-01 8.34965035e-01\n 8.36985237e-01 8.38850039e-01 8.43201243e-01 8.45221445e-01\n 8.45843046e-01 8.49883450e-01 8.51126651e-01 8.52369852e-01\n 8.54545455e-01 8.56565657e-01 8.60606061e-01 8.62781663e-01\n 8.64801865e-01 8.66822067e-01 8.68842269e-01 8.70862471e-01\n 8.73348873e-01 8.77389277e-01 8.81429681e-01 8.85470085e-01\n 8.89510490e-01 8.91996892e-01 8.96037296e-01 8.98057498e-01\n 9.00233100e-01 9.02253302e-01 9.03496503e-01 9.04739705e-01\n 9.08780109e-01 9.10800311e-01 9.12665113e-01 9.17016317e-01\n 9.18881119e-01 9.22610723e-01 9.24630925e-01 9.26806527e-01\n 9.30846931e-01 9.35198135e-01 9.41258741e-01 9.47319347e-01\n 9.53379953e-01 9.59440559e-01 9.65967366e-01 9.72027972e-01\n 9.75757576e-01 9.81351981e-01 9.87412587e-01 9.93473193e-01\n 1.00000000e+00]\n\n\n\n\n\n\nfrom scipy.stats import hypergeom\npopulation_size = len(df)\nsample_size = 10\nnum_columns = len(df.columns)\nnum_rows = (num_columns // 2) + (num_columns % 2)\nnum_cols = 2\n\nfig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 3 * num_rows))\nif num_rows == 1:\n    axes = axes.reshape(1, -1)\nfor i, column in enumerate(df.columns):\n    row_idx = i // num_cols\n    col_idx = i % num_cols\n\n    numeric_data = pd.to_numeric(df[column], errors='coerce')\n    numeric_data = numeric_data.dropna()\n    successes_in_population = np.sum(numeric_data &gt; 0)\n    print(column)\n    print('successes',successes_in_population)\n    k_values = np.arange(0, min(sample_size, successes_in_population) + 1)\n\n    pmf_values = hypergeom.pmf(k_values, population_size, successes_in_population, sample_size)\n    print(f'Hypergeometric Distribution PMF for {column}',pmf_values)\n    axes[row_idx, col_idx].bar(k_values, pmf_values, align='center', alpha=0.7)\n    axes[row_idx, col_idx].set_title(f'Hypergeometric Distribution PMF for {column}')\n    axes[row_idx, col_idx].set_xlabel('Number of Successes in Draws (k)')\n    axes[row_idx, col_idx].set_ylabel('Probability')\n\nif num_columns % 2 == 1:\n    fig.delaxes(axes[-1, -1])\n\nplt.tight_layout()\nplt.show()\n\nStore\nsuccesses 6435\nHypergeometric Distribution PMF for Store [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\nWeekly_Sales\nsuccesses 6435\nHypergeometric Distribution PMF for Weekly_Sales [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\nHoliday_Flag\nsuccesses 450\nHypergeometric Distribution PMF for Holiday_Flag [4.84091496e-01 3.64526729e-01 1.23226745e-01 2.46261099e-02\n 3.22190575e-03 2.88355177e-04 1.78785674e-05 7.58282840e-07\n 2.10546109e-08 3.45593360e-10 2.54647739e-12]\nTemperature\nsuccesses 6434\nHypergeometric Distribution PMF for Temperature [0.       0.       0.       0.       0.       0.       0.       0.\n 0.       0.001554 0.998446]\nFuel_Price\nsuccesses 6435\nHypergeometric Distribution PMF for Fuel_Price [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\nCPI\nsuccesses 6435\nHypergeometric Distribution PMF for CPI [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\nUnemployment\nsuccesses 6435\nHypergeometric Distribution PMF for Unemployment [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n\n\n\n\n\n\nfrom scipy.stats import norm\nnumeric_columns = df.select_dtypes(include=['int64', 'float64']).columns\n\nfig, axes = plt.subplots(nrows=len(numeric_columns), figsize=(8, 4 * len(numeric_columns)))\nfig.subplots_adjust(hspace=0.5)\n\nfor i, column in enumerate(numeric_columns):\n    sns.histplot(df[column], kde=True, ax=axes[i])\n    axes[i].set_title(f'Distribution of {column}')\n    axes[i].set_xlabel(column)\n    axes[i].set_ylabel('Density')\n    mu, std = norm.fit(df[column])\n    xmin, xmax = plt.xlim()\n    x = np.linspace(xmin, xmax, 100)\n    p = norm.pdf(x, mu, std)\n    axes[i].plot(x, p, 'k', linewidth=2)\n    axes[i].legend(['Gaussian fit'])\n\nplt.tight_layout()\nplt.show()"
  }
]